#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Batch Document Processor - Multi-Image Analysis
Supports 2 modes:
- Mode 1: Fixed batch size (5 images per batch)
- Mode 2: Smart batching (group by document boundaries)
"""

import sys
import os
import json
import base64
import re
import requests
from PIL import Image
import io

# Import existing engines and prompts
from ocr_engine_gemini_flash import (
    resize_image_smart, 
    parse_gemini_response,
    get_classification_prompt,
    get_classification_prompt_lite
)


def adapt_prompt_for_multi_image(single_image_prompt, batch_size):
    """
    Adapt single-image prompt to multi-image batch context
    
    Changes:
    1. Add multi-image context introduction
    2. Add document grouping instructions
    3. Change output format from single result to documents array
    4. Add page indexing (0-indexed)
    5. Emphasize MUST return ALL pages
    """
    
    # Use % formatting to avoid f-string issues with JSON examples
    multi_image_intro = """üéØ BATCH ANALYSIS - %d TRANG SCAN

B·∫°n ƒëang ph√¢n t√≠ch %d trang scan t√†i li·ªáu ƒë·∫•t ƒëai Vi·ªát Nam.
C√°c trang n√†y c√≥ th·ªÉ thu·ªôc 1 ho·∫∑c nhi·ªÅu t√†i li·ªáu kh√°c nhau.

üö® QUAN TR·ªåNG NH·∫§T - BATCH MODE vs SINGLE-FILE MODE:
Trong BATCH MODE n√†y, b·∫°n KH√îNG ph·∫£i single-file classifier!
- ‚ùå ƒê·ª™NG tr·∫£ v·ªÅ "UNKNOWN" cho continuation pages
- ‚úÖ B·∫°n PH·∫¢I t·ª± GOM continuation pages v√†o document tr∆∞·ªõc
- ‚úÖ B·∫°n c√≥ context t·ª´ nhi·ªÅu pages ‚Üí H√£y t·∫≠n d·ª•ng!

V√ç D·ª§:
Page 0: "TH√îNG B√ÅO THU·∫æ" ‚Üí Start TBT document
Page 1: "ƒêI·ªÄU 1" ‚Üí TBT continuation ‚Üí ADD v√†o pages c·ªßa TBT
Page 2: "III. T√çNH THU·∫æ" + b·∫£ng ‚Üí TBT continuation ‚Üí ADD v√†o pages c·ªßa TBT

Result: {{"type": "TBT", "pages": [0,1,2]}} ‚úÖ

KH√îNG L√ÄM (single-file style):
Result: 
  {{"type": "TBT", "pages": [0]}},
  {{"type": "UNKNOWN", "pages": [1,2]}} ‚ùå SAI!

NHI·ªÜM V·ª§:
1. X√°c ƒë·ªãnh c√≥ BAO NHI√äU t√†i li·ªáu kh√°c nhau trong %d trang n√†y
2. Nh√≥m c√°c trang theo t√†i li·ªáu (pages array)
3. Ph√¢n lo·∫°i lo·∫°i t√†i li·ªáu c·ªßa t·ª´ng nh√≥m
4. Tr√≠ch xu·∫•t metadata (ng√†y c·∫•p cho GCN, m√†u s·∫Øc, v.v.)

D·∫§U HI·ªÜU NH·∫¨N BI·∫æT:

TRANG 1 C·ª¶A T√ÄI LI·ªÜU (New Document):
- C√≥ TI√äU ƒê·ªÄ CH√çNH ·ªü TOP 30%% (ƒë·∫ßu trang)
- C·ª° ch·ªØ L·ªöN, IN HOA, cƒÉn gi·ªØa
- C√≥ qu·ªëc huy (ƒë·ªëi v·ªõi GCN)
- Kh√°c bi·ªát r√µ v·ªÅ format/m√†u s·∫Øc so v·ªõi trang tr∆∞·ªõc

TRANG TI·∫æP N·ªêI (Continuation - Trang 2, 3, 4...):
- KH√îNG c√≥ ti√™u ƒë·ªÅ ch√≠nh ·ªü ƒë·∫ßu
- Ch·ªâ c√≥ section headers: "II.", "III.", "ƒêI·ªÄU 2", "PH·∫¶N II"
- C√πng format/m√†u s·∫Øc v·ªõi trang tr∆∞·ªõc
- N·ªôi dung li√™n t·ª•c (ƒëi·ªÅu kho·∫£n, ch·ªØ k√Ω, b·∫£ng bi·ªÉu)

üö® QUAN TR·ªåNG - NH·∫¨N DI·ªÜN CONTINUATION PAGES:
C√°c d·∫•u hi·ªáu sau = CONTINUATION (trang ti·∫øp theo, KH√îNG ph·∫£i document m·ªõi):
1. Section headers v·ªõi s·ªë: "II.", "III.", "IV.", "V.", "ƒêI·ªÄU 2", "ƒêI·ªÄU 3", "PH·∫¶N II", "M·ª§C III"
2. B·∫£ng bi·ªÉu v·ªõi s·ªë ph√¢n c·∫•p: "4.1", "4.2", "4.2.1", "4.2.2", "(1.1)", "(2.1.3)"
3. Text body ti·∫øp n·ªëi: "...ti·∫øp theo...", "...nh∆∞ sau:", danh s√°ch bullet points
4. Ch·ªØ k√Ω/con d·∫•u ·ªü cu·ªëi trang
5. Kh√¥ng c√≥ header ch√≠nh th·ª©c (qu·ªëc huy, c∆° quan ban h√†nh)
6. **"L·ªúI CH·ª®NG C·ª¶A C√îNG CH·ª®NG VI√äN"** ‚Üí Trang ch·ªØ k√Ω c√¥ng ch·ª©ng (cu·ªëi document)
7. **Danh s√°ch ng∆∞·ªùi k√Ω, con d·∫•u c√¥ng ch·ª©ng** ‚Üí Trang cu·ªëi document

üö® ƒê·∫∂C BI·ªÜT - TRANG C√îNG CH·ª®NG (KH√îNG ph·∫£i document m·ªõi):
N·∫øu trang c√≥:
- "L·ªúI CH·ª®NG C·ª¶A C√îNG CH·ª®NG VI√äN"
- "C√îNG CH·ª®NG VI√äN"
- Con d·∫•u c√¥ng ch·ª©ng (h·ªìng/ƒë·ªè)
- Danh s√°ch ch·ªØ k√Ω c√°c √îng/B√†
- VƒÉn ph√≤ng c√¥ng ch·ª©ng

‚Üí ƒê√¢y l√† TRANG CU·ªêI (signature page) c·ªßa document
‚Üí KH√îNG ph·∫£i document m·ªõi
‚Üí GOM V√ÄO document tr∆∞·ªõc (TTHGD, PCTSVC, HDCQ, HDUQ, v.v.)

V√ç D·ª§ ƒê√öNG:
Page 0: "TH·ªéA THU·∫¨N H·ªò GIA ƒê√åNH" ‚Üí TTHGD
Page 1-3: N·ªôi dung th·ªèa thu·∫≠n ‚Üí TTHGD continuation
Page 4: "L·ªúI CH·ª®NG C·ª¶A C√îNG CH·ª®NG VI√äN" + danh s√°ch ‚Üí TTHGD continuation (signature page)
Page 5: Con d·∫•u, ch·ªØ k√Ω ‚Üí TTHGD continuation

Result: {{"type": "TTHGD", "pages": [0,1,2,3,4,5]}} ‚úÖ

KH√îNG L√ÄM:
  {{"type": "TTHGD", "pages": [0,1,2,3]}},
  {{"type": "GTLQ", "pages": [4,5]}}  ‚ùå SAI!

V√ç D·ª§ CONTINUATION - PH·∫¢I GOM V√ÄO DOCUMENT TR∆Ø·ªöC:
‚úÖ "III. T√çNH THU·∫æ C·ª¶A C∆† QUAN THU·∫æ" + b·∫£ng 4.1, 4.2
   ‚Üí Section header v·ªõi s·ªë La M√£
   ‚Üí B·∫£ng bi·ªÉu ph√¢n c·∫•p
   ‚Üí ƒê√¢y l√† continuation c·ªßa document tr∆∞·ªõc (c√≥ th·ªÉ l√† TBT, HDCQ, etc.)
   ‚Üí KH√îNG classify th√†nh UNKNOWN
   ‚Üí GOM V√ÄO document c√≥ trang tr∆∞·ªõc ƒë√≥
   
   V√ç D·ª§ C·ª§ TH·ªÇ:
   Page 4 (index 4): "TH√îNG B√ÅO THU·∫æ" (title) ‚Üí TBT
   Page 5 (index 5): "ƒêI·ªÄU 1: ..." ‚Üí TBT continuation
   Page 6 (index 6): "III. T√çNH THU·∫æ" + b·∫£ng 4.1 ‚Üí TBT continuation ‚úÖ (KH√îNG ph·∫£i UNKNOWN)
   
   Result: {{"type": "TBT", "pages": [4, 5, 6], ...}}

‚úÖ "ƒêI·ªÄU 2: N·ªòI DUNG TH·ªéA THU·∫¨N PH√ÇN CHIA"
   ‚Üí Section header
   ‚Üí Continuation c·ªßa TTHGD ho·∫∑c PCTSVC
   ‚Üí GOM V√ÄO document tr∆∞·ªõc

‚úÖ Trang ch·ªâ c√≥ b·∫£ng bi·ªÉu (kh√¥ng c√≥ title)
   ‚Üí Continuation
   ‚Üí GOM V√ÄO document tr∆∞·ªõc

‚úÖ Trang c√≥ "1.1 Tr∆∞·ªùng h·ª£p...", "1.2 Tr∆∞·ªùng h·ª£p..." (numbered list)
   ‚Üí Continuation v·ªõi structured content
   ‚Üí GOM V√ÄO document tr∆∞·ªõc

‚ùå SAI - Classify continuation th√†nh UNKNOWN:
Page 5: "TH√îNG B√ÅO THU·∫æ" (title)
Page 6: "III. T√çNH THU·∫æ" + b·∫£ng
‚Üí AI classify:
  {{"type": "TBT", "pages": [5]}},
  {{"type": "UNKNOWN", "pages": [6]}}  ‚ùå SAI!
‚Üí ƒê√öNG:
  {{"type": "TBT", "pages": [5, 6]}}  ‚úÖ

RANH GI·ªöI GI·ªÆA C√ÅC T√ÄI LI·ªÜU:
- Thay ƒë·ªïi r√µ r·ªát: m√†u gi·∫•y (h·ªìng ‚Üí tr·∫Øng), format (c√≥ qu·ªëc huy ‚Üí kh√¥ng c√≥)
- Xu·∫•t hi·ªán ti√™u ƒë·ªÅ ch√≠nh m·ªõi ·ªü TOP
- Thay ƒë·ªïi ho√†n to√†n v·ªÅ layout

üéØ D·∫§U HI·ªÜU VISUAL - C√ôNG DOCUMENT (C·ª∞C K·ª≤ QUAN TR·ªåNG):

**1. D·∫§U GI√ÅP LAI (Overlapping Stamp):**
N·∫øu th·∫•y CON D·∫§U ƒê·ªé/H·ªíNG b·ªã C·∫ÆT NGANG qua nhi·ªÅu pages:
- Page 1: C√≥ PH·∫¶N TR√äN c·ªßa con d·∫•u (top half)
- Page 2: C√≥ PH·∫¶N GI·ªÆA c·ªßa con d·∫•u (middle)
- Page 3: C√≥ PH·∫¶N D∆Ø·ªöI c·ªßa con d·∫•u (bottom half)

‚Üí ƒê√¢y l√† D·∫§U GI√ÅP LAI!
‚Üí 3-4 pages n√†y ƒë∆∞·ª£c ƒë√≥ng d·∫•u C√ôNG L√öC (gi·∫•y ch·ªìng l√™n nhau)
‚Üí **B·∫ÆT BU·ªòC C√ôNG 1 DOCUMENT**
‚Üí PH·∫¢I GOM T·∫§T C·∫¢ pages c√≥ partial stamp v√†o 1 document

V√ç D·ª§:
Page 0: "TH·ªéA THU·∫¨N H·ªò GIA ƒê√åNH" + ph·∫ßn tr√™n con d·∫•u ƒë·ªè (‚¨ÜÔ∏è top half)
Page 1: Text body + ph·∫ßn gi·ªØa con d·∫•u ƒë·ªè (‚¨å middle)
Page 2: Text body + ph·∫ßn gi·ªØa con d·∫•u ƒë·ªè (‚¨å middle)
Page 3: "L·ªúI CH·ª®NG..." + ph·∫ßn d∆∞·ªõi con d·∫•u ƒë·ªè (‚¨áÔ∏è bottom half)

‚Üí **4 pages c√≥ c√πng con d·∫•u b·ªã c·∫Øt** ‚Üí C√ôNG 1 DOCUMENT!
‚Üí Result: {{"type": "TTHGD", "pages": [0,1,2,3]}} ‚úÖ

**2. D·∫§U HO√ÄN CH·ªàNH (Complete Stamp):**
N·∫øu page c√≥ con d·∫•u HO√ÄN CH·ªàNH (full circle, kh√¥ng b·ªã c·∫Øt):
‚Üí ƒê√¢y c√≥ th·ªÉ l√† trang ƒê·ªòC L·∫¨P (single document)
‚Üí HO·∫∂C trang cu·ªëi c·ªßa document

**3. KH√îNG C√ì D·∫§U:**
N·∫øu page kh√¥ng c√≥ con d·∫•u:
‚Üí C√≥ th·ªÉ l√† trang gi·ªØa document
‚Üí Check title v√† continuation patterns

üö® NGUY√äN T·∫ÆC D·∫§U GI√ÅP LAI:
- Partial stamp (b·ªã c·∫Øt) = **STRONG SIGNAL** c√πng document
- ∆Øu ti√™n cao h∆°n c·∫£ title/content analysis
- N·∫øu th·∫•y d·∫•u gi√°p lai ‚Üí GOM NGAY, kh√¥ng c·∫ßn nghi ng·ªù

---

""" % (batch_size, batch_size, batch_size)

    unknown_rules = """

‚ö†Ô∏è QUAN TR·ªåNG - KHI N√ÄO TR·∫¢ V·ªÄ "UNKNOWN":
CH·ªà tr·∫£ v·ªÅ "UNKNOWN" khi:
1. Trang th·ª±c s·ª± kh√¥ng c√≥ ti√™u ƒë·ªÅ V√Ä kh√¥ng match continuation patterns
2. Title kh√¥ng thu·ªôc 98 lo·∫°i V√Ä kh√¥ng ph·∫£i continuation
3. Trang ho√†n to√†n tr·ªëng ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c

‚ùå KH√îNG tr·∫£ v·ªÅ "UNKNOWN" cho:
- Trang c√≥ section headers (III., ƒêI·ªÄU 2) ‚Üí Continuation, gom v√†o doc tr∆∞·ªõc
- Trang c√≥ b·∫£ng bi·ªÉu structured ‚Üí Continuation, gom v√†o doc tr∆∞·ªõc
- Trang c√≥ text li√™n t·ª•c v·ªõi trang tr∆∞·ªõc ‚Üí Continuation, gom v√†o doc tr∆∞·ªõc

üéØ NGUY√äN T·∫ÆC: Khi nghi ng·ªù ‚Üí Gom v√†o document tr∆∞·ªõc (safer than creating new UNKNOWN doc)

‚ö†Ô∏è ƒê·∫∂C BI·ªÜT - GOM CONTINUATION THAY V√å TR·∫¢ V·ªÄ UNKNOWN:
N·∫æU trang kh√¥ng c√≥ title NH∆ØNG c√≥ d·∫•u hi·ªáu continuation:
- Section headers: "II.", "III.", "ƒêI·ªÄU X"
- B·∫£ng bi·ªÉu: tables v·ªõi numbers
- Text body: ti·∫øp t·ª•c content

‚Üí KH√îNG t·∫°o document UNKNOWN ri√™ng
‚Üí GOM V√ÄO document tr∆∞·ªõc ƒë√≥
‚Üí Extend "pages" array c·ªßa document tr∆∞·ªõc

V√ç D·ª§ ƒê√öNG:
Page 0: "TH√îNG B√ÅO THU·∫æ" (title) ‚Üí TBT
Page 1: "ƒêI·ªÄU 1: ..." (section) ‚Üí TBT continuation
Page 2: "III. T√çNH THU·∫æ" + b·∫£ng (section + table) ‚Üí TBT continuation

Result: {{"type": "TBT", "pages": [0, 1, 2], ...}} ‚úÖ

KH√îNG L√ÄM:
  {{"type": "TBT", "pages": [0, 1]}},
  {{"type": "UNKNOWN", "pages": [2]}}  ‚ùå

CH·ªà TR·∫¢ V·ªÄ "UNKNOWN" KHI:
- Trang ho√†n to√†n l·∫° (kh√¥ng c√≥ title, kh√¥ng c√≥ continuation patterns)
- Title th·ª±c s·ª± kh√¥ng thu·ªôc 98 lo·∫°i (VD: "B·∫¢N GI·∫¢I TR√åNH", "VƒÇN B·∫¢N Y√äU C·∫¶U")
- Trang tr·ªëng, scan l·ªói, kh√¥ng ƒë·ªçc ƒë∆∞·ª£c

---

"""

    # GCN-specific metadata rules (CRITICAL!)
    gcn_metadata_rules = """

üö® C·ª∞C K·ª≤ QUAN TR·ªåNG - GCN METADATA (B·∫ÆT BU·ªòC):

Khi classify b·∫•t k·ª≥ page n√†o l√† "GCN", b·∫°n PH·∫¢I:

**1. T√åM M√ÄU S·∫ÆC (color):**
- Quan s√°t m√†u n·ªÅn gi·∫•y
- M√†u ƒë·ªè/cam (red/orange) ‚Üí "color": "red"
- M√†u h·ªìng (pink) ‚Üí "color": "pink"  
- M√†u tr·∫Øng ho·∫∑c kh√¥ng r√µ ‚Üí "color": "unknown"

**2. T√åM NG√ÄY C·∫§P (issue_date) - B·∫ÆT BU·ªòC:**
- ‚ö†Ô∏è KH√îNG BAO GI·ªú b·ªè qua b∆∞·ªõc n√†y!
- T√¨m ·ªü trang 2 (n·∫øu GCN A3) ho·∫∑c trang 1 bottom
- Text g·∫ßn: "Ng√†y c·∫•p", "C·∫•p ng√†y", "TM. UBND", ch·ªØ k√Ω
- C√≥ th·ªÉ vi·∫øt TAY (handwritten)

**Formats ph·ªï bi·∫øn:**
- "DD/MM/YYYY" ‚Üí Return: "27/10/2021"
- "Ng√†y 25 th√°ng 8 nƒÉm 2010" ‚Üí Convert & return: "25/8/2010"
- "MM/YYYY" (n·∫øu m·ªù) ‚Üí Return: "02/2012"
- "YYYY" (n·∫øu r·∫•t m·ªù) ‚Üí Return: "2012"
- Kh√¥ng t√¨m th·∫•y ‚Üí Return: null

**Confidence levels:**
- "full": ƒê·ªçc ƒë∆∞·ª£c ƒë·∫ßy ƒë·ªß DD/MM/YYYY
- "partial": Ch·ªâ MM/YYYY
- "year_only": Ch·ªâ YYYY
- "not_found": Kh√¥ng t√¨m th·∫•y

**3. METADATA RESPONSE - B·∫ÆT BU·ªòC:**

‚úÖ V√ç D·ª§ ƒê√öNG (C√≥ ng√†y c·∫•p):
{{
  "type": "GCN",
  "pages": [5, 6],
  "confidence": 0.98,
  "reasoning": "GCN m√†u h·ªìng, qu·ªëc huy, ng√†y c·∫•p 27/10/2021",
  "metadata": {{
    "color": "pink",
    "issue_date": "27/10/2021",
    "issue_date_confidence": "full"
  }}
}}

‚úÖ V√ç D·ª§ ƒê√öNG (Kh√¥ng t√¨m th·∫•y date):
{{
  "type": "GCN",
  "pages": [0],
  "confidence": 0.95,
  "reasoning": "GCN trang 1, m√†u h·ªìng, ch∆∞a c√≥ ng√†y c·∫•p",
  "metadata": {{
    "color": "pink",
    "issue_date": null,
    "issue_date_confidence": "not_found"
  }}
}}

‚ùå SAI - THI·∫æU METADATA:
{{
  "type": "GCN",
  "pages": [5, 6],
  "metadata": {{}}  // ‚ùå EMPTY! Must have color & issue_date!
}}

‚ùå SAI - KH√îNG T√åM DATE:
{{
  "type": "GCN",
  "pages": [5, 6],
  "metadata": {{
    "color": "pink"
    // ‚ùå MISSING issue_date fields!
  }}
}}

‚ö†Ô∏è NH·ªö: M·ªçi GCN document PH·∫¢I c√≥ metadata v·ªõi:
- "color": "red" | "pink" | "unknown"
- "issue_date": "DD/MM/YYYY" | null
- "issue_date_confidence": "full" | "partial" | "year_only" | "not_found"

---

"""

    output_format = f"""

---

üéØ OUTPUT FORMAT - B·∫ÆT BU·ªòC:

{{
  "documents": [
    {{
      "type": "HDCQ",
      "pages": [0, 1, 2, 3, 4],
      "confidence": 0.95,
      "reasoning": "5 trang ƒë·∫ßu c√πng format, trang 0 c√≥ ti√™u ƒë·ªÅ 'H·ª¢P ƒê·ªíNG CHUY·ªÇN NH∆Ø·ª¢NG', trang 1-4 l√† continuation pages v·ªõi ƒêI·ªÄU 2, ƒêI·ªÄU 3",
      "metadata": {{}}
    }},
    {{
      "type": "GCN",
      "pages": [5, 6],
      "confidence": 0.98,
      "reasoning": "Trang 5-6 l√† GCN m√†u h·ªìng, c√≥ qu·ªëc huy, t√¨m th·∫•y ng√†y c·∫•p ·ªü trang 6",
      "metadata": {{
        "color": "pink",
        "issue_date": "27/10/2021",
        "issue_date_confidence": "full"
      }}
    }},
    {{
      "type": "UNKNOWN",
      "pages": [7, 8, 9],
      "confidence": 0.3,
      "reasoning": "3 trang cu·ªëi kh√¥ng r√µ r√†ng, kh√¥ng c√≥ ti√™u ƒë·ªÅ, kh√¥ng match 98 lo·∫°i",
      "metadata": {{}}
    }}
  ]
}}

üö® C·ª∞C K·ª≤ QUAN TR·ªåNG - B·∫ÆT BU·ªòC RETURN T·∫§T C·∫¢ {batch_size} PAGES:
- B·∫°n PH·∫¢I assign M·ªåI page (0 ƒë·∫øn {batch_size-1}) v√†o 1 document
- N·∫øu page kh√¥ng r√µ ‚Üí assign v√†o document type "UNKNOWN"
- KH√îNG BAO GI·ªú b·ªè qua page n√†o
- T·ªïng s·ªë pages trong "pages" arrays = {batch_size}

V√ç D·ª§ ƒê√öNG ({batch_size} pages):
- Document 1: pages [0,1,2,3,4] (5 pages)
- Document 2: pages [5,6,7,8] (4 pages)
- Document 3: pages [9,10,...,{batch_size-1}] ({batch_size-9} pages)
‚Üí Total: {batch_size} pages ‚úÖ

V√ç D·ª§ SAI:
- Document 1: pages [0,1,2] (3 pages only)
- Document 2: pages [5,6] (2 pages, SKIP pages 3-4!)
‚Üí Total: 5 pages ‚ùå (Missing pages 3,4,7,8,...,{batch_size-1})

INDEXING:
- pages d√πng 0-indexed (trang ƒë·∫ßu ti√™n = 0, trang cu·ªëi = {batch_size-1})
- N·∫øu ch·ªâ c√≥ 1 document ‚Üí v·∫´n tr·∫£ v·ªÅ array v·ªõi 1 ph·∫ßn t·ª≠
"""

    # Combine: intro + original rules + unknown rules + GCN metadata + output format
    full_multi_prompt = multi_image_intro + single_image_prompt + unknown_rules + gcn_metadata_rules + output_format
    
    return full_multi_prompt


def get_multi_image_prompt_full(batch_size):
    """Get FULL prompt (Flash Full rules) for multi-image batch"""
    single_prompt = get_classification_prompt()
    return adapt_prompt_for_multi_image(single_prompt, batch_size)


def get_multi_image_prompt_lite(batch_size):
    """Get LITE prompt (Flash Lite rules) for multi-image batch"""
    single_prompt = get_classification_prompt_lite()
    return adapt_prompt_for_multi_image(single_prompt, batch_size)


def encode_image_base64(image_path, max_width=1500, max_height=2100):
    """Encode image to base64 with smart resize"""
    try:
        img = Image.open(image_path)
        
        # Convert to RGB if needed
        if img.mode in ('RGBA', 'LA', 'P'):
            img = img.convert('RGB')
        
        # Resize
        resized_img, resize_info = resize_image_smart(img, max_width, max_height)
        
        # Encode to base64
        buffer = io.BytesIO()
        resized_img.save(buffer, format='JPEG', quality=95)
        img_bytes = buffer.getvalue()
        encoded = base64.b64encode(img_bytes).decode('utf-8')
        
        return encoded, resize_info
    except Exception as e:
        print(f"Error encoding image {image_path}: {e}", file=sys.stderr)
        return None, None




def batch_classify_fixed(image_paths, api_key, engine_type='gemini-flash', batch_size=5, last_known_type=None):
    """
    Ph∆∞∆°ng √°n 1: Fixed Batch Size v·ªõi SEQUENTIAL METADATA
    
    Args:
        image_paths: List of file paths
        api_key: Google API key
        engine_type: 'gemini-flash', 'gemini-flash-lite', or 'gemini-flash-hybrid'
        batch_size: Files per batch
        last_known_type: Metadata t·ª´ file cu·ªëi batch tr∆∞·ªõc {short_code, confidence, has_title}
    
    Strategy:
        - Batch 1: Process files 0-4, return lastKnown t·ª´ file 4
        - Batch 2: Process files 5-9 WITH lastKnown t·ª´ file 4
          * File 5 c√≥ title ‚Üí B·ªè qua lastKnown, d√πng title m·ªõi
          * File 5 kh√¥ng c√≥ title ‚Üí √Åp d·ª•ng sequential t·ª´ lastKnown
        - No overlap needed ‚Üí 0% overhead!
    """
    
    # Determine model and prompt based on engine type
    if engine_type == 'gemini-flash-lite':
        model_name = 'gemini-2.5-flash-lite'
        prompt_getter = get_multi_image_prompt_lite
        print(f"\n{'='*80}", file=sys.stderr)
        print(f"‚ö° BATCH MODE: Fixed ({batch_size} files, NO overlap) + Flash LITE", file=sys.stderr)
        print(f"   Model: {model_name}", file=sys.stderr)
        print("   Prompt: Lite (simplified, 60% crop rules)", file=sys.stderr)
        print("   Metadata: Sequential naming from previous batch", file=sys.stderr)
        print(f"{'='*80}", file=sys.stderr)
    elif engine_type == 'gemini-flash-hybrid':
        model_name = 'gemini-2.5-flash-lite'  # Start with Lite for hybrid
        prompt_getter = get_multi_image_prompt_lite
        print(f"\n{'='*80}", file=sys.stderr)
        print(f"üîÑ BATCH MODE: Fixed ({batch_size} files, NO overlap) + HYBRID", file=sys.stderr)
        print("   Strategy: Two-tier (Lite ‚Üí Full if low confidence)", file=sys.stderr)
        print(f"   Model (Tier 1): {model_name}", file=sys.stderr)
        print("   Metadata: Sequential naming from previous batch", file=sys.stderr)
        print(f"{'='*80}", file=sys.stderr)
    else:  # gemini-flash (default)
        model_name = 'gemini-2.5-flash'
        prompt_getter = get_multi_image_prompt_full
        print(f"\n{'='*80}", file=sys.stderr)
        print(f"ü§ñ BATCH MODE: Fixed ({batch_size} files, NO overlap) + Flash FULL", file=sys.stderr)
        print(f"   Model: {model_name}", file=sys.stderr)
        print("   Prompt: Full (complete 98-rule classification)", file=sys.stderr)
        print("   Metadata: Sequential naming from previous batch", file=sys.stderr)
        print(f"{'='*80}", file=sys.stderr)
    
    if last_known_type:
        print(f"\nüìå Received lastKnown from previous batch:", file=sys.stderr)
        print(f"   Type: {last_known_type.get('short_code')}", file=sys.stderr)
        print(f"   Confidence: {last_known_type.get('confidence', 0):.0%}", file=sys.stderr)
        print(f"   Has title: {last_known_type.get('has_title', False)}", file=sys.stderr)
    
    all_results = []
    processed_files = set()
    batch_num = 0
    current_idx = 0
    current_last_known = last_known_type  # Start with provided lastKnown
    
    while current_idx < len(image_paths):
        batch_num += 1
        
        # Calculate batch range WITHOUT overlap
        batch_start = current_idx
        batch_end = min(len(image_paths), current_idx + batch_size)
        batch_paths = image_paths[batch_start:batch_end]
        
        print(f"\nüì¶ Batch {batch_num}: Files {batch_start}-{batch_end-1} ({len(batch_paths)} images)", file=sys.stderr)
        
        for i, path in enumerate(batch_paths):
            print(f"   [{i}] {os.path.basename(path)}", file=sys.stderr)
        
        # Encode all images in batch
        print(f"üñºÔ∏è Encoding {len(batch_paths)} images...", file=sys.stderr)
        encoded_images = []
        for path in batch_paths:
            encoded, resize_info = encode_image_base64(path)
            if encoded:
                encoded_images.append(encoded)
                print(f"   ‚úÖ {os.path.basename(path)}: {resize_info.get('original_size', 'N/A')} ‚Üí {resize_info.get('new_size', 'N/A')}", file=sys.stderr)
            else:
                print(f"   ‚ùå Failed to encode {os.path.basename(path)}", file=sys.stderr)
        
        if not encoded_images:
            print(f"‚ùå No valid images in batch {batch_num}", file=sys.stderr)
            continue
        
        # Build multi-image payload
        parts = [{"text": prompt_getter(len(batch_paths))}]
        for img_data in encoded_images:
            parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": img_data
                }
            })
        
        payload = {
            "contents": [{"parts": parts}],
            "generationConfig": {
                "temperature": 0.1,
                "topP": 0.8,
                "topK": 10,
                "maxOutputTokens": 8000,  # Large enough for 20 documents √ó 400 tokens each
                "responseMimeType": "application/json"  # Force JSON output
            },
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"}
            ]
        }
        
        # Call Gemini API with retry logic
        print(f"üì° Sending batch request to {model_name}...", file=sys.stderr)
        print(f"   Batch size: {len(batch_paths)} files", file=sys.stderr)
        # Calculate approximate request size
        import json
        payload_size_mb = len(json.dumps(payload)) / (1024 * 1024)
        print(f"   Request size: ~{payload_size_mb:.2f} MB", file=sys.stderr)
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}"
        
        max_retries = 3
        retry_delay = 10  # seconds
        
        for attempt in range(max_retries):
            try:
                response = requests.post(api_url, json=payload, timeout=120)
                response.raise_for_status()
                result_data = response.json()
                break  # Success, exit retry loop
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code in [500, 503]:
                    # 500 Internal Server Error or 503 Service Unavailable - retry
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                        print(f"‚ö†Ô∏è {e.response.status_code} Server Error, retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                        print(f"   Possible causes: Request too large, API overload, temporary issue", file=sys.stderr)
                        if batch_size > 5:
                            print(f"   üí° Tip: Try reducing Smart batch size to 5-8 in Settings", file=sys.stderr)
                        import time
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"‚ùå Max retries reached for batch {batch_num}", file=sys.stderr)
                        print(f"   Batch size: {batch_size} files", file=sys.stderr)
                        print(f"   üí° Recommendation: Reduce Smart batch size in Settings (‚öôÔ∏è C√†i ƒë·∫∑t)", file=sys.stderr)
                        raise
                elif e.response.status_code == 429:
                    # Rate limit - longer wait
                    if attempt < max_retries - 1:
                        wait_time = 60 * (2 ** attempt)  # Start with 60s
                        print(f"‚ö†Ô∏è 429 Rate Limit, retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                        import time
                        time.sleep(wait_time)
                        continue
                    else:
                        raise
                else:
                    # Other HTTP errors (400, 401, 404, etc.) - don't retry
                    print(f"‚ùå HTTP {e.response.status_code} Error: {e}", file=sys.stderr)
                    raise
            except requests.exceptions.RequestException as e:
                # Network errors - retry
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    print(f"‚ö†Ô∏è Network error, retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                    import time
                    time.sleep(wait_time)
                    continue
                else:
                    raise
        
        # Add delay between batches to avoid rate limiting
        if batch_num < ((len(image_paths) + batch_size - 1) // batch_size):
            import time
            inter_batch_delay = 5  # 5 seconds between batches
            print(f"‚è∏Ô∏è Waiting {inter_batch_delay}s before next batch...", file=sys.stderr)
            time.sleep(inter_batch_delay)
        
        try:
            
            print(f"üìä Response status: {response.status_code}", file=sys.stderr)
            
            # Debug: Check finish reason and safety
            if 'candidates' in result_data and len(result_data['candidates']) > 0:
                candidate = result_data['candidates'][0]
                finish_reason = candidate.get('finishReason', 'UNKNOWN')
                print(f"üîç Finish reason: {finish_reason}", file=sys.stderr)
                
                if finish_reason == 'MAX_TOKENS':
                    print("‚ö†Ô∏è WARNING: Response truncated due to MAX_TOKENS!", file=sys.stderr)
                    print("   Some pages may be missing from response", file=sys.stderr)
            
            # Parse response
            if 'candidates' in result_data and len(result_data['candidates']) > 0:
                candidate = result_data['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    parts = candidate['content']['parts']
                    if len(parts) > 0 and 'text' in parts[0]:
                        response_text = parts[0]['text']
                        
                        print(f"üìÑ Raw response preview: {response_text[:200]}...", file=sys.stderr)
                        
                        # DEBUG: Log full response for GCN documents
                        if '"type": "GCN"' in response_text or '"GCN"' in response_text:
                            print(f"\nüîç DEBUG - GCN DETECTED in response!", file=sys.stderr)
                            print(f"üìÑ Full JSON response:", file=sys.stderr)
                            print(response_text, file=sys.stderr)
                            print(f"\n", file=sys.stderr)
                        
                        # Extract JSON from response - try multiple patterns
                        json_match = re.search(r'\{[\s\S]*"documents"[\s\S]*\}', response_text)
                        if not json_match:
                            # Try finding JSON with triple backticks
                            json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', response_text)
                            if json_match:
                                response_text = json_match.group(1)
                            else:
                                # Try finding any JSON object
                                json_match = re.search(r'(\{[\s\S]*\})', response_text)
                                if json_match:
                                    response_text = json_match.group(1)
                        else:
                            response_text = json_match.group(0)
                        
                        if response_text:
                            try:
                                batch_result = json.loads(response_text)
                            
                                print(f"‚úÖ Batch {batch_num} complete:", file=sys.stderr)
                                
                                # Validate: Check if all pages are covered
                                total_pages_in_batch = len(batch_paths)
                                pages_returned = set()
                                
                                for doc in batch_result.get('documents', []):
                                    doc_type = doc.get('type', 'UNKNOWN')
                                    pages = doc.get('pages', [])
                                    confidence = doc.get('confidence', 0)
                                    print(f"   üìÑ {doc_type}: {len(pages)} pages, confidence {confidence:.0%}", file=sys.stderr)
                                    
                                    # Collect all page indices
                                    for p in pages:
                                        pages_returned.add(p)
                                
                                # Check for missing pages
                                expected_pages = set(range(total_pages_in_batch))
                                missing_pages = expected_pages - pages_returned
                                
                                if missing_pages:
                                    print(f"   ‚ö†Ô∏è WARNING: AI didn't return {len(missing_pages)} pages: {sorted(missing_pages)}", file=sys.stderr)
                                    print("      These files will be processed by fallback", file=sys.stderr)
                                else:
                                    print(f"   ‚úÖ All {total_pages_in_batch} pages accounted for", file=sys.stderr)
                                
                                # Map results back to original file paths WITH sequential naming
                                batch_results_with_sequential = []
                                
                                for doc in batch_result.get('documents', []):
                                    doc_type = doc.get('type', 'UNKNOWN')
                                    doc_confidence = doc.get('confidence', 0.5)
                                    doc_reasoning = doc.get('reasoning', '')
                                    doc_metadata = doc.get('metadata', {})
                                    
                                    # DEBUG: Log metadata for GCN
                                    if doc_type == 'GCN':
                                        print(f"\nüîç DEBUG - GCN Metadata:", file=sys.stderr)
                                        print(f"   Type: {doc_type}", file=sys.stderr)
                                        print(f"   Metadata: {doc_metadata}", file=sys.stderr)
                                        print(f"   Has color: {'color' in doc_metadata}", file=sys.stderr)
                                        print(f"   Has issue_date: {'issue_date' in doc_metadata}", file=sys.stderr)
                                        if doc_metadata:
                                            print(f"   color value: {doc_metadata.get('color', 'MISSING')}", file=sys.stderr)
                                            print(f"   issue_date value: {doc_metadata.get('issue_date', 'MISSING')}", file=sys.stderr)
                                        print(f"\n", file=sys.stderr)
                                    
                                    for page_idx in doc.get('pages', []):
                                        if page_idx < len(batch_paths):
                                            file_path = batch_paths[page_idx]
                                            file_name = os.path.basename(file_path)
                                            
                                            # Determine if this file has title (high confidence, not UNKNOWN)
                                            has_title = (doc_confidence >= 0.8 and doc_type != 'UNKNOWN')
                                            
                                            # Apply sequential naming logic
                                            final_type = doc_type
                                            final_confidence = doc_confidence
                                            applied_sequential = False
                                            
                                            # If file is UNKNOWN or low confidence AND we have lastKnown
                                            if (doc_type == 'UNKNOWN' or doc_confidence < 0.5) and current_last_known:
                                                final_type = current_last_known['short_code']
                                                final_confidence = current_last_known['confidence']
                                                applied_sequential = True
                                                print(f"   üîÑ Sequential: {file_name} ({doc_type} {doc_confidence:.0%}) ‚Üí {final_type}", file=sys.stderr)
                                            
                                            # Update lastKnown if this file has good classification
                                            if doc_type != 'UNKNOWN' and doc_confidence >= 0.7 and has_title:
                                                current_last_known = {
                                                    'short_code': doc_type,
                                                    'confidence': doc_confidence,
                                                    'has_title': True
                                                }
                                                print(f"   üìå Updated lastKnown: {doc_type} ({doc_confidence:.0%})", file=sys.stderr)
                                            
                                            processed_files.add(file_path)
                                            batch_results_with_sequential.append({
                                                'file_path': file_path,
                                                'file_name': file_name,
                                                'short_code': final_type,
                                                'confidence': final_confidence,
                                                'reasoning': doc_reasoning,
                                                'metadata': doc_metadata,
                                                'method': 'batch_fixed',
                                                'batch_num': batch_num,
                                                'applied_sequential': applied_sequential,
                                                'original_classification': doc_type if applied_sequential else None
                                            })
                                
                                all_results.extend(batch_results_with_sequential)
                            except json.JSONDecodeError as je:
                                print(f"‚ö†Ô∏è JSON decode error in batch {batch_num}: {je}", file=sys.stderr)
                                print(f"   Response text: {response_text[:500]}...", file=sys.stderr)
                        else:
                            print(f"‚ö†Ô∏è No valid JSON in response for batch {batch_num}", file=sys.stderr)
            
        except Exception as e:
            print(f"‚ùå Batch {batch_num} error: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
        
        # Move to next batch (increment by batch_size, not batch_end)
        current_idx += batch_size
    
    print(f"\n{'='*80}", file=sys.stderr)
    print(f"‚úÖ BATCH MODE 1 COMPLETE: {len(all_results)} files processed", file=sys.stderr)
    
    # Detect missing files
    all_input_files = set(image_paths)
    missing_files = all_input_files - processed_files
    
    if missing_files:
        print(f"‚ö†Ô∏è WARNING: {len(missing_files)} files were NOT processed by AI:", file=sys.stderr)
        for missing_file in sorted(missing_files):
            print(f"   ‚ùå {os.path.basename(missing_file)}", file=sys.stderr)
        print("   Possible causes: AI didn't return page indices, JSON parsing error", file=sys.stderr)
        print(f"\nüîÑ FALLBACK: Processing {len(missing_files)} missing files individually...", file=sys.stderr)
        
        # Fallback: Process missing files with single-file tier1 scan
        for missing_file in sorted(missing_files):
            try:
                print(f"   üîÑ Processing {os.path.basename(missing_file)}...", file=sys.stderr)
                result = quick_scan_tier1(missing_file, api_key)
                all_results.append({
                    'file_path': missing_file,
                    'file_name': os.path.basename(missing_file),
                    'short_code': result.get('short_code', 'UNKNOWN'),
                    'confidence': result.get('confidence', 0.5),
                    'reasoning': result.get('reasoning', 'Fallback single-file scan'),
                    'metadata': result.get('metadata', {}),
                    'method': 'batch_fallback',
                    'batch_num': 'fallback'
                })
                print(f"      ‚úÖ {result.get('short_code', 'UNKNOWN')} ({result.get('confidence', 0):.0%})", file=sys.stderr)
            except Exception as e:
                print(f"      ‚ùå Error: {e}", file=sys.stderr)
                # Add as UNKNOWN if fallback also fails
                all_results.append({
                    'file_path': missing_file,
                    'file_name': os.path.basename(missing_file),
                    'short_code': 'UNKNOWN',
                    'confidence': 0.0,
                    'reasoning': f'Fallback failed: {str(e)}',
                    'metadata': {},
                    'method': 'batch_fallback_failed',
                    'batch_num': 'fallback'
                })
        
        print(f"‚úÖ Fallback complete: {len(all_results)} total results (original + fallback)", file=sys.stderr)
    else:
        print(f"‚úÖ All {len(all_input_files)} input files were successfully processed", file=sys.stderr)
    
    print(f"{'='*80}", file=sys.stderr)
    
    # Return results AND lastKnown for next batch
    return {
        'results': all_results,
        'last_known_type': current_last_known
    }


def quick_scan_tier1(image_path, api_key):
    """Quick scan v·ªõi Tier 1 ƒë·ªÉ detect document boundaries"""
    from ocr_engine_gemini_flash import classify_document_gemini_flash
    
    try:
        result = classify_document_gemini_flash(
            image_path=image_path,
            api_key=api_key,
            crop_top_percent=0.60,
            model_type='gemini-flash-lite',
            enable_resize=True,
            max_width=1500,
            max_height=2100
        )
        return result
    except Exception as e:
        print(f"Quick scan error for {image_path}: {e}", file=sys.stderr)
        return {'short_code': 'ERROR', 'confidence': 0}


def group_by_document(quick_results, file_paths):
    """
    Nh√≥m files th√†nh documents d·ª±a tr√™n quick scan results
    Returns: List of document groups [[0,1,2], [3,4], [5,6,7,8], ...]
    """
    print("\nüß† Analyzing document boundaries...", file=sys.stderr)
    
    groups = []
    current_group = [0]
    last_type = quick_results[0].get('short_code', 'UNKNOWN')
    
    for i in range(1, len(quick_results)):
        result = quick_results[i]
        short_code = result.get('short_code', 'UNKNOWN')
        confidence = result.get('confidence', 0)
        reasoning = result.get('reasoning', '').lower()
        
        # Check if this is a new document
        is_new_document = False
        
        # High confidence with clear title ‚Üí New document
        if confidence >= 0.8 and short_code != 'UNKNOWN':
            is_new_document = True
            print(f"   üìÑ [{i}] New document detected: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Low confidence + continuation indicators ‚Üí Same document
        elif confidence < 0.5 and any(kw in reasoning for kw in ['section header', 'ii.', 'iii.', 'th·ª≠a ƒë·∫•t']):
            is_new_document = False
            print(f"   ‚û°Ô∏è [{i}] Continuation page: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Borderline case - use confidence
        else:
            is_new_document = (confidence >= 0.7)
            print(f"   ‚ùì [{i}] Borderline: {short_code} ({confidence:.0%}) ‚Üí {'New' if is_new_document else 'Continue'}", file=sys.stderr)
        
        if is_new_document:
            # Start new group
            groups.append(current_group)
            current_group = [i]
            last_type = short_code
        else:
            # Continue current group
            current_group.append(i)
    
    # Add last group
    if current_group:
        groups.append(current_group)
    
    print(f"\n‚úÖ Grouped into {len(groups)} documents:", file=sys.stderr)
    for g_idx, group in enumerate(groups):
        print(f"   Document {g_idx + 1}: {len(group)} pages {group}", file=sys.stderr)
    
    return groups


def batch_classify_smart(image_paths, api_key, engine_type='gemini-flash', last_known_type=None, max_batch_size=15):
    """
    Ph∆∞∆°ng √°n 2: Smart Batching v·ªõi SEQUENTIAL METADATA
    
    Args:
        max_batch_size: Maximum files per batch (default 15, can be reduced if needed)
    """
    print(f"\n{'='*80}", file=sys.stderr)
    print("üß† BATCH MODE 2: Smart Batching (AI Document Detection)", file=sys.stderr)
    print(f"{'='*80}", file=sys.stderr)
    
    total_files = len(image_paths)
    
    # Smart batch size strategy WITH user-configurable max
    if total_files <= max_batch_size:
        batch_size = total_files
        print(f"üìä Strategy: Send ALL {total_files} files in 1 batch (max={max_batch_size})", file=sys.stderr)
    else:
        batch_size = max_batch_size
        print(f"üìä Strategy: Send {batch_size} files per batch (user configured max={max_batch_size})", file=sys.stderr)
    
    print("   Sequential metadata: Pass lastKnown between batches (0% overhead)", file=sys.stderr)
    
    # Use fixed batch with smart size + sequential metadata
    return batch_classify_fixed(image_paths, api_key, engine_type=engine_type, batch_size=batch_size, last_known_type=last_known_type)


# CLI interface for testing
if __name__ == "__main__":
    if len(sys.argv) < 4:
        print("Usage: python batch_processor.py <mode> <engine_type> <api_key> <image1> <image2> ...", file=sys.stderr)
        print("Modes: fixed, smart", file=sys.stderr)
        print("Engine types: gemini-flash, gemini-flash-lite, gemini-flash-hybrid", file=sys.stderr)
        print("Example: python batch_processor.py smart gemini-flash-hybrid AIza... img1.jpg img2.jpg img3.jpg", file=sys.stderr)
        sys.exit(1)
    
    mode = sys.argv[1]
    engine_type = sys.argv[2]
    api_key = sys.argv[3]
    image_paths = sys.argv[4:]
    
    print(f"üîç Batch processing {len(image_paths)} images in '{mode}' mode with '{engine_type}'", file=sys.stderr)
    
    if mode == 'fixed':
        batch_data = batch_classify_fixed(image_paths, api_key, engine_type=engine_type, batch_size=5, last_known_type=None)
    elif mode == 'smart':
        # Check for optional max_batch_size env variable
        env_value = os.environ.get('SMART_MAX_BATCH_SIZE', '10')
        print(f"üîç DEBUG: SMART_MAX_BATCH_SIZE env = '{env_value}'", file=sys.stderr)
        max_batch_size = int(env_value)
        print(f"üìä Smart mode max_batch_size: {max_batch_size}", file=sys.stderr)
        batch_data = batch_classify_smart(image_paths, api_key, engine_type=engine_type, last_known_type=None, max_batch_size=max_batch_size)
    else:
        print(f"‚ùå Unknown mode: {mode}", file=sys.stderr)
        sys.exit(1)
    
    # Extract results
    results = batch_data['results'] if isinstance(batch_data, dict) else batch_data
    
    # Output JSON to stdout for IPC
    print(json.dumps(results, ensure_ascii=False))
    
    print(f"\nüìä BATCH COMPLETE: {len(results)} files processed", file=sys.stderr)
