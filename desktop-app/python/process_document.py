#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Standalone document processor for desktop app
Combines OCR + Rule-based classification for offline processing
"""
import sys
import json
import os
from pathlib import Path
import warnings

# Force UTF-8 encoding BEFORE any other imports
import io

# Reconfigure stdout/stderr for UTF-8
if sys.platform == 'win32':
    try:
        # Wrap binary buffers with UTF-8 text wrappers
        if hasattr(sys.stdout, 'buffer'):
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
        if hasattr(sys.stderr, 'buffer'):
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except Exception:
        # Fallback - already wrapped or other issue
        pass

# Suppress warnings
warnings.filterwarnings('ignore')
os.environ['GLOG_minloglevel'] = '2'
os.environ['FLAGS_use_mkldnn'] = '0'

# Add current directory to path
sys.path.insert(0, os.path.dirname(__file__))

# Import rule classifier (always needed)
from rule_classifier import RuleClassifier

# Lazy import OCR engines only when needed
# This allows Google/Azure to work even if Tesseract dependencies are missing
tesseract_engine = None
vietocr_engine = None
easyocr_engine = None


def extract_document_title_from_text(text: str) -> str:
    """
    Extract document title from OCR text using common patterns
    """
    import re

    title_patterns = [
        # GI·∫§Y TI·∫æP NH·∫¨N H·ªí S∆† V√Ä H·∫∏N TR·∫¢ K·∫æT QU·∫¢ (GTLQ)
        r'(GI[A√Å·∫§]Y\s+TI[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]P\s+NH[·∫¨AƒÇ√Ç√Å√Ä√É·∫†√Ç·∫§ƒÇ·∫Æ]N\s+H[O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢][\s]*S[O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]|GI[A√Å·∫§]Y\s+TI[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]P\s+NH[·∫¨AƒÇ√Ç√Å√Ä√É·∫†√Ç·∫§ƒÇ·∫Æ]N\s+H·ªé\s*S∆†)\s+V[√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥]\s+H·∫∏N\s+TR·∫¢\s+K[√äE]T\s+QU·∫¢',
        # ƒê∆†N ƒêƒÇNG K√ù BI·∫æN ƒê·ªòNG
        r'(ƒê[O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]N\s+[ƒêD][AƒÇ]NG\s+K[Y√ù]\s+BI[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]N\s+[ƒêD][O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]NG(?:\s+[ƒêD][A√Å·∫§]T\s+[ƒêD]AI)?(?:\s*,?\s*T[A√Ä]I\s+S[A·∫¢]N)?(?:\s+G[A·∫Æ]N\s+LI[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]N\s+V[O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]I\s+[ƒêD][A√Å·∫§]T)?)',
        # H·ª¢P ƒê·ªíNG CHUY·ªÇN NH∆Ø·ª¢NG
        r'(H[O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]P\s+[ƒêD][O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]NG\s+CHUY[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]N\s+NH[U∆Ø√ö√ô·ª¶≈®·ª§·ª®·ª™·ª¨·ªÆ·ª∞][O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]NG(?:\s+QUY[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]N)?(?:\s+S[U∆Ø√ö√ô·ª¶≈®·ª§·ª®·ª™·ª¨·ªÆ·ª∞]\s+D[U·ª§]NG\s+[ƒêD][A√Å·∫§]T)?)',
        # H·ª¢P ƒê·ªíNG ·ª¶Y QUY·ªÄN
        r'(H[O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]P\s+[ƒêD][O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]NG\s+(?:[U·ª¶][\s·ª∂]*Y|U[·ª∂Y])\s+QUY[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]N)',
        # GI·∫§Y CH·ª®NG NH·∫¨N QUY·ªÄN S·ª¨ D·ª§NG ƒê·∫§T
        r'(GI[A√Å·∫§]Y\s+CH[U∆Ø√ö√ô·ª¶≈®·ª§·ª®·ª™·ª¨·ªÆ·ª∞]NG\s+NH[A·∫¨]N\s+QUY[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]N\s+S[U∆Ø√ö√ô·ª¶≈®·ª§·ª®·ª™·ª¨·ªÆ·ª∞]\s+D[U·ª§]NG\s+[ƒêD][A√Å·∫§]T)',
        # GI·∫§Y ·ª¶Y QUY·ªÄN
        r'(GI[A√Å·∫§]Y\s+(?:[U·ª¶][\s·ª∂]*Y|U[·ª∂Y])\s+QUY[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]N)',
        # QUY·∫æT ƒê·ªäNH (kh√°i qu√°t)
        r'(QUY[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]T\s+[ƒêD][I·ªä]NH(?:\s+[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥ƒê√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏]{1,30})?)',
        # ƒê∆†N XIN
        r'(ƒê[O√î∆†√ì√í·ªé√ï·ªå·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢]N\s+XIN(?:\s+[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥ƒê√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏]{1,30})?)',
        # BI√äN B·∫¢N
        r'(BI[E√ä√â√à·∫æ·ªÄ·ªÇ·ªÑ·ªÜ]N\s+B[A·∫¢]N(?:\s+[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥ƒê√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏]{1,30})?)',
    ]

    for pattern in title_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            title = match.group(1).strip()
            # Keep only the uppercase title part
            title = re.sub(r'\s+[a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µƒë√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπ].*$', '', title)
            if title and len(title) < 200:
                return title
    return ""


def process_document(file_path: str, ocr_engine_type: str = 'tesseract', cloud_api_key: str = None, cloud_endpoint: str = None) -> dict:
    """
    Process a document using OCR + Rules with font height detection
    """
    try:
        # Handle Gemini Flash Hybrid (Two-Tier AI classification)
        if ocr_engine_type == 'gemini-flash-hybrid':
            if not cloud_api_key:
                return {
                    "success": False,
                    "error": "Google API key is required for Gemini Flash Hybrid",
                    "method": "config_error"
                }

            print(f"üîÑ Using Gemini Flash HYBRID (Two-Tier) classification", file=sys.stderr)

            from ocr_engine_gemini_flash_hybrid import classify_document_gemini_flash_hybrid
            from rule_classifier import classify_document_name_from_code
            import time

            # Get resize settings from environment (set by Electron)
            enable_resize = os.environ.get('ENABLE_RESIZE', 'true').lower() == 'true'
            max_width = int(os.environ.get('MAX_WIDTH', '1500'))
            max_height = int(os.environ.get('MAX_HEIGHT', '2100'))
            
            # Get confidence threshold from environment (default: 0.80)
            confidence_threshold = float(os.environ.get('HYBRID_CONFIDENCE_THRESHOLD', '0.80'))

            print(f"üì∏ Two-Tier strategy:", file=sys.stderr)
            print(f"   ‚îú‚îÄ Tier 1: Flash Lite (60% crop) for easy documents", file=sys.stderr)
            print(f"   ‚îú‚îÄ Tier 2: Flash Full (100% image) if confidence < {confidence_threshold:.0%} or complex doc", file=sys.stderr)
            print(f"   ‚îî‚îÄ Smart resize: max {max_width}x{max_height}px", file=sys.stderr)
            
            start_time = time.time()

            # Call hybrid engine
            result = classify_document_gemini_flash_hybrid(
                image_path=file_path, 
                api_key=cloud_api_key, 
                confidence_threshold=confidence_threshold,
                complex_doc_types=['GCN', 'GCNM', 'GCNC'],
                enable_resize=enable_resize,
                max_width=max_width,
                max_height=max_height
            )

            scan_time = time.time() - start_time
            tier_used = result.get('tier_used', 'unknown')
            print(f"‚è±Ô∏è Result: {result.get('short_code')} (confidence: {result.get('confidence'):.2f}, tier: {tier_used}, time: {scan_time:.1f}s)", file=sys.stderr)
            
            method_used = "gemini_hybrid_two_tier"
            
            # Check for errors
            if result.get("short_code") == "ERROR":
                return {
                    "success": False,
                    "error": result.get("reasoning", "Gemini Hybrid error"),
                    "method": "gemini_hybrid_failed"
                }
            
            # Common processing for all Gemini modes (hybrid + flash + lite)
            from rule_classifier import classify_document_name_from_code, EXACT_TITLE_MAPPING, DOCUMENT_RULES
            
            short_code = result.get("short_code", "UNKNOWN")
            
            # ‚úÖ CODE ALIAS MAPPING: Map alternate codes to standard codes
            CODE_ALIASES = {
                "HDTG": "HDCQ",  # H·ª£p ƒë·ªìng t·∫∑ng cho ‚Üí H·ª£p ƒë·ªìng chuy·ªÉn nh∆∞·ª£ng, t·∫∑ng cho
                "BVDS": "HSKT",  # B·∫£n v·∫Ω ƒëo s∆° / B·∫£n ƒë·ªì ƒë·ªãa ch√≠nh ‚Üí H·ªì s∆° k·ªπ thu·∫≠t
            }
            
            # Apply alias mapping if needed
            if short_code in CODE_ALIASES:
                original_code = short_code
                short_code = CODE_ALIASES[short_code]
                result["short_code"] = short_code
                print(f"üîÑ Mapped code '{original_code}' ‚Üí '{short_code}'", file=sys.stderr)
            
            # ‚úÖ VALIDATE: Gemini sometimes creates invalid codes (e.g., "LCHO" not in our 98 valid codes)
            # Get all valid codes from rule_classifier
            VALID_CODES = set(EXACT_TITLE_MAPPING.values())
            VALID_CODES.update(DOCUMENT_RULES.keys())
            
            # If Gemini returns invalid code, force to UNKNOWN
            if short_code not in VALID_CODES and short_code != "UNKNOWN":
                print(f"‚ö†Ô∏è Gemini Hybrid returned INVALID code '{short_code}' (not in 98 valid codes). Forcing to UNKNOWN.", file=sys.stderr)
                print(f"   Original reasoning: {result.get('reasoning', 'N/A')}", file=sys.stderr)
                result["short_code"] = "UNKNOWN"
                result["confidence"] = 0.1
                result["reasoning"] = f"AI returned invalid code '{short_code}' (not in system). Original: {result.get('reasoning', '')}"
                short_code = "UNKNOWN"
            
            doc_name = classify_document_name_from_code(short_code)

            # Extract color, issue_date and issue_date_confidence for GCN documents
            color = result.get("color", None)
            issue_date = result.get("issue_date", None)
            issue_date_confidence = result.get("issue_date_confidence", None)
            
            # Tier-specific metadata for hybrid mode
            tier1_confidence = result.get('tier1_confidence', 0)
            tier2_confidence = result.get('tier2_confidence', None)
            escalation_reason = result.get('escalation_reason', 'none')
            
            return {
                "success": True,
                "type": short_code,
                "doc_type": doc_name,
                "short_code": short_code,
                "confidence": result.get("confidence", 0.5),
                "matched_keywords": [result.get("reasoning", "Hybrid AI classification")],
                "title_boost_applied": True if short_code != "UNKNOWN" else False,
                "title_extracted_via_pattern": True if short_code != "UNKNOWN" else False,
                "reasoning": result.get("reasoning", ""),
                "color": color,
                "issue_date": issue_date,
                "issue_date_confidence": issue_date_confidence,
                "method": method_used,
                "accuracy_estimate": f"{int(result.get('confidence', 0.5) * 100)}%",
                "recommend_cloud_boost": False,
                "avg_font_height": 0,
                # Hybrid-specific stats
                "tier_used": tier_used,
                "tier1_confidence": tier1_confidence,
                "tier2_confidence": tier2_confidence,
                "escalation_reason": escalation_reason,
                "cost_estimate": result.get('cost_estimate', 'medium'),
                "usage": {},  # Hybrid mode doesn't expose token counts directly
                "estimated_cost_usd": 0  # Could be calculated based on tier_used
            }

        # Handle OpenAI GPT-4o mini Vision (AI classification)
        elif ocr_engine_type == 'openai-gpt4o-mini':
            if not cloud_api_key:
                return {
                    "success": False,
                    "error": "OpenAI API key is required for GPT-4o mini",
                    "method": "config_error"
                }

            print(f"ü§ñ Using OpenAI GPT-4o mini Vision AI classification", file=sys.stderr)

            from ocr_engine_openai_vision import classify_document_openai_vision
            from rule_classifier import classify_document_name_from_code
            import time

            # Get resize settings from environment (set by Electron)
            enable_resize = os.environ.get('ENABLE_RESIZE', 'true').lower() == 'true'
            max_width = int(os.environ.get('MAX_WIDTH', '1500'))
            max_height = int(os.environ.get('MAX_HEIGHT', '2100'))

            if enable_resize:
                print(f"üí∞ Smart resize enabled: max {max_width}x{max_height}px", file=sys.stderr)
            start_time = time.time()

            # Call OpenAI Vision classifier
            result = classify_document_openai_vision(
                file_path, 
                cloud_api_key,
                enable_resize=enable_resize,
                max_width=max_width,
                max_height=max_height
            )

            scan_time = time.time() - start_time
            print(f"‚è±Ô∏è Result: {result.get('short_code')} (confidence: {result.get('confidence'):.2f}, time: {scan_time:.1f}s)", file=sys.stderr)

            if result.get("short_code") == "ERROR":
                return {
                    "success": False,
                    "error": result.get("reasoning", "OpenAI Vision error"),
                    "method": "openai_vision_failed"
                }

            method_used = "openai_gpt4o_mini_vision"
            short_code = result.get("short_code", "UNKNOWN")

            # Code alias mapping (same as Gemini)
            from rule_classifier import EXACT_TITLE_MAPPING, DOCUMENT_RULES
            CODE_ALIASES = {
                "HDTG": "HDCQ",
                "BVDS": "HSKT",
            }
            
            if short_code in CODE_ALIASES:
                original_code = short_code
                short_code = CODE_ALIASES[short_code]
                result["short_code"] = short_code
                print(f"üîÑ Mapped code '{original_code}' ‚Üí '{short_code}'", file=sys.stderr)
            
            # Validate code
            VALID_CODES = set(EXACT_TITLE_MAPPING.values())
            VALID_CODES.update(DOCUMENT_RULES.keys())
            
            if short_code not in VALID_CODES and short_code != "UNKNOWN":
                print(f"‚ö†Ô∏è OpenAI returned INVALID code '{short_code}'. Forcing to UNKNOWN.", file=sys.stderr)
                result["short_code"] = "UNKNOWN"
                result["confidence"] = 0.1
                result["reasoning"] = f"AI returned invalid code '{short_code}'. Original: {result.get('reasoning', '')}"
                short_code = "UNKNOWN"
            
            doc_name = classify_document_name_from_code(short_code)

            # Extract GCN metadata
            color = result.get("color", None)
            issue_date = result.get("issue_date", None)
            issue_date_confidence = result.get("issue_date_confidence", None)
            
            # Calculate cost (OpenAI pricing)
            usage = result.get('usage', {})
            input_tokens = usage.get('input_tokens', 0)
            output_tokens = usage.get('output_tokens', 0)
            
            # GPT-4o mini pricing (as of Jan 2025)
            # Input: $0.15 per 1M tokens, Output: $0.60 per 1M tokens
            cost_usd = (input_tokens * 0.15 / 1_000_000) + (output_tokens * 0.60 / 1_000_000)
            
            return {
                "success": True,
                "type": short_code,
                "doc_type": doc_name,
                "short_code": short_code,
                "confidence": result.get("confidence", 0.5),
                "matched_keywords": [result.get("reasoning", "OpenAI AI classification")],
                "title_boost_applied": True if short_code != "UNKNOWN" else False,
                "title_extracted_via_pattern": True if short_code != "UNKNOWN" else False,
                "reasoning": result.get("reasoning", ""),
                "color": color,
                "issue_date": issue_date,
                "issue_date_confidence": issue_date_confidence,
                "method": method_used,
                "accuracy_estimate": f"{int(result.get('confidence', 0.5) * 100)}%",
                "recommend_cloud_boost": False,
                "avg_font_height": 0,
                "usage": usage,
                "estimated_cost_usd": cost_usd
            }

        # Handle Gemini Flash & Tesseract+Text (AI classification) - POSITION-AWARE APPROACH
        elif ocr_engine_type in ['gemini-flash', 'gemini-flash-lite', 'gemini-flash-hybrid', 'gemini-flash-text']:
            if not cloud_api_key:
                return {
                    "success": False,
                    "error": "Google API key is required for Gemini Flash",
                    "method": "config_error"
                }

            # Check if Tesseract+Text mode
            if ocr_engine_type == 'gemini-flash-text':
                print(f"üî¨ Using Tesseract + Gemini Text mode (sequential)", file=sys.stderr)
                from tesseract_text_classifier import process_image as tesseract_text_process
                import time
                
                start_time = time.time()
                result = tesseract_text_process(file_path, cloud_api_key)
                scan_time = time.time() - start_time
                
                print(f"‚è±Ô∏è Result: {result.get('short_code')} (confidence: {result.get('confidence'):.2f}, time: {scan_time:.1f}s)", file=sys.stderr)
                
                if result.get("short_code") == "ERROR":
                    return {
                        "success": False,
                        "error": result.get("reasoning", "Tesseract+Text error"),
                        "method": "tesseract_text_failed"
                    }
                
                method_used = "tesseract_text"
                short_code = result.get("short_code", "UNKNOWN")
            else:
                # Standard Gemini Vision mode
                model_type = 'Lite' if ocr_engine_type == 'gemini-flash-lite' else 'Flash'
                if ocr_engine_type == 'gemini-flash-hybrid':
                    model_type = 'Hybrid'
                print(f"ü§ñ Using Gemini {model_type} AI with POSITION-AWARE classification", file=sys.stderr)

                from ocr_engine_gemini_flash import classify_document_gemini_flash
                from rule_classifier import classify_document_name_from_code
                import time

                # Get resize settings from environment (set by Electron)
                enable_resize = os.environ.get('ENABLE_RESIZE', 'true').lower() == 'true'
                max_width = int(os.environ.get('MAX_WIDTH', '2000'))
                max_height = int(os.environ.get('MAX_HEIGHT', '2800'))

                print("üì∏ Scanning FULL IMAGE with position-aware analysis...", file=sys.stderr)
                if enable_resize:
                    print(f"üí∞ Smart resize enabled: max {max_width}x{max_height}px", file=sys.stderr)
                start_time = time.time()

                # Pass model type and resize settings to classifier
                result = classify_document_gemini_flash(
                    file_path, 
                    cloud_api_key, 
                    crop_top_percent=1.0, 
                    model_type=ocr_engine_type,
                    enable_resize=enable_resize,
                    max_width=max_width,
                    max_height=max_height
                )

                scan_time = time.time() - start_time
                print(f"‚è±Ô∏è Result: {result.get('short_code')} (confidence: {result.get('confidence'):.2f}, position: {result.get('title_position', 'unknown')}, time: {scan_time:.1f}s)", file=sys.stderr)

                if result.get("short_code") == "ERROR":
                    return {
                        "success": False,
                        "error": result.get("reasoning", "Gemini Flash error"),
                        "method": "gemini_flash_failed"
                    }

                title_position = result.get("title_position", "unknown")
                short_code = result.get("short_code", "UNKNOWN")
                if title_position in ["middle", "bottom"] and short_code != "UNKNOWN":
                    print(f"‚ö†Ô∏è Title found at {title_position} (not top), treating as mention", file=sys.stderr)
                    result["short_code"] = "UNKNOWN"
                    result["confidence"] = 0.1
                    result["reasoning"] = f"Text pattern found at {title_position}, not a main title"

                method_used = "gemini_position_aware"

                # Map Gemini result to rule_classifier format
                short_code = result.get("short_code", "UNKNOWN")
            
            # ‚úÖ CODE ALIAS MAPPING: Map alternate codes to standard codes
            CODE_ALIASES = {
                "HDTG": "HDCQ",  # H·ª£p ƒë·ªìng t·∫∑ng cho ‚Üí H·ª£p ƒë·ªìng chuy·ªÉn nh∆∞·ª£ng, t·∫∑ng cho
                "BVDS": "HSKT",  # B·∫£n v·∫Ω ƒëo s∆° / B·∫£n ƒë·ªì ƒë·ªãa ch√≠nh ‚Üí H·ªì s∆° k·ªπ thu·∫≠t
            }
            
            # Apply alias mapping if needed
            if short_code in CODE_ALIASES:
                original_code = short_code
                short_code = CODE_ALIASES[short_code]
                result["short_code"] = short_code
                print(f"üîÑ Mapped code '{original_code}' ‚Üí '{short_code}'", file=sys.stderr)
            
            # ‚úÖ VALIDATE: Gemini sometimes creates invalid codes (e.g., "LCHO" not in our 98 valid codes)
            # Get all valid codes from rule_classifier
            from rule_classifier import EXACT_TITLE_MAPPING, DOCUMENT_RULES
            VALID_CODES = set(EXACT_TITLE_MAPPING.values())
            VALID_CODES.update(DOCUMENT_RULES.keys())
            
            # If Gemini returns invalid code, force to UNKNOWN
            if short_code not in VALID_CODES and short_code != "UNKNOWN":
                print(f"‚ö†Ô∏è Gemini returned INVALID code '{short_code}' (not in 98 valid codes). Forcing to UNKNOWN.", file=sys.stderr)
                print(f"   Original reasoning: {result.get('reasoning', 'N/A')}", file=sys.stderr)
                result["short_code"] = "UNKNOWN"
                result["confidence"] = 0.1
                result["reasoning"] = f"AI returned invalid code '{short_code}' (not in system). Original: {result.get('reasoning', '')}"
                short_code = "UNKNOWN"
            
            doc_name = classify_document_name_from_code(short_code)

            # Usage tokens for cost estimation
            usage = result.get('usage') or {}
            input_tokens = int(usage.get('input_tokens', 0) or 0)
            output_tokens = int(usage.get('output_tokens', 0) or 0)

            # Pricing (USD per 1,000,000 tokens) - configurable via env
            # Defaults aligned with AI Studio typical pricing; override via env when needed
            INPUT_RATE_PER_M = float(os.environ.get('GEMINI_INPUT_RATE_PER_M', '0.30') or 0)
            OUTPUT_RATE_PER_M = float(os.environ.get('GEMINI_OUTPUT_RATE_PER_M', '2.50') or 0)
            estimated_cost_usd = (input_tokens * INPUT_RATE_PER_M + output_tokens * OUTPUT_RATE_PER_M) / 1_000_000.0

            # Extract color, issue_date and issue_date_confidence for GCN documents
            color = result.get("color", None)
            issue_date = result.get("issue_date", None)
            issue_date_confidence = result.get("issue_date_confidence", None)
            
            return {
                "success": True,
                "type": short_code,
                "doc_type": doc_name,
                "short_code": short_code,
                "confidence": result.get("confidence", 0.5),
                "matched_keywords": [result.get("reasoning", "AI classification")],
                "title_boost_applied": True if short_code != "UNKNOWN" else False,
                "title_extracted_via_pattern": True if short_code != "UNKNOWN" else False,
                "reasoning": result.get("reasoning", ""),
                "color": color,
                "issue_date": issue_date,
                "issue_date_confidence": issue_date_confidence,
                "method": method_used,
                "accuracy_estimate": f"{int(result.get('confidence', 0.5) * 100)}%",
                "recommend_cloud_boost": False,
                "avg_font_height": 0,
                "hybrid_stats": result.get('hybrid_stats', {}),
                "usage": {
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": int(usage.get('total_tokens', input_tokens + output_tokens) or 0)
                },
                "estimated_cost_usd": round(estimated_cost_usd, 6)
            }

        # Handle Cloud OCR engines
        if ocr_engine_type == 'google':
            if not cloud_api_key:
                return {
                    "success": False,
                    "error": "Google Cloud Vision API key is required",
                    "method": "config_error"
                }

            print("‚òÅÔ∏è Using Google Cloud Vision", file=sys.stderr)

            from ocr_engine_google import ocr_google_cloud_vision
            text, confidence, error = ocr_google_cloud_vision(file_path, cloud_api_key)

            if error:
                return {
                    "success": False,
                    "error": error,
                    "method": "cloud_ocr_failed"
                }

            engine_name = "Google Cloud Vision"
            extracted_text = text
            ocr_confidence = confidence

        elif ocr_engine_type == 'azure':
            if not cloud_api_key or not cloud_endpoint:
                return {
                    "success": False,
                    "error": "Azure Computer Vision API key and endpoint are required",
                    "method": "config_error"
                }

            print("‚òÅÔ∏è Using Azure Computer Vision", file=sys.stderr)

            from ocr_engine_azure import ocr_azure_computer_vision
            text, confidence, error = ocr_azure_computer_vision(file_path, cloud_api_key, cloud_endpoint)

            if error:
                return {
                    "success": False,
                    "error": error,
                    "method": "cloud_ocr_failed"
                }

            engine_name = "Azure Computer Vision"
            extracted_text = text
            ocr_confidence = confidence

        else:
            # Offline OCR engines - Lazy load on demand
            global tesseract_engine, vietocr_engine, easyocr_engine

            # Select OCR engine based on preference
            if ocr_engine_type == 'vietocr':
                if vietocr_engine is None:
                    try:
                        from ocr_engine_vietocr import OCREngine as VietOCREngine
                        vietocr_engine = VietOCREngine()
                        print("‚úÖ VietOCR engine loaded", file=sys.stderr)
                    except Exception as e:
                        print(f"‚ö†Ô∏è VietOCR load failed: {e}, falling back to Tesseract", file=sys.stderr)

                if vietocr_engine is not None:
                    ocr_engine = vietocr_engine
                    engine_name = "VietOCR"
                    print("üîç Using VietOCR engine", file=sys.stderr)
                else:
                    if tesseract_engine is None:
                        from ocr_engine_tesseract import OCREngine as TesseractEngine
                        tesseract_engine = TesseractEngine()
                    ocr_engine = tesseract_engine
                    engine_name = "Tesseract"

            elif ocr_engine_type == 'easyocr':
                if easyocr_engine is None:
                    try:
                        from ocr_engine_easyocr import OCREngine as EasyOCREngine
                        easyocr_engine = EasyOCREngine()
                        print("‚úÖ EasyOCR engine loaded", file=sys.stderr)
                    except Exception as e:
                        print(f"‚ö†Ô∏è EasyOCR load failed: {e}, falling back to Tesseract", file=sys.stderr)

                if easyocr_engine is not None:
                    ocr_engine = easyocr_engine
                    engine_name = "EasyOCR"
                    print("üîç Using EasyOCR engine", file=sys.stderr)
                else:
                    if tesseract_engine is None:
                        from ocr_engine_tesseract import OCREngine as TesseractEngine
                        tesseract_engine = TesseractEngine()
                    ocr_engine = tesseract_engine
                    engine_name = "Tesseract"

            else:
                if tesseract_engine is None:
                    try:
                        from ocr_engine_tesseract import OCREngine as TesseractEngine
                        tesseract_engine = TesseractEngine()
                        print("‚úÖ Tesseract engine loaded", file=sys.stderr)
                    except Exception as e:
                        return {
                            "success": False,
                            "error": f"Tesseract not available: {e}",
                            "method": "engine_load_failed"
                        }

                ocr_engine = tesseract_engine
                engine_name = "Tesseract"
                print("üîç Using Tesseract engine", file=sys.stderr)

            # Extract text
            ocr_result = ocr_engine.extract_text(file_path)
            if isinstance(ocr_result, dict):
                extracted_text = ocr_result.get('full_text', '')
                title_text = ocr_result.get('title_text', '')
                avg_height = ocr_result.get('avg_height', 0)
            else:
                extracted_text = ocr_result
                title_text = extracted_text
                avg_height = 0

            ocr_confidence = None

        classifier = RuleClassifier()

        if not extracted_text or extracted_text.strip() == "":
            return {
                "success": False,
                "error": "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t text t·ª´ ·∫£nh",
                "method": "ocr_failed"
            }

        print(f"üìù Full text (first 500 chars): {extracted_text[:500]}", file=sys.stderr)

        extracted_title = extract_document_title_from_text(extracted_text)
        if extracted_title:
            print(f"‚úÖ Extracted title via pattern: {extracted_title[:80]}...", file=sys.stderr)
        else:
            print("‚ö†Ô∏è No title pattern found in full text", file=sys.stderr)

        if extracted_title:
            final_title = extracted_title
        elif ocr_engine_type in ['google', 'azure']:
            final_title = extracted_text
        else:
            final_title = title_text

        result = classifier.classify(extracted_text, title_text=final_title, ocr_engine=ocr_engine_type)
        if not result or 'doc_type' not in result:
            result = {
                'doc_type': 'Kh√¥ng x√°c ƒë·ªãnh',
                'short_code': 'UNKNOWN',
                'confidence': 0.0,
                'reasoning': 'Kh√¥ng th·ªÉ nh·∫≠n di·ªán lo·∫°i t√†i li·ªáu'
            }

        confidence_threshold = 0.7
        is_cloud = ocr_engine_type in ['google', 'azure']
        recommend_cloud_boost = not is_cloud and result['confidence'] < confidence_threshold

        title_boost_info = ""
        if result.get('title_boost', False):
            title_boost_info = " [TITLE DETECTED ‚úì]"

        response = {
            "success": True,
            "method": "cloud_ocr" if is_cloud else "offline_ocr",
            "ocr_engine": engine_name,
            "original_text": extracted_text,
            "title_text": final_title,
            "title_extracted_via_pattern": bool(extracted_title),
            "doc_type": result['doc_type'],
            "confidence": result['confidence'],
            "short_code": result['short_code'],
            "reasoning": result.get('reasoning', '') + title_boost_info,
            "recommend_cloud_boost": recommend_cloud_boost,
            "title_boost_applied": result.get('title_boost', False)
        }

        if is_cloud:
            response["ocr_confidence"] = float(ocr_confidence) if ocr_confidence else 0.9
            response["accuracy_estimate"] = "90-96%"
        else:
            response["title_text_ocr"] = title_text if 'title_text' in locals() else final_title
            response["avg_font_height"] = round(avg_height, 1) if 'avg_height' in locals() else 0
            response["accuracy_estimate"] = "88-91%" if result.get('title_boost') else "85-88%"

        return response

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"‚ùå Error: {error_detail}", file=sys.stderr)
        return {
            "success": False,
            "error": str(e),
            "method": "processing_failed"
        }


def main():
    if len(sys.argv) < 2:
        print(json.dumps({
            "error": "Usage: python process_document.py <file_path> [ocr_engine_type] [cloud_api_key] [cloud_endpoint]",
            "success": False
        }, ensure_ascii=True))
        sys.exit(1)

    file_path = sys.argv[1]
    ocr_engine_type = sys.argv[2] if len(sys.argv) > 2 else 'tesseract'
    cloud_api_key = sys.argv[3] if len(sys.argv) > 3 else None
    cloud_endpoint = sys.argv[4] if len(sys.argv) > 4 else None

    if not file_path or not os.path.exists(file_path):
        error_msg = json.dumps({
            "error": "File not found or invalid path",
            "success": False
        }, ensure_ascii=False)
        sys.stdout.write(error_msg)
        sys.stdout.write('\n')
        sys.stdout.flush()
        sys.exit(1)

    result = process_document(file_path, ocr_engine_type, cloud_api_key, cloud_endpoint)

    output = json.dumps(result, ensure_ascii=True, indent=None)
    sys.stdout.write(output)
    sys.stdout.write('\n')
    sys.stdout.flush()

    sys.exit(0 if result.get('success') else 1)


if __name__ == "__main__":
    main()
