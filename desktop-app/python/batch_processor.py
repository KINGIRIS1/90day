#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Batch Document Processor - Multi-Image Analysis
Supports 2 modes:
- Mode 1: Fixed batch size (5 images per batch)
- Mode 2: Smart batching (group by document boundaries)
"""

import sys
import os
import json
import base64
import re
import requests
from PIL import Image
import io

# Import existing engines and prompts
from ocr_engine_gemini_flash import (
    resize_image_smart, 
    parse_gemini_response,
    get_classification_prompt,
    get_classification_prompt_lite
)


def adapt_prompt_for_multi_image(single_image_prompt, batch_size):
    """
    Adapt single-image prompt to multi-image batch context
    
    Changes:
    1. Add multi-image context introduction
    2. Add document grouping instructions
    3. Change output format from single result to documents array
    4. Add page indexing (0-indexed)
    5. Emphasize MUST return ALL pages
    """
    
    # Use % formatting to avoid f-string issues with JSON examples
    multi_image_intro = """ğŸ¯ BATCH ANALYSIS - %d TRANG SCAN

Báº¡n Ä‘ang phÃ¢n tÃ­ch %d trang scan tÃ i liá»‡u Ä‘áº¥t Ä‘ai Viá»‡t Nam.
CÃ¡c trang nÃ y cÃ³ thá»ƒ thuá»™c 1 hoáº·c nhiá»u tÃ i liá»‡u khÃ¡c nhau.

ğŸš¨ QUAN TRá»ŒNG NHáº¤T - BATCH MODE vs SINGLE-FILE MODE:
Trong BATCH MODE nÃ y, báº¡n KHÃ”NG pháº£i single-file classifier!
- âŒ Äá»ªNG tráº£ vá» "UNKNOWN" cho continuation pages
- âœ… Báº¡n PHáº¢I tá»± GOM continuation pages vÃ o document trÆ°á»›c
- âœ… Báº¡n cÃ³ context tá»« nhiá»u pages â†’ HÃ£y táº­n dá»¥ng!

VÃ Dá»¤:
Page 0: "THÃ”NG BÃO THUáº¾" â†’ Start TBT document
Page 1: "ÄIá»€U 1" â†’ TBT continuation â†’ ADD vÃ o pages cá»§a TBT
Page 2: "III. TÃNH THUáº¾" + báº£ng â†’ TBT continuation â†’ ADD vÃ o pages cá»§a TBT

Result: {{"type": "TBT", "pages": [0,1,2]}} âœ…

KHÃ”NG LÃ€M (single-file style):
Result: 
  {{"type": "TBT", "pages": [0]}},
  {{"type": "UNKNOWN", "pages": [1,2]}} âŒ SAI!

NHIá»†M Vá»¤:
1. XÃ¡c Ä‘á»‹nh cÃ³ BAO NHIÃŠU tÃ i liá»‡u khÃ¡c nhau trong %d trang nÃ y
2. NhÃ³m cÃ¡c trang theo tÃ i liá»‡u (pages array)
3. PhÃ¢n loáº¡i loáº¡i tÃ i liá»‡u cá»§a tá»«ng nhÃ³m
4. TrÃ­ch xuáº¥t metadata (ngÃ y cáº¥p cho GCN, mÃ u sáº¯c, v.v.)

Dáº¤U HIá»†U NHáº¬N BIáº¾T:

TRANG 1 Cá»¦A TÃ€I LIá»†U (New Document):
- CÃ³ TIÃŠU Äá»€ CHÃNH á»Ÿ TOP 30%% (Ä‘áº§u trang)
- Cá»¡ chá»¯ Lá»šN, IN HOA, cÄƒn giá»¯a
- CÃ³ quá»‘c huy (Ä‘á»‘i vá»›i GCN)
- KhÃ¡c biá»‡t rÃµ vá» format/mÃ u sáº¯c so vá»›i trang trÆ°á»›c

TRANG TIáº¾P Ná»I (Continuation - Trang 2, 3, 4...):
- KHÃ”NG cÃ³ tiÃªu Ä‘á» chÃ­nh á»Ÿ Ä‘áº§u
- Chá»‰ cÃ³ section headers: "II.", "III.", "ÄIá»€U 2", "PHáº¦N II"
- CÃ¹ng format/mÃ u sáº¯c vá»›i trang trÆ°á»›c
- Ná»™i dung liÃªn tá»¥c (Ä‘iá»u khoáº£n, chá»¯ kÃ½, báº£ng biá»ƒu)

ğŸš¨ QUAN TRá»ŒNG - NHáº¬N DIá»†N CONTINUATION PAGES:
CÃ¡c dáº¥u hiá»‡u sau = CONTINUATION (trang tiáº¿p theo, KHÃ”NG pháº£i document má»›i):
1. Section headers vá»›i sá»‘: "II.", "III.", "IV.", "V.", "ÄIá»€U 2", "ÄIá»€U 3", "PHáº¦N II", "Má»¤C III"
2. Báº£ng biá»ƒu vá»›i sá»‘ phÃ¢n cáº¥p: "4.1", "4.2", "4.2.1", "4.2.2", "(1.1)", "(2.1.3)"
3. Text body tiáº¿p ná»‘i: "...tiáº¿p theo...", "...nhÆ° sau:", danh sÃ¡ch bullet points
4. Chá»¯ kÃ½/con dáº¥u á»Ÿ cuá»‘i trang
5. KhÃ´ng cÃ³ header chÃ­nh thá»©c (quá»‘c huy, cÆ¡ quan ban hÃ nh)
6. **"Lá»œI CHá»¨NG Cá»¦A CÃ”NG CHá»¨NG VIÃŠN"** â†’ Trang chá»¯ kÃ½ cÃ´ng chá»©ng (cuá»‘i document)
7. **Danh sÃ¡ch ngÆ°á»i kÃ½, con dáº¥u cÃ´ng chá»©ng** â†’ Trang cuá»‘i document

ğŸš¨ Äáº¶C BIá»†T - TRANG CÃ”NG CHá»¨NG (KHÃ”NG pháº£i document má»›i):
Náº¿u trang cÃ³:
- "Lá»œI CHá»¨NG Cá»¦A CÃ”NG CHá»¨NG VIÃŠN"
- "CÃ”NG CHá»¨NG VIÃŠN"
- Con dáº¥u cÃ´ng chá»©ng (há»“ng/Ä‘á»)
- Danh sÃ¡ch chá»¯ kÃ½ cÃ¡c Ã”ng/BÃ 
- VÄƒn phÃ²ng cÃ´ng chá»©ng

â†’ ÄÃ¢y lÃ  TRANG CUá»I (signature page) cá»§a document
â†’ KHÃ”NG pháº£i document má»›i
â†’ GOM VÃ€O document trÆ°á»›c (TTHGD, PCTSVC, HDCQ, HDUQ, v.v.)

VÃ Dá»¤ ÄÃšNG:
Page 0: "THá»A THUáº¬N Há»˜ GIA ÄÃŒNH" â†’ TTHGD
Page 1-3: Ná»™i dung thá»a thuáº­n â†’ TTHGD continuation
Page 4: "Lá»œI CHá»¨NG Cá»¦A CÃ”NG CHá»¨NG VIÃŠN" + danh sÃ¡ch â†’ TTHGD continuation (signature page)
Page 5: Con dáº¥u, chá»¯ kÃ½ â†’ TTHGD continuation

Result: {{"type": "TTHGD", "pages": [0,1,2,3,4,5]}} âœ…

KHÃ”NG LÃ€M:
  {{"type": "TTHGD", "pages": [0,1,2,3]}},
  {{"type": "GTLQ", "pages": [4,5]}}  âŒ SAI!

VÃ Dá»¤ CONTINUATION - PHáº¢I GOM VÃ€O DOCUMENT TRÆ¯á»šC:
âœ… "III. TÃNH THUáº¾ Cá»¦A CÆ  QUAN THUáº¾" + báº£ng 4.1, 4.2
   â†’ Section header vá»›i sá»‘ La MÃ£
   â†’ Báº£ng biá»ƒu phÃ¢n cáº¥p
   â†’ ÄÃ¢y lÃ  continuation cá»§a document trÆ°á»›c (cÃ³ thá»ƒ lÃ  TBT, HDCQ, etc.)
   â†’ KHÃ”NG classify thÃ nh UNKNOWN
   â†’ GOM VÃ€O document cÃ³ trang trÆ°á»›c Ä‘Ã³
   
   VÃ Dá»¤ Cá»¤ THá»‚:
   Page 4 (index 4): "THÃ”NG BÃO THUáº¾" (title) â†’ TBT
   Page 5 (index 5): "ÄIá»€U 1: ..." â†’ TBT continuation
   Page 6 (index 6): "III. TÃNH THUáº¾" + báº£ng 4.1 â†’ TBT continuation âœ… (KHÃ”NG pháº£i UNKNOWN)
   
   Result: {{"type": "TBT", "pages": [4, 5, 6], ...}}

âœ… "ÄIá»€U 2: Ná»˜I DUNG THá»A THUáº¬N PHÃ‚N CHIA"
   â†’ Section header
   â†’ Continuation cá»§a TTHGD hoáº·c PCTSVC
   â†’ GOM VÃ€O document trÆ°á»›c

âœ… Trang chá»‰ cÃ³ báº£ng biá»ƒu (khÃ´ng cÃ³ title)
   â†’ Continuation
   â†’ GOM VÃ€O document trÆ°á»›c

âœ… Trang cÃ³ "1.1 TrÆ°á»ng há»£p...", "1.2 TrÆ°á»ng há»£p..." (numbered list)
   â†’ Continuation vá»›i structured content
   â†’ GOM VÃ€O document trÆ°á»›c

âŒ SAI - Classify continuation thÃ nh UNKNOWN:
Page 5: "THÃ”NG BÃO THUáº¾" (title)
Page 6: "III. TÃNH THUáº¾" + báº£ng
â†’ AI classify:
  {{"type": "TBT", "pages": [5]}},
  {{"type": "UNKNOWN", "pages": [6]}}  âŒ SAI!
â†’ ÄÃšNG:
  {{"type": "TBT", "pages": [5, 6]}}  âœ…

RANH GIá»šI GIá»®A CÃC TÃ€I LIá»†U:
- Thay Ä‘á»•i rÃµ rá»‡t: mÃ u giáº¥y (há»“ng â†’ tráº¯ng), format (cÃ³ quá»‘c huy â†’ khÃ´ng cÃ³)
- Xuáº¥t hiá»‡n tiÃªu Ä‘á» chÃ­nh má»›i á»Ÿ TOP
- Thay Ä‘á»•i hoÃ n toÃ n vá» layout

ğŸ¯ Dáº¤U HIá»†U VISUAL - CÃ™NG DOCUMENT (Cá»°C Ká»² QUAN TRá»ŒNG):

**1. Dáº¤U GIÃP LAI (Overlapping Stamp):**
Náº¿u tháº¥y CON Dáº¤U Äá»/Há»’NG bá»‹ Cáº®T NGANG qua nhiá»u pages:
- Page 1: CÃ³ PHáº¦N TRÃŠN cá»§a con dáº¥u (top half)
- Page 2: CÃ³ PHáº¦N GIá»®A cá»§a con dáº¥u (middle)
- Page 3: CÃ³ PHáº¦N DÆ¯á»šI cá»§a con dáº¥u (bottom half)

â†’ ÄÃ¢y lÃ  Dáº¤U GIÃP LAI!
â†’ 3-4 pages nÃ y Ä‘Æ°á»£c Ä‘Ã³ng dáº¥u CÃ™NG LÃšC (giáº¥y chá»“ng lÃªn nhau)
â†’ **Báº®T BUá»˜C CÃ™NG 1 DOCUMENT**
â†’ PHáº¢I GOM Táº¤T Cáº¢ pages cÃ³ partial stamp vÃ o 1 document

VÃ Dá»¤:
Page 0: "THá»A THUáº¬N Há»˜ GIA ÄÃŒNH" + pháº§n trÃªn con dáº¥u Ä‘á» (â¬†ï¸ top half)
Page 1: Text body + pháº§n giá»¯a con dáº¥u Ä‘á» (â¬Œ middle)
Page 2: Text body + pháº§n giá»¯a con dáº¥u Ä‘á» (â¬Œ middle)
Page 3: "Lá»œI CHá»¨NG..." + pháº§n dÆ°á»›i con dáº¥u Ä‘á» (â¬‡ï¸ bottom half)

â†’ **4 pages cÃ³ cÃ¹ng con dáº¥u bá»‹ cáº¯t** â†’ CÃ™NG 1 DOCUMENT!
â†’ Result: {{"type": "TTHGD", "pages": [0,1,2,3]}} âœ…

**2. Dáº¤U HOÃ€N CHá»ˆNH (Complete Stamp):**
Náº¿u page cÃ³ con dáº¥u HOÃ€N CHá»ˆNH (full circle, khÃ´ng bá»‹ cáº¯t):
â†’ ÄÃ¢y cÃ³ thá»ƒ lÃ  trang Äá»˜C Láº¬P (single document)
â†’ HOáº¶C trang cuá»‘i cá»§a document

**3. KHÃ”NG CÃ“ Dáº¤U:**
Náº¿u page khÃ´ng cÃ³ con dáº¥u:
â†’ CÃ³ thá»ƒ lÃ  trang giá»¯a document
â†’ Check title vÃ  continuation patterns

ğŸš¨ NGUYÃŠN Táº®C Dáº¤U GIÃP LAI:
- Partial stamp (bá»‹ cáº¯t) = **STRONG SIGNAL** cÃ¹ng document
- Æ¯u tiÃªn cao hÆ¡n cáº£ title/content analysis
- Náº¿u tháº¥y dáº¥u giÃ¡p lai â†’ GOM NGAY, khÃ´ng cáº§n nghi ngá»

---

""" % (batch_size, batch_size, batch_size)

    unknown_rules = """

âš ï¸ QUAN TRá»ŒNG - KHI NÃ€O TRáº¢ Vá»€ "UNKNOWN":
CHá»ˆ tráº£ vá» "UNKNOWN" khi:
1. Trang thá»±c sá»± khÃ´ng cÃ³ tiÃªu Ä‘á» VÃ€ khÃ´ng match continuation patterns
2. Title khÃ´ng thuá»™c 98 loáº¡i VÃ€ khÃ´ng pháº£i continuation
3. Trang hoÃ n toÃ n trá»‘ng hoáº·c khÃ´ng Ä‘á»c Ä‘Æ°á»£c

âŒ KHÃ”NG tráº£ vá» "UNKNOWN" cho:
- Trang cÃ³ section headers (III., ÄIá»€U 2) â†’ Continuation, gom vÃ o doc trÆ°á»›c
- Trang cÃ³ báº£ng biá»ƒu structured â†’ Continuation, gom vÃ o doc trÆ°á»›c
- Trang cÃ³ text liÃªn tá»¥c vá»›i trang trÆ°á»›c â†’ Continuation, gom vÃ o doc trÆ°á»›c

ğŸ¯ NGUYÃŠN Táº®C: Khi nghi ngá» â†’ Gom vÃ o document trÆ°á»›c (safer than creating new UNKNOWN doc)

âš ï¸ Äáº¶C BIá»†T - GOM CONTINUATION THAY VÃŒ TRáº¢ Vá»€ UNKNOWN:
Náº¾U trang khÃ´ng cÃ³ title NHÆ¯NG cÃ³ dáº¥u hiá»‡u continuation:
- Section headers: "II.", "III.", "ÄIá»€U X"
- Báº£ng biá»ƒu: tables vá»›i numbers
- Text body: tiáº¿p tá»¥c content

â†’ KHÃ”NG táº¡o document UNKNOWN riÃªng
â†’ GOM VÃ€O document trÆ°á»›c Ä‘Ã³
â†’ Extend "pages" array cá»§a document trÆ°á»›c

VÃ Dá»¤ ÄÃšNG:
Page 0: "THÃ”NG BÃO THUáº¾" (title) â†’ TBT
Page 1: "ÄIá»€U 1: ..." (section) â†’ TBT continuation
Page 2: "III. TÃNH THUáº¾" + báº£ng (section + table) â†’ TBT continuation

Result: {{"type": "TBT", "pages": [0, 1, 2], ...}} âœ…

KHÃ”NG LÃ€M:
  {{"type": "TBT", "pages": [0, 1]}},
  {{"type": "UNKNOWN", "pages": [2]}}  âŒ

CHá»ˆ TRáº¢ Vá»€ "UNKNOWN" KHI:
- Trang hoÃ n toÃ n láº¡ (khÃ´ng cÃ³ title, khÃ´ng cÃ³ continuation patterns)
- Title thá»±c sá»± khÃ´ng thuá»™c 98 loáº¡i (VD: "Báº¢N GIáº¢I TRÃŒNH", "VÄ‚N Báº¢N YÃŠU Cáº¦U")
- Trang trá»‘ng, scan lá»—i, khÃ´ng Ä‘á»c Ä‘Æ°á»£c

---

"""

    output_format = f"""

---

ğŸ¯ OUTPUT FORMAT - Báº®T BUá»˜C:

{{
  "documents": [
    {{
      "type": "HDCQ",
      "pages": [0, 1, 2, 3, 4],
      "confidence": 0.95,
      "reasoning": "5 trang Ä‘áº§u cÃ¹ng format, trang 0 cÃ³ tiÃªu Ä‘á» 'Há»¢P Äá»’NG CHUYá»‚N NHÆ¯á»¢NG', trang 1-4 lÃ  continuation pages vá»›i ÄIá»€U 2, ÄIá»€U 3",
      "metadata": {{}}
    }},
    {{
      "type": "GCN",
      "pages": [5, 6],
      "confidence": 0.98,
      "reasoning": "Trang 5-6 lÃ  GCN mÃ u há»“ng, cÃ³ quá»‘c huy, tÃ¬m tháº¥y ngÃ y cáº¥p á»Ÿ trang 6",
      "metadata": {{
        "color": "pink",
        "issue_date": "27/10/2021",
        "issue_date_confidence": "full"
      }}
    }},
    {{
      "type": "UNKNOWN",
      "pages": [7, 8, 9],
      "confidence": 0.3,
      "reasoning": "3 trang cuá»‘i khÃ´ng rÃµ rÃ ng, khÃ´ng cÃ³ tiÃªu Ä‘á», khÃ´ng match 98 loáº¡i",
      "metadata": {{}}
    }}
  ]
}}

ğŸš¨ Cá»°C Ká»² QUAN TRá»ŒNG - Báº®T BUá»˜C RETURN Táº¤T Cáº¢ {batch_size} PAGES:
- Báº¡n PHáº¢I assign Má»ŒI page (0 Ä‘áº¿n {batch_size-1}) vÃ o 1 document
- Náº¿u page khÃ´ng rÃµ â†’ assign vÃ o document type "UNKNOWN"
- KHÃ”NG BAO GIá»œ bá» qua page nÃ o
- Tá»•ng sá»‘ pages trong "pages" arrays = {batch_size}

VÃ Dá»¤ ÄÃšNG ({batch_size} pages):
- Document 1: pages [0,1,2,3,4] (5 pages)
- Document 2: pages [5,6,7,8] (4 pages)
- Document 3: pages [9,10,...,{batch_size-1}] ({batch_size-9} pages)
â†’ Total: {batch_size} pages âœ…

VÃ Dá»¤ SAI:
- Document 1: pages [0,1,2] (3 pages only)
- Document 2: pages [5,6] (2 pages, SKIP pages 3-4!)
â†’ Total: 5 pages âŒ (Missing pages 3,4,7,8,...,{batch_size-1})

INDEXING:
- pages dÃ¹ng 0-indexed (trang Ä‘áº§u tiÃªn = 0, trang cuá»‘i = {batch_size-1})
- Náº¿u chá»‰ cÃ³ 1 document â†’ váº«n tráº£ vá» array vá»›i 1 pháº§n tá»­
"""

    # Combine: intro + original rules + unknown rules + output format
    full_multi_prompt = multi_image_intro + single_image_prompt + unknown_rules + output_format
    
    return full_multi_prompt


def get_multi_image_prompt_full(batch_size):
    """Get FULL prompt (Flash Full rules) for multi-image batch"""
    single_prompt = get_classification_prompt()
    return adapt_prompt_for_multi_image(single_prompt, batch_size)


def get_multi_image_prompt_lite(batch_size):
    """Get LITE prompt (Flash Lite rules) for multi-image batch"""
    single_prompt = get_classification_prompt_lite()
    return adapt_prompt_for_multi_image(single_prompt, batch_size)


def encode_image_base64(image_path, max_width=1500, max_height=2100):
    """Encode image to base64 with smart resize"""
    try:
        img = Image.open(image_path)
        
        # Convert to RGB if needed
        if img.mode in ('RGBA', 'LA', 'P'):
            img = img.convert('RGB')
        
        # Resize
        resized_img, resize_info = resize_image_smart(img, max_width, max_height)
        
        # Encode to base64
        buffer = io.BytesIO()
        resized_img.save(buffer, format='JPEG', quality=95)
        img_bytes = buffer.getvalue()
        encoded = base64.b64encode(img_bytes).decode('utf-8')
        
        return encoded, resize_info
    except Exception as e:
        print(f"Error encoding image {image_path}: {e}", file=sys.stderr)
        return None, None




def batch_classify_fixed(image_paths, api_key, engine_type='gemini-flash', batch_size=5, last_known_type=None):
    """
    PhÆ°Æ¡ng Ã¡n 1: Fixed Batch Size vá»›i SEQUENTIAL METADATA
    
    Args:
        image_paths: List of file paths
        api_key: Google API key
        engine_type: 'gemini-flash', 'gemini-flash-lite', or 'gemini-flash-hybrid'
        batch_size: Files per batch
        last_known_type: Metadata tá»« file cuá»‘i batch trÆ°á»›c {short_code, confidence, has_title}
    
    Strategy:
        - Batch 1: Process files 0-4, return lastKnown tá»« file 4
        - Batch 2: Process files 5-9 WITH lastKnown tá»« file 4
          * File 5 cÃ³ title â†’ Bá» qua lastKnown, dÃ¹ng title má»›i
          * File 5 khÃ´ng cÃ³ title â†’ Ãp dá»¥ng sequential tá»« lastKnown
        - No overlap needed â†’ 0% overhead!
    """
    
    # Determine model and prompt based on engine type
    if engine_type == 'gemini-flash-lite':
        model_name = 'gemini-2.5-flash-lite'
        prompt_getter = get_multi_image_prompt_lite
        print(f"\n{'='*80}", file=sys.stderr)
        print(f"âš¡ BATCH MODE: Fixed ({batch_size} files, NO overlap) + Flash LITE", file=sys.stderr)
        print(f"   Model: {model_name}", file=sys.stderr)
        print("   Prompt: Lite (simplified, 60% crop rules)", file=sys.stderr)
        print("   Metadata: Sequential naming from previous batch", file=sys.stderr)
        print(f"{'='*80}", file=sys.stderr)
    elif engine_type == 'gemini-flash-hybrid':
        model_name = 'gemini-2.5-flash-lite'  # Start with Lite for hybrid
        prompt_getter = get_multi_image_prompt_lite
        print(f"\n{'='*80}", file=sys.stderr)
        print(f"ğŸ”„ BATCH MODE: Fixed ({batch_size} files, NO overlap) + HYBRID", file=sys.stderr)
        print("   Strategy: Two-tier (Lite â†’ Full if low confidence)", file=sys.stderr)
        print(f"   Model (Tier 1): {model_name}", file=sys.stderr)
        print("   Metadata: Sequential naming from previous batch", file=sys.stderr)
        print(f"{'='*80}", file=sys.stderr)
    else:  # gemini-flash (default)
        model_name = 'gemini-2.5-flash'
        prompt_getter = get_multi_image_prompt_full
        print(f"\n{'='*80}", file=sys.stderr)
        print(f"ğŸ¤– BATCH MODE: Fixed ({batch_size} files, NO overlap) + Flash FULL", file=sys.stderr)
        print(f"   Model: {model_name}", file=sys.stderr)
        print("   Prompt: Full (complete 98-rule classification)", file=sys.stderr)
        print("   Metadata: Sequential naming from previous batch", file=sys.stderr)
        print(f"{'='*80}", file=sys.stderr)
    
    if last_known_type:
        print(f"\nğŸ“Œ Received lastKnown from previous batch:", file=sys.stderr)
        print(f"   Type: {last_known_type.get('short_code')}", file=sys.stderr)
        print(f"   Confidence: {last_known_type.get('confidence', 0):.0%}", file=sys.stderr)
        print(f"   Has title: {last_known_type.get('has_title', False)}", file=sys.stderr)
    
    all_results = []
    processed_files = set()  # Track processed files to detect missing ones
    batch_num = 0
    current_idx = 0
    
    while current_idx < len(image_paths):
        batch_num += 1
        
        # Calculate batch range with overlap
        batch_start = max(0, current_idx - overlap) if batch_num > 1 else current_idx
        batch_end = min(len(image_paths), current_idx + batch_size)
        batch_paths = image_paths[batch_start:batch_end]
        
        # Track which files are NEW in this batch (not overlap)
        new_file_start_idx = current_idx - batch_start
        
        print(f"\nğŸ“¦ Batch {batch_num}: Files {batch_start}-{batch_end-1} ({len(batch_paths)} images)", file=sys.stderr)
        if batch_num > 1:
            print(f"   â†©ï¸ Overlap: {overlap} files from previous batch (for context)", file=sys.stderr)
            print(f"   ğŸ†• New files: {batch_end - current_idx} (starting from index {new_file_start_idx})", file=sys.stderr)
        
        for i, path in enumerate(batch_paths):
            marker = "ğŸ†•" if i >= new_file_start_idx else "â†©ï¸"
            print(f"   [{i}] {marker} {os.path.basename(path)}", file=sys.stderr)
        
        # Encode all images in batch
        print(f"ğŸ–¼ï¸ Encoding {len(batch_paths)} images...", file=sys.stderr)
        encoded_images = []
        for path in batch_paths:
            encoded, resize_info = encode_image_base64(path)
            if encoded:
                encoded_images.append(encoded)
                print(f"   âœ… {os.path.basename(path)}: {resize_info.get('original_size', 'N/A')} â†’ {resize_info.get('new_size', 'N/A')}", file=sys.stderr)
            else:
                print(f"   âŒ Failed to encode {os.path.basename(path)}", file=sys.stderr)
        
        if not encoded_images:
            print(f"âŒ No valid images in batch {batch_num}", file=sys.stderr)
            continue
        
        # Build multi-image payload
        parts = [{"text": prompt_getter(len(batch_paths))}]
        for img_data in encoded_images:
            parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": img_data
                }
            })
        
        payload = {
            "contents": [{"parts": parts}],
            "generationConfig": {
                "temperature": 0.1,
                "topP": 0.8,
                "topK": 10,
                "maxOutputTokens": 8000,  # Large enough for 20 documents Ã— 400 tokens each
                "responseMimeType": "application/json"  # Force JSON output
            },
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"}
            ]
        }
        
        # Call Gemini API
        print(f"ğŸ“¡ Sending batch request to {model_name}...", file=sys.stderr)
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}"
        
        try:
            response = requests.post(api_url, json=payload, timeout=120)
            response.raise_for_status()
            result_data = response.json()
            
            print(f"ğŸ“Š Response status: {response.status_code}", file=sys.stderr)
            
            # Debug: Check finish reason and safety
            if 'candidates' in result_data and len(result_data['candidates']) > 0:
                candidate = result_data['candidates'][0]
                finish_reason = candidate.get('finishReason', 'UNKNOWN')
                print(f"ğŸ” Finish reason: {finish_reason}", file=sys.stderr)
                
                if finish_reason == 'MAX_TOKENS':
                    print("âš ï¸ WARNING: Response truncated due to MAX_TOKENS!", file=sys.stderr)
                    print("   Some pages may be missing from response", file=sys.stderr)
            
            # Parse response
            if 'candidates' in result_data and len(result_data['candidates']) > 0:
                candidate = result_data['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    parts = candidate['content']['parts']
                    if len(parts) > 0 and 'text' in parts[0]:
                        response_text = parts[0]['text']
                        
                        print(f"ğŸ“„ Raw response preview: {response_text[:200]}...", file=sys.stderr)
                        
                        # Extract JSON from response - try multiple patterns
                        json_match = re.search(r'\{[\s\S]*"documents"[\s\S]*\}', response_text)
                        if not json_match:
                            # Try finding JSON with triple backticks
                            json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', response_text)
                            if json_match:
                                response_text = json_match.group(1)
                            else:
                                # Try finding any JSON object
                                json_match = re.search(r'(\{[\s\S]*\})', response_text)
                                if json_match:
                                    response_text = json_match.group(1)
                        else:
                            response_text = json_match.group(0)
                        
                        if response_text:
                            try:
                                batch_result = json.loads(response_text)
                            
                                print(f"âœ… Batch {batch_num} complete:", file=sys.stderr)
                                
                                # Validate: Check if all pages are covered
                                total_pages_in_batch = len(batch_paths)
                                pages_returned = set()
                                
                                for doc in batch_result.get('documents', []):
                                    doc_type = doc.get('type', 'UNKNOWN')
                                    pages = doc.get('pages', [])
                                    confidence = doc.get('confidence', 0)
                                    print(f"   ğŸ“„ {doc_type}: {len(pages)} pages, confidence {confidence:.0%}", file=sys.stderr)
                                    
                                    # Collect all page indices
                                    for p in pages:
                                        pages_returned.add(p)
                                
                                # Check for missing pages
                                expected_pages = set(range(total_pages_in_batch))
                                missing_pages = expected_pages - pages_returned
                                
                                if missing_pages:
                                    print(f"   âš ï¸ WARNING: AI didn't return {len(missing_pages)} pages: {sorted(missing_pages)}", file=sys.stderr)
                                    print("      These files will be processed by fallback", file=sys.stderr)
                                else:
                                    print(f"   âœ… All {total_pages_in_batch} pages accounted for", file=sys.stderr)
                                
                                # Map results back to original file paths
                                # ONLY process NEW files (skip overlap files)
                                for doc in batch_result.get('documents', []):
                                    for page_idx in doc.get('pages', []):
                                        # Check if this is a NEW file (not overlap)
                                        if page_idx >= new_file_start_idx and page_idx < len(batch_paths):
                                            file_path = batch_paths[page_idx]
                                            
                                            # Skip if already processed (from previous batch)
                                            if file_path in processed_files:
                                                print(f"   â­ï¸ Skipping duplicate: {os.path.basename(file_path)}", file=sys.stderr)
                                                continue
                                            
                                            processed_files.add(file_path)  # Track this file
                                            all_results.append({
                                                'file_path': file_path,
                                                'file_name': os.path.basename(file_path),
                                                'short_code': doc.get('type', 'UNKNOWN'),
                                                'confidence': doc.get('confidence', 0.5),
                                                'reasoning': doc.get('reasoning', ''),
                                                'metadata': doc.get('metadata', {}),
                                                'method': 'batch_fixed',
                                                'batch_num': batch_num
                                            })
                            except json.JSONDecodeError as je:
                                print(f"âš ï¸ JSON decode error in batch {batch_num}: {je}", file=sys.stderr)
                                print(f"   Response text: {response_text[:500]}...", file=sys.stderr)
                        else:
                            print(f"âš ï¸ No valid JSON in response for batch {batch_num}", file=sys.stderr)
            
        except Exception as e:
            print(f"âŒ Batch {batch_num} error: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
        
        # Move to next batch (increment by batch_size, not batch_end)
        current_idx += batch_size
    
    print(f"\n{'='*80}", file=sys.stderr)
    print(f"âœ… BATCH MODE 1 COMPLETE: {len(all_results)} files processed", file=sys.stderr)
    
    # Detect missing files
    all_input_files = set(image_paths)
    missing_files = all_input_files - processed_files
    
    if missing_files:
        print(f"âš ï¸ WARNING: {len(missing_files)} files were NOT processed by AI:", file=sys.stderr)
        for missing_file in sorted(missing_files):
            print(f"   âŒ {os.path.basename(missing_file)}", file=sys.stderr)
        print("   Possible causes: AI didn't return page indices, JSON parsing error", file=sys.stderr)
        print(f"\nğŸ”„ FALLBACK: Processing {len(missing_files)} missing files individually...", file=sys.stderr)
        
        # Fallback: Process missing files with single-file tier1 scan
        for missing_file in sorted(missing_files):
            try:
                print(f"   ğŸ”„ Processing {os.path.basename(missing_file)}...", file=sys.stderr)
                result = quick_scan_tier1(missing_file, api_key)
                all_results.append({
                    'file_path': missing_file,
                    'file_name': os.path.basename(missing_file),
                    'short_code': result.get('short_code', 'UNKNOWN'),
                    'confidence': result.get('confidence', 0.5),
                    'reasoning': result.get('reasoning', 'Fallback single-file scan'),
                    'metadata': result.get('metadata', {}),
                    'method': 'batch_fallback',
                    'batch_num': 'fallback'
                })
                print(f"      âœ… {result.get('short_code', 'UNKNOWN')} ({result.get('confidence', 0):.0%})", file=sys.stderr)
            except Exception as e:
                print(f"      âŒ Error: {e}", file=sys.stderr)
                # Add as UNKNOWN if fallback also fails
                all_results.append({
                    'file_path': missing_file,
                    'file_name': os.path.basename(missing_file),
                    'short_code': 'UNKNOWN',
                    'confidence': 0.0,
                    'reasoning': f'Fallback failed: {str(e)}',
                    'metadata': {},
                    'method': 'batch_fallback_failed',
                    'batch_num': 'fallback'
                })
        
        print(f"âœ… Fallback complete: {len(all_results)} total results (original + fallback)", file=sys.stderr)
    else:
        print(f"âœ… All {len(all_input_files)} input files were successfully processed", file=sys.stderr)
    
    print(f"{'='*80}", file=sys.stderr)
    
    return all_results


def quick_scan_tier1(image_path, api_key):
    """Quick scan vá»›i Tier 1 Ä‘á»ƒ detect document boundaries"""
    from ocr_engine_gemini_flash import classify_document_gemini_flash
    
    try:
        result = classify_document_gemini_flash(
            image_path=image_path,
            api_key=api_key,
            crop_top_percent=0.60,
            model_type='gemini-flash-lite',
            enable_resize=True,
            max_width=1500,
            max_height=2100
        )
        return result
    except Exception as e:
        print(f"Quick scan error for {image_path}: {e}", file=sys.stderr)
        return {'short_code': 'ERROR', 'confidence': 0}


def group_by_document(quick_results, file_paths):
    """
    NhÃ³m files thÃ nh documents dá»±a trÃªn quick scan results
    Returns: List of document groups [[0,1,2], [3,4], [5,6,7,8], ...]
    """
    print("\nğŸ§  Analyzing document boundaries...", file=sys.stderr)
    
    groups = []
    current_group = [0]
    last_type = quick_results[0].get('short_code', 'UNKNOWN')
    
    for i in range(1, len(quick_results)):
        result = quick_results[i]
        short_code = result.get('short_code', 'UNKNOWN')
        confidence = result.get('confidence', 0)
        reasoning = result.get('reasoning', '').lower()
        
        # Check if this is a new document
        is_new_document = False
        
        # High confidence with clear title â†’ New document
        if confidence >= 0.8 and short_code != 'UNKNOWN':
            is_new_document = True
            print(f"   ğŸ“„ [{i}] New document detected: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Low confidence + continuation indicators â†’ Same document
        elif confidence < 0.5 and any(kw in reasoning for kw in ['section header', 'ii.', 'iii.', 'thá»­a Ä‘áº¥t']):
            is_new_document = False
            print(f"   â¡ï¸ [{i}] Continuation page: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Borderline case - use confidence
        else:
            is_new_document = (confidence >= 0.7)
            print(f"   â“ [{i}] Borderline: {short_code} ({confidence:.0%}) â†’ {'New' if is_new_document else 'Continue'}", file=sys.stderr)
        
        if is_new_document:
            # Start new group
            groups.append(current_group)
            current_group = [i]
            last_type = short_code
        else:
            # Continue current group
            current_group.append(i)
    
    # Add last group
    if current_group:
        groups.append(current_group)
    
    print(f"\nâœ… Grouped into {len(groups)} documents:", file=sys.stderr)
    for g_idx, group in enumerate(groups):
        print(f"   Document {g_idx + 1}: {len(group)} pages {group}", file=sys.stderr)
    
    return groups


def batch_classify_smart(image_paths, api_key, engine_type='gemini-flash'):
    """
    PhÆ°Æ¡ng Ã¡n 2: Smart Batching - TRUE AI-POWERED vá»›i OVERLAP
    Gá»­i nhiá»u files (10-20) vá»›i overlap Ä‘á»ƒ AI cÃ³ full context
    """
    print(f"\n{'='*80}", file=sys.stderr)
    print("ğŸ§  BATCH MODE 2: Smart Batching (AI Document Detection)", file=sys.stderr)
    print(f"{'='*80}", file=sys.stderr)
    
    total_files = len(image_paths)
    
    # Smart batch size strategy vá»›i overlap
    if total_files <= 20:
        # Small batch: Send all at once, no overlap needed
        batch_size = total_files
        overlap = 0
        print(f"ğŸ“Š Strategy: Send ALL {total_files} files in 1 batch", file=sys.stderr)
    elif total_files <= 60:
        # Medium batch: 20 files per batch, 5 files overlap
        batch_size = 20
        overlap = 5
        print(f"ğŸ“Š Strategy: Send {batch_size} files per batch vá»›i {overlap} files overlap", file=sys.stderr)
    else:
        # Large batch: 15 files per batch, 4 files overlap
        batch_size = 15
        overlap = 4
        print(f"ğŸ“Š Strategy: Send {batch_size} files per batch vá»›i {overlap} files overlap (large dataset)", file=sys.stderr)
    
    if overlap > 0:
        print(f"   â†©ï¸ Overlap purpose: Batch sau tháº¥y {overlap} files cuá»‘i cá»§a batch trÆ°á»›c", file=sys.stderr)
        print("   Why? File 16 khÃ´ng cÃ³ title â†’ cáº§n tháº¥y file 14-15 Ä‘á»ƒ biáº¿t nÃ³ thuá»™c document nÃ o", file=sys.stderr)
    
    print("   AI needs 10-20 files to detect document boundaries accurately", file=sys.stderr)
    
    # Use fixed batch with smart size + overlap + engine type
    return batch_classify_fixed(image_paths, api_key, engine_type=engine_type, batch_size=batch_size, overlap=overlap)


# CLI interface for testing
if __name__ == "__main__":
    if len(sys.argv) < 4:
        print("Usage: python batch_processor.py <mode> <engine_type> <api_key> <image1> <image2> ...", file=sys.stderr)
        print("Modes: fixed, smart", file=sys.stderr)
        print("Engine types: gemini-flash, gemini-flash-lite, gemini-flash-hybrid", file=sys.stderr)
        print("Example: python batch_processor.py smart gemini-flash-hybrid AIza... img1.jpg img2.jpg img3.jpg", file=sys.stderr)
        sys.exit(1)
    
    mode = sys.argv[1]
    engine_type = sys.argv[2]
    api_key = sys.argv[3]
    image_paths = sys.argv[4:]
    
    print(f"ğŸ” Batch processing {len(image_paths)} images in '{mode}' mode with '{engine_type}'", file=sys.stderr)
    
    if mode == 'fixed':
        results = batch_classify_fixed(image_paths, api_key, engine_type=engine_type, batch_size=5, overlap=2)
    elif mode == 'smart':
        results = batch_classify_smart(image_paths, api_key, engine_type=engine_type)
    else:
        print(f"âŒ Unknown mode: {mode}", file=sys.stderr)
        sys.exit(1)
    
    # Output JSON to stdout for IPC
    print(json.dumps(results, ensure_ascii=False))
    
    print(f"\nğŸ“Š BATCH COMPLETE: {len(results)} files processed", file=sys.stderr)
