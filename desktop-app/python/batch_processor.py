#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Batch Document Processor - Multi-Image Analysis
Supports 2 modes:
- Mode 1: Fixed batch size (5 images per batch)
- Mode 2: Smart batching (group by document boundaries)
"""

import sys
import os
import json
import base64
import re
import requests
from PIL import Image
import io

# Import existing engines and prompts
from ocr_engine_gemini_flash import (
    resize_image_smart, 
    parse_gemini_response,
    get_classification_prompt,
    get_classification_prompt_lite
)


def adapt_prompt_for_multi_image(single_image_prompt, batch_size):
    """
    Adapt single-image prompt to multi-image batch context
    
    Changes:
    1. Add multi-image context introduction
    2. Add document grouping instructions
    3. Change output format from single result to documents array
    4. Add page indexing (0-indexed)
    5. Emphasize MUST return ALL pages
    """
    
    multi_image_intro = f"""üéØ BATCH ANALYSIS - {batch_size} TRANG SCAN

B·∫°n ƒëang ph√¢n t√≠ch {batch_size} trang scan t√†i li·ªáu ƒë·∫•t ƒëai Vi·ªát Nam.
C√°c trang n√†y c√≥ th·ªÉ thu·ªôc 1 ho·∫∑c nhi·ªÅu t√†i li·ªáu kh√°c nhau.

NHI·ªÜM V·ª§:
1. X√°c ƒë·ªãnh c√≥ BAO NHI√äU t√†i li·ªáu kh√°c nhau trong {batch_size} trang n√†y
2. Nh√≥m c√°c trang theo t√†i li·ªáu (pages array)
3. Ph√¢n lo·∫°i lo·∫°i t√†i li·ªáu c·ªßa t·ª´ng nh√≥m
4. Tr√≠ch xu·∫•t metadata (ng√†y c·∫•p cho GCN, m√†u s·∫Øc, v.v.)

D·∫§U HI·ªÜU NH·∫¨N BI·∫æT:

TRANG 1 C·ª¶A T√ÄI LI·ªÜU (New Document):
- C√≥ TI√äU ƒê·ªÄ CH√çNH ·ªü TOP 30% (ƒë·∫ßu trang)
- C·ª° ch·ªØ L·ªöN, IN HOA, cƒÉn gi·ªØa
- C√≥ qu·ªëc huy (ƒë·ªëi v·ªõi GCN)
- Kh√°c bi·ªát r√µ v·ªÅ format/m√†u s·∫Øc so v·ªõi trang tr∆∞·ªõc

TRANG TI·∫æP N·ªêI (Continuation - Trang 2, 3, 4...):
- KH√îNG c√≥ ti√™u ƒë·ªÅ ch√≠nh ·ªü ƒë·∫ßu
- Ch·ªâ c√≥ section headers: "II.", "III.", "ƒêI·ªÄU 2", "PH·∫¶N II"
- C√πng format/m√†u s·∫Øc v·ªõi trang tr∆∞·ªõc
- N·ªôi dung li√™n t·ª•c (ƒëi·ªÅu kho·∫£n, ch·ªØ k√Ω, b·∫£ng bi·ªÉu)

üö® QUAN TR·ªåNG - NH·∫¨N DI·ªÜN CONTINUATION PAGES:
C√°c d·∫•u hi·ªáu sau = CONTINUATION (trang ti·∫øp theo, KH√îNG ph·∫£i document m·ªõi):
1. Section headers v·ªõi s·ªë: "II.", "III.", "IV.", "V.", "ƒêI·ªÄU 2", "ƒêI·ªÄU 3", "PH·∫¶N II", "M·ª§C III"
2. B·∫£ng bi·ªÉu v·ªõi s·ªë ph√¢n c·∫•p: "4.1", "4.2", "4.2.1", "4.2.2", "(1.1)", "(2.1.3)"
3. Text body ti·∫øp n·ªëi: "...ti·∫øp theo...", "...nh∆∞ sau:", danh s√°ch bullet points
4. Ch·ªØ k√Ω/con d·∫•u ·ªü cu·ªëi trang
5. Kh√¥ng c√≥ header ch√≠nh th·ª©c (qu·ªëc huy, c∆° quan ban h√†nh)

V√ç D·ª§ CONTINUATION - PH·∫¢I GOM V√ÄO DOCUMENT TR∆Ø·ªöC:
‚úÖ "III. T√çNH THU·∫æ C·ª¶A C∆† QUAN THU·∫æ" + b·∫£ng 4.1, 4.2
   ‚Üí Section header v·ªõi s·ªë La M√£
   ‚Üí B·∫£ng bi·ªÉu ph√¢n c·∫•p
   ‚Üí ƒê√¢y l√† continuation c·ªßa document tr∆∞·ªõc (c√≥ th·ªÉ l√† TBT, HDCQ, etc.)
   ‚Üí KH√îNG classify th√†nh UNKNOWN
   ‚Üí GOM V√ÄO document c√≥ trang tr∆∞·ªõc ƒë√≥
   
   V√ç D·ª§ C·ª§ TH·ªÇ:
   Page 4 (index 4): "TH√îNG B√ÅO THU·∫æ" (title) ‚Üí TBT
   Page 5 (index 5): "ƒêI·ªÄU 1: ..." ‚Üí TBT continuation
   Page 6 (index 6): "III. T√çNH THU·∫æ" + b·∫£ng 4.1 ‚Üí TBT continuation ‚úÖ (KH√îNG ph·∫£i UNKNOWN)
   
   Result: {"type": "TBT", "pages": [4, 5, 6], ...}

‚úÖ "ƒêI·ªÄU 2: N·ªòI DUNG TH·ªéA THU·∫¨N PH√ÇN CHIA"
   ‚Üí Section header
   ‚Üí Continuation c·ªßa TTHGD ho·∫∑c PCTSVC
   ‚Üí GOM V√ÄO document tr∆∞·ªõc

‚úÖ Trang ch·ªâ c√≥ b·∫£ng bi·ªÉu (kh√¥ng c√≥ title)
   ‚Üí Continuation
   ‚Üí GOM V√ÄO document tr∆∞·ªõc

‚úÖ Trang c√≥ "1.1 Tr∆∞·ªùng h·ª£p...", "1.2 Tr∆∞·ªùng h·ª£p..." (numbered list)
   ‚Üí Continuation v·ªõi structured content
   ‚Üí GOM V√ÄO document tr∆∞·ªõc

‚ùå SAI - Classify continuation th√†nh UNKNOWN:
Page 5: "TH√îNG B√ÅO THU·∫æ" (title)
Page 6: "III. T√çNH THU·∫æ" + b·∫£ng
‚Üí AI classify:
  {"type": "TBT", "pages": [5]},
  {"type": "UNKNOWN", "pages": [6]}  ‚ùå SAI!
‚Üí ƒê√öNG:
  {"type": "TBT", "pages": [5, 6]}  ‚úÖ

RANH GI·ªöI GI·ªÆA C√ÅC T√ÄI LI·ªÜU:
- Thay ƒë·ªïi r√µ r·ªát: m√†u gi·∫•y (h·ªìng ‚Üí tr·∫Øng), format kh√°c
- Xu·∫•t hi·ªán ti√™u ƒë·ªÅ ch√≠nh m·ªõi ·ªü TOP
- Layout ho√†n to√†n kh√°c

---

"""

    unknown_rules = f"""

‚ö†Ô∏è QUAN TR·ªåNG - KHI N√ÄO TR·∫¢ V·ªÄ "UNKNOWN":
CH·ªà tr·∫£ v·ªÅ "UNKNOWN" khi:
1. Trang th·ª±c s·ª± kh√¥ng c√≥ ti√™u ƒë·ªÅ V√Ä kh√¥ng match continuation patterns
2. Title kh√¥ng thu·ªôc 98 lo·∫°i V√Ä kh√¥ng ph·∫£i continuation
3. Trang ho√†n to√†n tr·ªëng ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c

‚ùå KH√îNG tr·∫£ v·ªÅ "UNKNOWN" cho:
- Trang c√≥ section headers (III., ƒêI·ªÄU 2) ‚Üí Continuation, gom v√†o doc tr∆∞·ªõc
- Trang c√≥ b·∫£ng bi·ªÉu structured ‚Üí Continuation, gom v√†o doc tr∆∞·ªõc
- Trang c√≥ text li√™n t·ª•c v·ªõi trang tr∆∞·ªõc ‚Üí Continuation, gom v√†o doc tr∆∞·ªõc

üéØ NGUY√äN T·∫ÆC: Khi nghi ng·ªù ‚Üí Gom v√†o document tr∆∞·ªõc (safer than creating new UNKNOWN doc)

V√ç D·ª§:
Page 0: "TH√îNG B√ÅO THU·∫æ" ‚Üí TBT
Page 1: "ƒêI·ªÄU 1: ..." ‚Üí TBT continuation
Page 2: "III. T√çNH THU·∫æ" + b·∫£ng ‚Üí TBT continuation (KH√îNG ph·∫£i UNKNOWN)

Result: {{"type": "TBT", "pages": [0, 1, 2], ...}} ‚úÖ

---

"""

    output_format = f"""

---

üéØ OUTPUT FORMAT - B·∫ÆT BU·ªòC:

{{
  "documents": [
    {{
      "type": "HDCQ",
      "pages": [0, 1, 2, 3, 4],
      "confidence": 0.95,
      "reasoning": "5 trang ƒë·∫ßu c√πng format, trang 0 c√≥ ti√™u ƒë·ªÅ 'H·ª¢P ƒê·ªíNG CHUY·ªÇN NH∆Ø·ª¢NG', trang 1-4 l√† continuation pages v·ªõi ƒêI·ªÄU 2, ƒêI·ªÄU 3",
      "metadata": {{}}
    }},
    {{
      "type": "GCN",
      "pages": [5, 6],
      "confidence": 0.98,
      "reasoning": "Trang 5-6 l√† GCN m√†u h·ªìng, c√≥ qu·ªëc huy, t√¨m th·∫•y ng√†y c·∫•p ·ªü trang 6",
      "metadata": {{
        "color": "pink",
        "issue_date": "27/10/2021",
        "issue_date_confidence": "full"
      }}
    }},
    {{
      "type": "UNKNOWN",
      "pages": [7, 8, 9],
      "confidence": 0.3,
      "reasoning": "3 trang cu·ªëi kh√¥ng r√µ r√†ng, kh√¥ng c√≥ ti√™u ƒë·ªÅ, kh√¥ng match 98 lo·∫°i",
      "metadata": {{}}
    }}
  ]
}}

üö® C·ª∞C K·ª≤ QUAN TR·ªåNG - B·∫ÆT BU·ªòC RETURN T·∫§T C·∫¢ {batch_size} PAGES:
- B·∫°n PH·∫¢I assign M·ªåI page (0 ƒë·∫øn {batch_size-1}) v√†o 1 document
- N·∫øu page kh√¥ng r√µ ‚Üí assign v√†o document type "UNKNOWN"
- KH√îNG BAO GI·ªú b·ªè qua page n√†o
- T·ªïng s·ªë pages trong "pages" arrays = {batch_size}

V√ç D·ª§ ƒê√öNG ({batch_size} pages):
- Document 1: pages [0,1,2,3,4] (5 pages)
- Document 2: pages [5,6,7,8] (4 pages)
- Document 3: pages [9,10,...,{batch_size-1}] ({batch_size-9} pages)
‚Üí Total: {batch_size} pages ‚úÖ

V√ç D·ª§ SAI:
- Document 1: pages [0,1,2] (3 pages only)
- Document 2: pages [5,6] (2 pages, SKIP pages 3-4!)
‚Üí Total: 5 pages ‚ùå (Missing pages 3,4,7,8,...,{batch_size-1})

INDEXING:
- pages d√πng 0-indexed (trang ƒë·∫ßu ti√™n = 0, trang cu·ªëi = {batch_size-1})
- N·∫øu ch·ªâ c√≥ 1 document ‚Üí v·∫´n tr·∫£ v·ªÅ array v·ªõi 1 ph·∫ßn t·ª≠
"""

    # Combine: intro + original rules + unknown rules + output format
    full_multi_prompt = multi_image_intro + single_image_prompt + unknown_rules + output_format
    
    return full_multi_prompt


def get_multi_image_prompt_full(batch_size):
    """Get FULL prompt (Flash Full rules) for multi-image batch"""
    single_prompt = get_classification_prompt()
    return adapt_prompt_for_multi_image(single_prompt, batch_size)


def get_multi_image_prompt_lite(batch_size):
    """Get LITE prompt (Flash Lite rules) for multi-image batch"""
    single_prompt = get_classification_prompt_lite()
    return adapt_prompt_for_multi_image(single_prompt, batch_size)


def encode_image_base64(image_path, max_width=1500, max_height=2100):
    """Encode image to base64 with smart resize"""
    try:
        img = Image.open(image_path)
        
        # Convert to RGB if needed
        if img.mode in ('RGBA', 'LA', 'P'):
            img = img.convert('RGB')
        
        # Resize
        resized_img, resize_info = resize_image_smart(img, max_width, max_height)
        
        # Encode to base64
        buffer = io.BytesIO()
        resized_img.save(buffer, format='JPEG', quality=95)
        img_bytes = buffer.getvalue()
        encoded = base64.b64encode(img_bytes).decode('utf-8')
        
        return encoded, resize_info
    except Exception as e:
        print(f"Error encoding image {image_path}: {e}", file=sys.stderr)
        return None, None




def batch_classify_fixed(image_paths, api_key, batch_size=5, overlap=3):
    """
    Ph∆∞∆°ng √°n 1: Fixed Batch Size v·ªõi OVERLAP
    Gom m·ªói 5 files nh∆∞ng overlap 3 files ƒë·ªÉ gi·ªØ context
    
    V√≠ d·ª• overlap=3, batch_size=15:
      Batch 1: Files 0-14  (15 files)
      Batch 2: Files 12-29 (18 files) ‚Üí Overlap files 12,13,14
      Batch 3: Files 27-44 (18 files) ‚Üí Overlap files 27,28,29
      
    T·∫°i sao? File 15,16,17 c√≥ th·ªÉ l√† continuation c·ªßa file 14.
    N·∫øu batch 2 kh√¥ng th·∫•y file 14 ‚Üí classify sai!
    """
    print(f"\n{'='*80}", file=sys.stderr)
    print(f"üîÑ BATCH MODE 1: Fixed Batch Size ({batch_size} files, overlap {overlap})", file=sys.stderr)
    print(f"{'='*80}", file=sys.stderr)
    
    all_results = []
    processed_files = set()  # Track processed files to detect missing ones
    batch_num = 0
    current_idx = 0
    
    while current_idx < len(image_paths):
        batch_num += 1
        
        # Calculate batch range with overlap
        batch_start = max(0, current_idx - overlap) if batch_num > 1 else current_idx
        batch_end = min(len(image_paths), current_idx + batch_size)
        batch_paths = image_paths[batch_start:batch_end]
        
        # Track which files are NEW in this batch (not overlap)
        new_file_start_idx = current_idx - batch_start
        
        print(f"\nüì¶ Batch {batch_num}: Files {batch_start}-{batch_end-1} ({len(batch_paths)} images)", file=sys.stderr)
        if batch_num > 1:
            print(f"   ‚Ü©Ô∏è Overlap: {overlap} files from previous batch (for context)", file=sys.stderr)
            print(f"   üÜï New files: {batch_end - current_idx} (starting from index {new_file_start_idx})", file=sys.stderr)
        
        for i, path in enumerate(batch_paths):
            marker = "üÜï" if i >= new_file_start_idx else "‚Ü©Ô∏è"
            print(f"   [{i}] {marker} {os.path.basename(path)}", file=sys.stderr)
        
        # Encode all images in batch
        print(f"üñºÔ∏è Encoding {len(batch_paths)} images...", file=sys.stderr)
        encoded_images = []
        for path in batch_paths:
            encoded, resize_info = encode_image_base64(path)
            if encoded:
                encoded_images.append(encoded)
                print(f"   ‚úÖ {os.path.basename(path)}: {resize_info.get('original_size', 'N/A')} ‚Üí {resize_info.get('new_size', 'N/A')}", file=sys.stderr)
            else:
                print(f"   ‚ùå Failed to encode {os.path.basename(path)}", file=sys.stderr)
        
        if not encoded_images:
            print(f"‚ùå No valid images in batch {batch_num}", file=sys.stderr)
            continue
        
        # Build multi-image payload
        parts = [{"text": get_multi_image_prompt_full(len(batch_paths))}]
        for img_data in encoded_images:
            parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": img_data
                }
            })
        
        payload = {
            "contents": [{"parts": parts}],
            "generationConfig": {
                "temperature": 0.1,
                "topP": 0.8,
                "topK": 10,
                "maxOutputTokens": 8000,  # Large enough for 20 documents √ó 400 tokens each
                "responseMimeType": "application/json"  # Force JSON output
            },
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"}
            ]
        }
        
        # Call Gemini API
        print("üì° Sending batch request to Gemini Flash...", file=sys.stderr)
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={api_key}"
        
        try:
            response = requests.post(api_url, json=payload, timeout=120)
            response.raise_for_status()
            result_data = response.json()
            
            print(f"üìä Response status: {response.status_code}", file=sys.stderr)
            
            # Debug: Check finish reason and safety
            if 'candidates' in result_data and len(result_data['candidates']) > 0:
                candidate = result_data['candidates'][0]
                finish_reason = candidate.get('finishReason', 'UNKNOWN')
                print(f"üîç Finish reason: {finish_reason}", file=sys.stderr)
                
                if finish_reason == 'MAX_TOKENS':
                    print("‚ö†Ô∏è WARNING: Response truncated due to MAX_TOKENS!", file=sys.stderr)
                    print("   Some pages may be missing from response", file=sys.stderr)
            
            # Parse response
            if 'candidates' in result_data and len(result_data['candidates']) > 0:
                candidate = result_data['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    parts = candidate['content']['parts']
                    if len(parts) > 0 and 'text' in parts[0]:
                        response_text = parts[0]['text']
                        
                        print(f"üìÑ Raw response preview: {response_text[:200]}...", file=sys.stderr)
                        
                        # Extract JSON from response - try multiple patterns
                        json_match = re.search(r'\{[\s\S]*"documents"[\s\S]*\}', response_text)
                        if not json_match:
                            # Try finding JSON with triple backticks
                            json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', response_text)
                            if json_match:
                                response_text = json_match.group(1)
                            else:
                                # Try finding any JSON object
                                json_match = re.search(r'(\{[\s\S]*\})', response_text)
                                if json_match:
                                    response_text = json_match.group(1)
                        else:
                            response_text = json_match.group(0)
                        
                        if response_text:
                            try:
                                batch_result = json.loads(response_text)
                            
                                print(f"‚úÖ Batch {batch_num} complete:", file=sys.stderr)
                                
                                # Validate: Check if all pages are covered
                                total_pages_in_batch = len(batch_paths)
                                pages_returned = set()
                                
                                for doc in batch_result.get('documents', []):
                                    doc_type = doc.get('type', 'UNKNOWN')
                                    pages = doc.get('pages', [])
                                    confidence = doc.get('confidence', 0)
                                    print(f"   üìÑ {doc_type}: {len(pages)} pages, confidence {confidence:.0%}", file=sys.stderr)
                                    
                                    # Collect all page indices
                                    for p in pages:
                                        pages_returned.add(p)
                                
                                # Check for missing pages
                                expected_pages = set(range(total_pages_in_batch))
                                missing_pages = expected_pages - pages_returned
                                
                                if missing_pages:
                                    print(f"   ‚ö†Ô∏è WARNING: AI didn't return {len(missing_pages)} pages: {sorted(missing_pages)}", file=sys.stderr)
                                    print("      These files will be processed by fallback", file=sys.stderr)
                                else:
                                    print(f"   ‚úÖ All {total_pages_in_batch} pages accounted for", file=sys.stderr)
                                
                                # Map results back to original file paths
                                # ONLY process NEW files (skip overlap files)
                                for doc in batch_result.get('documents', []):
                                    for page_idx in doc.get('pages', []):
                                        # Check if this is a NEW file (not overlap)
                                        if page_idx >= new_file_start_idx and page_idx < len(batch_paths):
                                            file_path = batch_paths[page_idx]
                                            
                                            # Skip if already processed (from previous batch)
                                            if file_path in processed_files:
                                                print(f"   ‚è≠Ô∏è Skipping duplicate: {os.path.basename(file_path)}", file=sys.stderr)
                                                continue
                                            
                                            processed_files.add(file_path)  # Track this file
                                            all_results.append({
                                                'file_path': file_path,
                                                'file_name': os.path.basename(file_path),
                                                'short_code': doc.get('type', 'UNKNOWN'),
                                                'confidence': doc.get('confidence', 0.5),
                                                'reasoning': doc.get('reasoning', ''),
                                                'metadata': doc.get('metadata', {}),
                                                'method': 'batch_fixed',
                                                'batch_num': batch_num
                                            })
                            except json.JSONDecodeError as je:
                                print(f"‚ö†Ô∏è JSON decode error in batch {batch_num}: {je}", file=sys.stderr)
                                print(f"   Response text: {response_text[:500]}...", file=sys.stderr)
                        else:
                            print(f"‚ö†Ô∏è No valid JSON in response for batch {batch_num}", file=sys.stderr)
            
        except Exception as e:
            print(f"‚ùå Batch {batch_num} error: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
        
        # Move to next batch (increment by batch_size, not batch_end)
        current_idx += batch_size
    
    print(f"\n{'='*80}", file=sys.stderr)
    print(f"‚úÖ BATCH MODE 1 COMPLETE: {len(all_results)} files processed", file=sys.stderr)
    
    # Detect missing files
    all_input_files = set(image_paths)
    missing_files = all_input_files - processed_files
    
    if missing_files:
        print(f"‚ö†Ô∏è WARNING: {len(missing_files)} files were NOT processed by AI:", file=sys.stderr)
        for missing_file in sorted(missing_files):
            print(f"   ‚ùå {os.path.basename(missing_file)}", file=sys.stderr)
        print("   Possible causes: AI didn't return page indices, JSON parsing error", file=sys.stderr)
        print(f"\nüîÑ FALLBACK: Processing {len(missing_files)} missing files individually...", file=sys.stderr)
        
        # Fallback: Process missing files with single-file tier1 scan
        for missing_file in sorted(missing_files):
            try:
                print(f"   üîÑ Processing {os.path.basename(missing_file)}...", file=sys.stderr)
                result = quick_scan_tier1(missing_file, api_key)
                all_results.append({
                    'file_path': missing_file,
                    'file_name': os.path.basename(missing_file),
                    'short_code': result.get('short_code', 'UNKNOWN'),
                    'confidence': result.get('confidence', 0.5),
                    'reasoning': result.get('reasoning', 'Fallback single-file scan'),
                    'metadata': result.get('metadata', {}),
                    'method': 'batch_fallback',
                    'batch_num': 'fallback'
                })
                print(f"      ‚úÖ {result.get('short_code', 'UNKNOWN')} ({result.get('confidence', 0):.0%})", file=sys.stderr)
            except Exception as e:
                print(f"      ‚ùå Error: {e}", file=sys.stderr)
                # Add as UNKNOWN if fallback also fails
                all_results.append({
                    'file_path': missing_file,
                    'file_name': os.path.basename(missing_file),
                    'short_code': 'UNKNOWN',
                    'confidence': 0.0,
                    'reasoning': f'Fallback failed: {str(e)}',
                    'metadata': {},
                    'method': 'batch_fallback_failed',
                    'batch_num': 'fallback'
                })
        
        print(f"‚úÖ Fallback complete: {len(all_results)} total results (original + fallback)", file=sys.stderr)
    else:
        print(f"‚úÖ All {len(all_input_files)} input files were successfully processed", file=sys.stderr)
    
    print(f"{'='*80}", file=sys.stderr)
    
    return all_results


def quick_scan_tier1(image_path, api_key):
    """Quick scan v·ªõi Tier 1 ƒë·ªÉ detect document boundaries"""
    from ocr_engine_gemini_flash import classify_document_gemini_flash
    
    try:
        result = classify_document_gemini_flash(
            image_path=image_path,
            api_key=api_key,
            crop_top_percent=0.60,
            model_type='gemini-flash-lite',
            enable_resize=True,
            max_width=1500,
            max_height=2100
        )
        return result
    except Exception as e:
        print(f"Quick scan error for {image_path}: {e}", file=sys.stderr)
        return {'short_code': 'ERROR', 'confidence': 0}


def group_by_document(quick_results, file_paths):
    """
    Nh√≥m files th√†nh documents d·ª±a tr√™n quick scan results
    Returns: List of document groups [[0,1,2], [3,4], [5,6,7,8], ...]
    """
    print("\nüß† Analyzing document boundaries...", file=sys.stderr)
    
    groups = []
    current_group = [0]
    last_type = quick_results[0].get('short_code', 'UNKNOWN')
    
    for i in range(1, len(quick_results)):
        result = quick_results[i]
        short_code = result.get('short_code', 'UNKNOWN')
        confidence = result.get('confidence', 0)
        reasoning = result.get('reasoning', '').lower()
        
        # Check if this is a new document
        is_new_document = False
        
        # High confidence with clear title ‚Üí New document
        if confidence >= 0.8 and short_code != 'UNKNOWN':
            is_new_document = True
            print(f"   üìÑ [{i}] New document detected: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Low confidence + continuation indicators ‚Üí Same document
        elif confidence < 0.5 and any(kw in reasoning for kw in ['section header', 'ii.', 'iii.', 'th·ª≠a ƒë·∫•t']):
            is_new_document = False
            print(f"   ‚û°Ô∏è [{i}] Continuation page: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Borderline case - use confidence
        else:
            is_new_document = (confidence >= 0.7)
            print(f"   ‚ùì [{i}] Borderline: {short_code} ({confidence:.0%}) ‚Üí {'New' if is_new_document else 'Continue'}", file=sys.stderr)
        
        if is_new_document:
            # Start new group
            groups.append(current_group)
            current_group = [i]
            last_type = short_code
        else:
            # Continue current group
            current_group.append(i)
    
    # Add last group
    if current_group:
        groups.append(current_group)
    
    print(f"\n‚úÖ Grouped into {len(groups)} documents:", file=sys.stderr)
    for g_idx, group in enumerate(groups):
        print(f"   Document {g_idx + 1}: {len(group)} pages {group}", file=sys.stderr)
    
    return groups


def batch_classify_smart(image_paths, api_key):
    """
    Ph∆∞∆°ng √°n 2: Smart Batching - TRUE AI-POWERED v·ªõi OVERLAP
    G·ª≠i nhi·ªÅu files (10-20) v·ªõi overlap ƒë·ªÉ AI c√≥ full context
    """
    print(f"\n{'='*80}", file=sys.stderr)
    print("üß† BATCH MODE 2: Smart Batching (AI Document Detection)", file=sys.stderr)
    print(f"{'='*80}", file=sys.stderr)
    
    total_files = len(image_paths)
    
    # Smart batch size strategy v·ªõi overlap
    if total_files <= 20:
        # Small batch: Send all at once, no overlap needed
        batch_size = total_files
        overlap = 0
        print(f"üìä Strategy: Send ALL {total_files} files in 1 batch", file=sys.stderr)
    elif total_files <= 60:
        # Medium batch: 20 files per batch, 5 files overlap
        batch_size = 20
        overlap = 5
        print(f"üìä Strategy: Send {batch_size} files per batch v·ªõi {overlap} files overlap", file=sys.stderr)
    else:
        # Large batch: 15 files per batch, 4 files overlap
        batch_size = 15
        overlap = 4
        print(f"üìä Strategy: Send {batch_size} files per batch v·ªõi {overlap} files overlap (large dataset)", file=sys.stderr)
    
    if overlap > 0:
        print(f"   ‚Ü©Ô∏è Overlap purpose: Batch sau th·∫•y {overlap} files cu·ªëi c·ªßa batch tr∆∞·ªõc", file=sys.stderr)
        print("   Why? File 16 kh√¥ng c√≥ title ‚Üí c·∫ßn th·∫•y file 14-15 ƒë·ªÉ bi·∫øt n√≥ thu·ªôc document n√†o", file=sys.stderr)
    
    print("   AI needs 10-20 files to detect document boundaries accurately", file=sys.stderr)
    
    # Use fixed batch with smart size + overlap
    return batch_classify_fixed(image_paths, api_key, batch_size=batch_size, overlap=overlap)


# CLI interface for testing
if __name__ == "__main__":
    if len(sys.argv) < 4:
        print("Usage: python batch_processor.py <mode> <api_key> <image1> <image2> ...", file=sys.stderr)
        print("Modes: fixed, smart", file=sys.stderr)
        print("Example: python batch_processor.py fixed AIza... img1.jpg img2.jpg img3.jpg", file=sys.stderr)
        sys.exit(1)
    
    mode = sys.argv[1]
    api_key = sys.argv[2]
    image_paths = sys.argv[3:]
    
    print(f"üîç Batch processing {len(image_paths)} images in '{mode}' mode", file=sys.stderr)
    
    if mode == 'fixed':
        results = batch_classify_fixed(image_paths, api_key, batch_size=5, overlap=2)
    elif mode == 'smart':
        results = batch_classify_smart(image_paths, api_key)
    else:
        print(f"‚ùå Unknown mode: {mode}", file=sys.stderr)
        sys.exit(1)
    
    # Output JSON to stdout for IPC
    print(json.dumps(results, ensure_ascii=False))
    
    print(f"\nüìä BATCH COMPLETE: {len(results)} files processed", file=sys.stderr)
