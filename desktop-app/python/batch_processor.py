#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Batch Document Processor - Multi-Image Analysis
Supports 2 modes:
- Mode 1: Fixed batch size (5 images per batch)
- Mode 2: Smart batching (group by document boundaries)
"""

import sys
import os
import json
import base64
import re
import requests
from PIL import Image
import io

# Import existing engines
from ocr_engine_gemini_flash import resize_image_smart, parse_gemini_response


def encode_image_base64(image_path, max_width=1500, max_height=2100):
    """Encode image to base64 with smart resize"""
    try:
        img = Image.open(image_path)
        
        # Convert to RGB if needed
        if img.mode in ('RGBA', 'LA', 'P'):
            img = img.convert('RGB')
        
        # Resize
        resized_img, resize_info = resize_image_smart(img, max_width, max_height)
        
        # Encode to base64
        buffer = io.BytesIO()
        resized_img.save(buffer, format='JPEG', quality=95)
        img_bytes = buffer.getvalue()
        encoded = base64.b64encode(img_bytes).decode('utf-8')
        
        return encoded, resize_info
    except Exception as e:
        print(f"Error encoding image {image_path}: {e}", file=sys.stderr)
        return None, None


def get_multi_image_prompt():
    """Get prompt for multi-image batch analysis with full 98 document types"""
    return """B·∫°n ƒëang ph√¢n t√≠ch nhi·ªÅu trang scan t√†i li·ªáu ƒë·∫•t ƒëai Vi·ªát Nam (c√≥ th·ªÉ thu·ªôc 1 ho·∫∑c nhi·ªÅu t√†i li·ªáu kh√°c nhau).

NHI·ªÜM V·ª§:
1. X√°c ƒë·ªãnh c√≥ BAO NHI√äU t√†i li·ªáu kh√°c nhau trong c√°c trang n√†y
2. Nh√≥m c√°c trang theo t√†i li·ªáu
3. Ph√¢n lo·∫°i lo·∫°i t√†i li·ªáu c·ªßa t·ª´ng nh√≥m theo DANH S√ÅCH 98 LO·∫†I b√™n d∆∞·ªõi
4. Tr√≠ch xu·∫•t metadata (ng√†y c·∫•p cho GCN, m√†u s·∫Øc, v.v.)

D·∫§U HI·ªÜU NH·∫¨N BI·∫æT TRANG M·ªöI vs TRANG TI·∫æP N·ªêI:

TRANG 1 C·ª¶A T√ÄI LI·ªÜU (New Document):
- C√≥ TI√äU ƒê·ªÄ CH√çNH ·ªü TOP 30% (ƒë·∫ßu trang)
- C·ª° ch·ªØ L·ªöN, IN HOA, cƒÉn gi·ªØa
- V√≠ d·ª•: "H·ª¢P ƒê·ªíNG CHUY·ªÇN NH∆Ø·ª¢NG", "ƒê∆†N ƒêƒÇNG K√ù BI·∫æN ƒê·ªòNG"
- C√≥ qu·ªëc huy (ƒë·ªëi v·ªõi GCN)
- Kh√°c bi·ªát r√µ v·ªÅ format/m√†u s·∫Øc so v·ªõi trang tr∆∞·ªõc

TRANG TI·∫æP N·ªêI (Continuation - Trang 2, 3, 4...):
- KH√îNG c√≥ ti√™u ƒë·ªÅ ch√≠nh ·ªü ƒë·∫ßu
- Ch·ªâ c√≥ section headers: "II.", "III.", "ƒêI·ªÄU 2", "PH·∫¶N II", "M·ª§C III"
- C√πng format/m√†u s·∫Øc v·ªõi trang tr∆∞·ªõc
- N·ªôi dung li√™n t·ª•c (ƒëi·ªÅu kho·∫£n, s∆° ƒë·ªì, b·∫£ng bi·ªÉu, ch·ªØ k√Ω)

RANH GI·ªöI GI·ªÆA C√ÅC T√ÄI LI·ªÜU:
- Thay ƒë·ªïi r√µ r·ªát: m√†u gi·∫•y (h·ªìng ‚Üí tr·∫Øng), format (c√≥ qu·ªëc huy ‚Üí kh√¥ng c√≥)
- Xu·∫•t hi·ªán ti√™u ƒë·ªÅ ch√≠nh m·ªõi ·ªü TOP
- Thay ƒë·ªïi ho√†n to√†n v·ªÅ layout

‚ö†Ô∏è DANH S√ÅCH 98 LO·∫†I T√ÄI LI·ªÜU (CH·ªà D√ôNG M√É TRONG DANH S√ÅCH N√ÄY):

üìã NH√ìM 1: B·∫¢N V·∫º / B·∫¢N ƒê·ªí (5 lo·∫°i)
BMT = B·∫£n m√¥ t·∫£ ranh gi·ªõi
HSKT = B·∫£n v·∫Ω (tr√≠ch l·ª•c, ƒëo t√°ch)
BVHC = B·∫£n v·∫Ω ho√†n c√¥ng
BVN = B·∫£n v·∫Ω nh√†
SDTT = S∆° ƒë·ªì d·ª± ki·∫øn t√°ch th·ª≠a

üìã NH√ìM 2: B·∫¢NG K√ä / DANH S√ÅCH (4 lo·∫°i)
BKKDT = B·∫£ng k√™ khai di·ªán t√≠ch
DSCG = B·∫£ng li·ªát k√™ danh s√°ch th·ª≠a ƒë·∫•t
DS15 = Danh s√°ch ch·ªß s·ª≠ d·ª•ng (M·∫´u 15)
DSCK = Danh s√°ch c√¥ng khai h·ªì s∆° c·∫•p gi·∫•y

üìã NH√ìM 3: BI√äN B·∫¢N (10 lo·∫°i)
BBBDG = Bi√™n b·∫£n b√°n ƒë·∫•u gi√°
BBGD = Bi√™n b·∫£n b√†n giao ƒë·∫•t
BBHDDK = Bi√™n b·∫£n h·ªôi ƒë·ªìng ƒëƒÉng k√Ω ƒë·∫•t ƒëai
BBNT = Bi√™n b·∫£n nghi·ªám thu c√¥ng tr√¨nh
BBKTSS = Bi√™n b·∫£n ki·ªÉm tra sai s√≥t
BBKTHT = Bi√™n b·∫£n ki·ªÉm tra hi·ªán tr·∫°ng
BBKTDC = Bi√™n b·∫£n k·∫øt th√∫c c√¥ng khai di ch√∫c
KTCKCG = Bi√™n b·∫£n k·∫øt th√∫c th√¥ng b√°o c·∫•p GCN
KTCKMG = Bi√™n b·∫£n k·∫øt th√∫c th√¥ng b√°o m·∫•t GCN
BLTT = Bi√™n lai thu thu·∫ø

üìã NH√ìM 4: GI·∫§Y T·ªú C√Å NH√ÇN (4 lo·∫°i)
CCCD = CƒÉn c∆∞·ªõc c√¥ng d√¢n
GKS = Gi·∫•y khai sinh
GKH = Gi·∫•y k·∫øt h√¥n
DICHUC = Di ch√∫c

üìã NH√ìM 5: GI·∫§Y CH·ª®NG NH·∫¨N (9 lo·∫°i)
üö® GCN = Gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t (‚ùå KH√îNG bao gi·ªù tr·∫£ v·ªÅ GCNM/GCNC)
  ‚ö†Ô∏è B·∫ÆT BU·ªòC t√¨m NG√ÄY C·∫§P (c√≥ th·ªÉ vi·∫øt tay, format: DD/MM/YYYY ho·∫∑c MM/YYYY ho·∫∑c YYYY)
  ‚ö†Ô∏è N·∫øu th·∫•y "Ng√†y XX th√°ng YY nƒÉm ZZZZ" ‚Üí chuy·ªÉn th√†nh "XX/YY/ZZZZ"
GXNNVTC = Gi·∫•y x√°c nh·∫≠n n·ªôp v√†o ng√¢n s√°ch
GNT = Gi·∫•y n·ªôp ti·ªÅn
GSND = Gi·∫•y sang nh∆∞·ª£ng ƒë·∫•t
GTLQ = Gi·∫•y t·ªù li√™n quan (gi·∫•y ti·∫øp nh·∫≠n, bi√™n nh·∫≠n h·ªì s∆°, phi·∫øu ki·ªÉm so√°t)
GUQ = Gi·∫•y ·ªßy quy·ªÅn
GXNDKLD = Gi·∫•y x√°c nh·∫≠n ƒëƒÉng k√Ω l·∫ßn ƒë·∫ßu
GPXD = Gi·∫•y ph√©p x√¢y d·ª±ng

üìã NH√ìM 6: H·ª¢P ƒê·ªíNG (7 lo·∫°i)
HDCQ = H·ª£p ƒë·ªìng chuy·ªÉn nh∆∞·ª£ng, t·∫∑ng cho
HDUQ = H·ª£p ƒë·ªìng ·ªßy quy·ªÅn
HDTHC = H·ª£p ƒë·ªìng th·∫ø ch·∫•p
HDTD = H·ª£p ƒë·ªìng thu√™ ƒë·∫•t
HDTCO = H·ª£p ƒë·ªìng thi c√¥ng
HDBDG = H·ª£p ƒë·ªìng mua b√°n ƒë·∫•u gi√°
hoadon = H√≥a ƒë∆°n gi√° tr·ªã gia tƒÉng

üìã NH√ìM 7: ƒê∆†N (15 lo·∫°i)
DDKBD = ƒê∆°n ƒëƒÉng k√Ω bi·∫øn ƒë·ªông (c√≥ "BI·∫æN ƒê·ªòNG")
DDK = ƒê∆°n ƒëƒÉng k√Ω (kh√¥ng c√≥ "BI·∫æN ƒê·ªòNG")
DCK = ƒê∆°n cam k·∫øt, gi·∫•y cam k·∫øt
CHTGD = ƒê∆°n chuy·ªÉn h√¨nh th·ª©c giao ƒë·∫•t
DCQDGD = ƒê∆°n ƒëi·ªÅu ch·ªânh quy·∫øt ƒë·ªãnh giao ƒë·∫•t
DMG = ƒê∆°n mi·ªÖn gi·∫£m l·ªá ph√≠
DMD = ƒê∆°n ƒëa m·ª•c ƒë√≠ch
DXN = ƒê∆°n x√°c nh·∫≠n
DXCMD = ƒê∆°n xin chuy·ªÉn m·ª•c ƒë√≠ch
DGH = ƒê∆°n xin gia h·∫°n
DXGD = ƒê∆°n xin giao ƒë·∫•t
DXTHT = ƒê∆°n xin t√°ch/h·ª£p th·ª≠a
DXCD = ƒê∆°n xin c·∫•p ƒë·ªïi GCN
DDCTH = ƒê∆°n ƒëi·ªÅu ch·ªânh th·ªùi h·∫°n d·ª± √°n
DXNTH = ƒê∆°n x√°c nh·∫≠n th·ªùi h·∫°n n√¥ng nghi·ªáp

üìã NH√ìM 8: QUY·∫æT ƒê·ªäNH (15 lo·∫°i)
QDGTD = Quy·∫øt ƒë·ªãnh giao ƒë·∫•t/cho thu√™
QDCMD = Quy·∫øt ƒë·ªãnh chuy·ªÉn m·ª•c ƒë√≠ch
QDTH = Quy·∫øt ƒë·ªãnh thu h·ªìi ƒë·∫•t
QDGH = Quy·∫øt ƒë·ªãnh gia h·∫°n
QDTT = Quy·∫øt ƒë·ªãnh t√°ch/h·ª£p th·ª≠a
QDCHTGD = Quy·∫øt ƒë·ªãnh chuy·ªÉn h√¨nh th·ª©c giao ƒë·∫•t
QDDCGD = Quy·∫øt ƒë·ªãnh ƒëi·ªÅu ch·ªânh Qƒê giao ƒë·∫•t
QDDCTH = Quy·∫øt ƒë·ªãnh ƒëi·ªÅu ch·ªânh th·ªùi h·∫°n d·ª± √°n
QDHG = Quy·∫øt ƒë·ªãnh h·ªßy GCN
QDPDBT = Quy·∫øt ƒë·ªãnh ph√™ duy·ªát b·ªìi th∆∞·ªùng
QDDCQH = Quy·∫øt ƒë·ªãnh ƒëi·ªÅu ch·ªânh quy ho·∫°ch
QDPDDG = Quy·∫øt ƒë·ªãnh ph√™ quy·∫øt ƒë∆°n gi√°
QDTHA = Quy·∫øt ƒë·ªãnh thi h√†nh √°n
QDHTSD = Quy·∫øt ƒë·ªãnh h√¨nh th·ª©c s·ª≠ d·ª•ng ƒë·∫•t
QDXP = Quy·∫øt ƒë·ªãnh x·ª≠ ph·∫°t

üìã NH√ìM 9: PHI·∫æU (8 lo·∫°i)
PCT = Phi·∫øu chuy·ªÉn th√¥ng tin nghƒ©a v·ª• t√†i ch√≠nh
PKTHS = Phi·∫øu ki·ªÉm tra h·ªì s∆° (KI·ªÇM TRA, kh√¥ng ph·∫£i KI·ªÇM SO√ÅT)
PLYKDC = Phi·∫øu l·∫•y √Ω ki·∫øn khu d√¢n c∆∞
PXNKQDD = Phi·∫øu x√°c nh·∫≠n k·∫øt qu·∫£ ƒëo ƒë·∫°c
DKTC = Phi·∫øu y√™u c·∫ßu ƒëƒÉng k√Ω bi·ªán ph√°p b·∫£o ƒë·∫£m
DKTD = Phi·∫øu y√™u c·∫ßu thay ƒë·ªïi bi·ªán ph√°p b·∫£o ƒë·∫£m
DKXTC = Phi·∫øu y√™u c·∫ßu x√≥a ƒëƒÉng k√Ω bi·ªán ph√°p b·∫£o ƒë·∫£m
QR = Qu√©t m√£ QR

üìã NH√ìM 10: TH√îNG B√ÅO (8 lo·∫°i)
TBT = Th√¥ng b√°o thu·∫ø
TBMG = Th√¥ng b√°o v·ªÅ vi·ªác m·∫•t GCN
TBCKCG = Th√¥ng b√°o c√¥ng khai k·∫øt qu·∫£ c·∫•p GCN
TBCKMG = Th√¥ng b√°o ni√™m y·∫øt m·∫•t GCN
HTNVTC = Th√¥ng b√°o ho√†n th√†nh nghƒ©a v·ª• t√†i ch√≠nh
TBCNBD = Th√¥ng b√°o c·∫≠p nh·∫≠t bi·∫øn ƒë·ªông
CKDC = Th√¥ng b√°o c√¥ng b·ªë di ch√∫c
HTBTH = Ho√†n th√†nh b·ªìi th∆∞·ªùng h·ªó tr·ª£

üìã NH√ìM 11: T·ªú KHAI / T·ªú TR√åNH (3 lo·∫°i)
TKT = T·ªù khai thu·∫ø
TTr = T·ªù tr√¨nh v·ªÅ giao ƒë·∫•t (ch·ªØ "r" th∆∞·ªùng)
TTCG = T·ªù tr√¨nh v·ªÅ ƒëƒÉng k√Ω ƒë·∫•t ƒëai (UBND x√£)

üìã NH√ìM 12: VƒÇN B·∫¢N (10 lo·∫°i)
CKTSR = VƒÉn b·∫£n cam k·∫øt t√†i s·∫£n ri√™ng
VBCTCMD = VƒÉn b·∫£n ch·∫•p thu·∫≠n chuy·ªÉn m·ª•c ƒë√≠ch
VBDNCT = VƒÉn b·∫£n ƒë·ªÅ ngh·ªã ch·∫•p thu·∫≠n nh·∫≠n chuy·ªÉn nh∆∞·ª£ng
PDPASDD = VƒÉn b·∫£n ƒë·ªÅ ngh·ªã th·∫©m ƒë·ªãnh ph∆∞∆°ng √°n
VBTK = VƒÉn b·∫£n th·ªèa thu·∫≠n ph√¢n chia DI S·∫¢N TH·ª™A K·∫æ
TTHGD = VƒÉn b·∫£n th·ªèa thu·∫≠n H·ªò GIA ƒê√åNH (kh√¥ng ph·∫£i v·ª£ ch·ªìng)
CDLK = VƒÉn b·∫£n ch·∫•m d·ª©t quy·ªÅn h·∫°n ch·∫ø li·ªÅn k·ªÅ
HCLK = VƒÉn b·∫£n x√°c l·∫≠p quy·ªÅn h·∫°n ch·∫ø li·ªÅn k·ªÅ
VBTC = VƒÉn b·∫£n t·ª´ ch·ªëi nh·∫≠n di s·∫£n
PCTSVC = VƒÉn b·∫£n ph√¢n chia t√†i s·∫£n V·ª¢ CH·ªíNG (kh√¥ng ph·∫£i h·ªô gia ƒë√¨nh)

‚ö†Ô∏è QUAN TR·ªåNG - QUY T·∫ÆC PH√ÇN LO·∫†I:
- CH·ªà ph√¢n lo·∫°i d·ª±a v√†o TI√äU ƒê·ªÄ CH√çNH ·ªü TOP 30%
- B·ªé QUA section headers (ƒêI·ªÄU 2, PH·∫¶N II, ...)
- B·ªé QUA mentions trong body text
- ‚ùå TUY·ªÜT ƒê·ªêI kh√¥ng tr·∫£ v·ªÅ GCNM ho·∫∑c GCNC, ch·ªâ tr·∫£ v·ªÅ GCN
- N·∫øu kh√¥ng kh·ªõp 98 lo·∫°i ‚Üí tr·∫£ v·ªÅ "UNKNOWN"

üö® ƒê·∫∂C BI·ªÜT V·ªöI GCN:
- Lu√¥n tr·∫£ v·ªÅ "GCN" (kh√¥ng ph·∫£i GCNM/GCNC)
- B·∫ÆT BU·ªòC t√¨m NG√ÄY C·∫§P (issue_date) ·ªü m·ªçi trang
- Format ng√†y: DD/MM/YYYY (ho·∫∑c MM/YYYY, YYYY n·∫øu m·ªù)
- V·ªã tr√≠: Th∆∞·ªùng ·ªü trang 2, bottom, g·∫ßn ch·ªØ k√Ω/con d·∫•u
- N·∫øu kh√¥ng t√¨m th·∫•y ‚Üí issue_date: null, issue_date_confidence: "not_found"

OUTPUT JSON:
{
  "documents": [
    {
      "type": "HDCQ",
      "pages": [0, 1, 2],
      "confidence": 0.95,
      "reasoning": "3 trang ƒë·∫ßu c√πng format, trang 0 c√≥ ti√™u ƒë·ªÅ 'H·ª¢P ƒê·ªíNG CHUY·ªÇN NH∆Ø·ª¢NG QUY·ªÄN S·ª¨ D·ª§NG ƒê·∫§T'",
      "metadata": {}
    },
    {
      "type": "GCN",
      "pages": [3, 4],
      "confidence": 0.98,
      "reasoning": "Trang 3-4 l√† GCN m√†u h·ªìng, c√≥ qu·ªëc huy, t√¨m th·∫•y ng√†y c·∫•p ·ªü trang 4",
      "metadata": {
        "color": "pink",
        "issue_date": "27/10/2021",
        "issue_date_confidence": "full"
      }
    }
  ]
}

L∆∞u √Ω:
- pages d√πng 0-indexed (trang ƒë·∫ßu ti√™n = 0)
- N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, ƒë√°nh d·∫•u confidence th·∫•p
- N·∫øu ch·ªâ c√≥ 1 t√†i li·ªáu, v·∫´n tr·∫£ v·ªÅ array v·ªõi 1 ph·∫ßn t·ª≠
"""


def batch_classify_fixed(image_paths, api_key, batch_size=5):
    """
    Ph∆∞∆°ng √°n 1: Fixed Batch Size
    Gom m·ªói 5 files v√† g·ª≠i c√πng l√∫c
    """
    print(f"\n{'='*80}", file=sys.stderr)
    print(f"üîÑ BATCH MODE 1: Fixed Batch Size ({batch_size} images per batch)", file=sys.stderr)
    print(f"{'='*80}", file=sys.stderr)
    
    all_results = []
    total_batches = (len(image_paths) + batch_size - 1) // batch_size
    
    for batch_idx in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[batch_idx:batch_idx + batch_size]
        batch_num = batch_idx // batch_size + 1
        
        print(f"\nüì¶ Batch {batch_num}/{total_batches}: Processing {len(batch_paths)} images", file=sys.stderr)
        for i, path in enumerate(batch_paths):
            print(f"   [{i}] {os.path.basename(path)}", file=sys.stderr)
        
        # Encode all images in batch
        print(f"üñºÔ∏è Encoding {len(batch_paths)} images...", file=sys.stderr)
        encoded_images = []
        for path in batch_paths:
            encoded, resize_info = encode_image_base64(path)
            if encoded:
                encoded_images.append(encoded)
                print(f"   ‚úÖ {os.path.basename(path)}: {resize_info.get('original_size', 'N/A')} ‚Üí {resize_info.get('new_size', 'N/A')}", file=sys.stderr)
            else:
                print(f"   ‚ùå Failed to encode {os.path.basename(path)}", file=sys.stderr)
        
        if not encoded_images:
            print(f"‚ùå No valid images in batch {batch_num}", file=sys.stderr)
            continue
        
        # Build multi-image payload
        parts = [{"text": get_multi_image_prompt()}]
        for img_data in encoded_images:
            parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": img_data
                }
            })
        
        payload = {
            "contents": [{"parts": parts}],
            "generationConfig": {
                "temperature": 0.1,
                "topP": 0.8,
                "topK": 10,
                "maxOutputTokens": 4000  # Larger for multi-document response
            },
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"}
            ]
        }
        
        # Call Gemini API
        print(f"üì° Sending batch request to Gemini Flash...", file=sys.stderr)
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={api_key}"
        
        try:
            response = requests.post(api_url, json=payload, timeout=120)
            response.raise_for_status()
            result_data = response.json()
            
            print(f"üìä Response status: {response.status_code}", file=sys.stderr)
            
            # Parse response
            if 'candidates' in result_data and len(result_data['candidates']) > 0:
                candidate = result_data['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    parts = candidate['content']['parts']
                    if len(parts) > 0 and 'text' in parts[0]:
                        response_text = parts[0]['text']
                        
                        print(f"üìÑ Raw response preview: {response_text[:200]}...", file=sys.stderr)
                        
                        # Extract JSON from response - try multiple patterns
                        json_match = re.search(r'\{[\s\S]*"documents"[\s\S]*\}', response_text)
                        if not json_match:
                            # Try finding JSON with triple backticks
                            json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', response_text)
                            if json_match:
                                response_text = json_match.group(1)
                            else:
                                # Try finding any JSON object
                                json_match = re.search(r'(\{[\s\S]*\})', response_text)
                                if json_match:
                                    response_text = json_match.group(1)
                        else:
                            response_text = json_match.group(0)
                        
                        if response_text:
                            try:
                                batch_result = json.loads(response_text)
                            
                                print(f"‚úÖ Batch {batch_num} complete:", file=sys.stderr)
                                for doc in batch_result.get('documents', []):
                                    doc_type = doc.get('type', 'UNKNOWN')
                                    pages = doc.get('pages', [])
                                    confidence = doc.get('confidence', 0)
                                    print(f"   üìÑ {doc_type}: {len(pages)} pages, confidence {confidence:.0%}", file=sys.stderr)
                                
                                # Map results back to original file paths
                                for doc in batch_result.get('documents', []):
                                    for page_idx in doc.get('pages', []):
                                        if page_idx < len(batch_paths):
                                            file_path = batch_paths[page_idx]
                                            all_results.append({
                                                'file_path': file_path,
                                                'file_name': os.path.basename(file_path),
                                                'short_code': doc.get('type', 'UNKNOWN'),
                                                'confidence': doc.get('confidence', 0.5),
                                                'reasoning': doc.get('reasoning', ''),
                                                'metadata': doc.get('metadata', {}),
                                                'method': 'batch_fixed',
                                                'batch_num': batch_num
                                            })
                            except json.JSONDecodeError as je:
                                print(f"‚ö†Ô∏è JSON decode error in batch {batch_num}: {je}", file=sys.stderr)
                                print(f"   Response text: {response_text[:500]}...", file=sys.stderr)
                        else:
                            print(f"‚ö†Ô∏è No valid JSON in response for batch {batch_num}", file=sys.stderr)
            
        except Exception as e:
            print(f"‚ùå Batch {batch_num} error: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
    
    print(f"\n{'='*80}", file=sys.stderr)
    print(f"‚úÖ BATCH MODE 1 COMPLETE: {len(all_results)} files processed", file=sys.stderr)
    print(f"{'='*80}", file=sys.stderr)
    
    return all_results


def quick_scan_tier1(image_path, api_key):
    """Quick scan v·ªõi Tier 1 ƒë·ªÉ detect document boundaries"""
    from ocr_engine_gemini_flash import classify_document_gemini_flash
    
    try:
        result = classify_document_gemini_flash(
            image_path=image_path,
            api_key=api_key,
            crop_top_percent=0.60,
            model_type='gemini-flash-lite',
            enable_resize=True,
            max_width=1500,
            max_height=2100
        )
        return result
    except Exception as e:
        print(f"Quick scan error for {image_path}: {e}", file=sys.stderr)
        return {'short_code': 'ERROR', 'confidence': 0}


def group_by_document(quick_results, file_paths):
    """
    Nh√≥m files th√†nh documents d·ª±a tr√™n quick scan results
    Returns: List of document groups [[0,1,2], [3,4], [5,6,7,8], ...]
    """
    print(f"\nüß† Analyzing document boundaries...", file=sys.stderr)
    
    groups = []
    current_group = [0]
    last_type = quick_results[0].get('short_code', 'UNKNOWN')
    
    for i in range(1, len(quick_results)):
        result = quick_results[i]
        short_code = result.get('short_code', 'UNKNOWN')
        confidence = result.get('confidence', 0)
        reasoning = result.get('reasoning', '').lower()
        
        # Check if this is a new document
        is_new_document = False
        
        # High confidence with clear title ‚Üí New document
        if confidence >= 0.8 and short_code != 'UNKNOWN':
            is_new_document = True
            print(f"   üìÑ [{i}] New document detected: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Low confidence + continuation indicators ‚Üí Same document
        elif confidence < 0.5 and any(kw in reasoning for kw in ['section header', 'ii.', 'iii.', 'th·ª≠a ƒë·∫•t']):
            is_new_document = False
            print(f"   ‚û°Ô∏è [{i}] Continuation page: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Borderline case - use confidence
        else:
            is_new_document = (confidence >= 0.7)
            print(f"   ‚ùì [{i}] Borderline: {short_code} ({confidence:.0%}) ‚Üí {'New' if is_new_document else 'Continue'}", file=sys.stderr)
        
        if is_new_document:
            # Start new group
            groups.append(current_group)
            current_group = [i]
            last_type = short_code
        else:
            # Continue current group
            current_group.append(i)
    
    # Add last group
    if current_group:
        groups.append(current_group)
    
    print(f"\n‚úÖ Grouped into {len(groups)} documents:", file=sys.stderr)
    for g_idx, group in enumerate(groups):
        print(f"   Document {g_idx + 1}: {len(group)} pages {group}", file=sys.stderr)
    
    return groups


def batch_classify_smart(image_paths, api_key):
    """
    Ph∆∞∆°ng √°n 2: Smart Batching - TRUE AI-POWERED
    G·ª≠i nhi·ªÅu files (10-20) ƒë·ªÉ AI t·ª± detect document boundaries
    """
    print(f"\n{'='*80}", file=sys.stderr)
    print(f"üß† BATCH MODE 2: Smart Batching (AI Document Detection)", file=sys.stderr)
    print(f"{'='*80}", file=sys.stderr)
    
    total_files = len(image_paths)
    
    # Smart batch size strategy
    if total_files <= 20:
        # Small batch: Send all at once
        batch_size = total_files
        print(f"üìä Strategy: Send ALL {total_files} files in 1 batch", file=sys.stderr)
    elif total_files <= 60:
        # Medium batch: 15-20 files per batch
        batch_size = 20
        print(f"üìä Strategy: Send {batch_size} files per batch", file=sys.stderr)
    else:
        # Large batch: 15 files per batch (more batches but still smart)
        batch_size = 15
        print(f"üìä Strategy: Send {batch_size} files per batch (large dataset)", file=sys.stderr)
    
    print(f"   Why? AI needs 10-20 files to detect document boundaries accurately", file=sys.stderr)
    print(f"   Fixed Batch (5 files) may cut documents in half ‚ùå", file=sys.stderr)
    print(f"   Smart Batch ({batch_size} files) sees full documents ‚úÖ", file=sys.stderr)
    
    # Use fixed batch with smart size
    return batch_classify_fixed(image_paths, api_key, batch_size=batch_size)


# CLI interface for testing
if __name__ == "__main__":
    if len(sys.argv) < 4:
        print("Usage: python batch_processor.py <mode> <api_key> <image1> <image2> ...", file=sys.stderr)
        print("Modes: fixed, smart", file=sys.stderr)
        print("Example: python batch_processor.py fixed AIza... img1.jpg img2.jpg img3.jpg", file=sys.stderr)
        sys.exit(1)
    
    mode = sys.argv[1]
    api_key = sys.argv[2]
    image_paths = sys.argv[3:]
    
    print(f"üîç Batch processing {len(image_paths)} images in '{mode}' mode", file=sys.stderr)
    
    if mode == 'fixed':
        results = batch_classify_fixed(image_paths, api_key, batch_size=5)
    elif mode == 'smart':
        results = batch_classify_smart(image_paths, api_key)
    else:
        print(f"‚ùå Unknown mode: {mode}", file=sys.stderr)
        sys.exit(1)
    
    # Output JSON to stdout for IPC
    print(json.dumps(results, ensure_ascii=False))
    
    print(f"\nüìä BATCH COMPLETE: {len(results)} files processed", file=sys.stderr)
