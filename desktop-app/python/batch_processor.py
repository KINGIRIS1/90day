#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Batch Document Processor - Multi-Image Analysis
Supports 2 modes:
- Mode 1: Fixed batch size (5 images per batch)
- Mode 2: Smart batching (group by document boundaries)
"""

import sys
import os
import json
import base64
import re
import requests
from PIL import Image
import io

# Import centralized error handler
try:
    from error_handler import (
        handle_error, 
        handle_success, 
        get_error_type_from_status,
        print_error_response
    )
    ERROR_HANDLER_AVAILABLE = True
except ImportError:
    ERROR_HANDLER_AVAILABLE = False
    print("[WARNING] error_handler not available - using legacy error handling", file=sys.stderr)

# Import existing engines and prompts
from ocr_engine_gemini_flash import (
    resize_image_smart, 
    parse_gemini_response,
    get_classification_prompt,
    get_classification_prompt_lite
)

# Import Tesseract+Text classifier
try:
    from tesseract_text_classifier import process_image as tesseract_text_process
    TESSERACT_TEXT_AVAILABLE = True
except ImportError:
    TESSERACT_TEXT_AVAILABLE = False
    print("[WARNING] tesseract_text_classifier not available", file=sys.stderr)


def adapt_prompt_for_multi_image(single_image_prompt, batch_size):
    """
    Adapt single-image prompt to multi-image batch context
    
    Changes:
    1. Add multi-image context introduction
    2. Add document grouping instructions
    3. Change output format from single result to documents array
    4. Add page indexing (0-indexed)
    5. Emphasize MUST return ALL pages
    """
    
    # Use % formatting to avoid f-string issues with JSON examples
    multi_image_intro = """ğŸ¯ BATCH ANALYSIS - %d TRANG SCAN

Báº¡n Ä‘ang phÃ¢n tÃ­ch %d trang scan tÃ i liá»‡u Ä‘áº¥t Ä‘ai Viá»‡t Nam.
CÃ¡c trang nÃ y cÃ³ thá»ƒ thuá»™c 1 hoáº·c nhiá»u tÃ i liá»‡u khÃ¡c nhau.

ğŸš¨ QUAN TRá»ŒNG NHáº¤T - BATCH MODE vs SINGLE-FILE MODE:
Trong BATCH MODE nÃ y, báº¡n KHÃ”NG pháº£i single-file classifier!
- âŒ Äá»ªNG tráº£ vá» "UNKNOWN" cho continuation pages
- âœ… Báº¡n PHáº¢I tá»± GOM continuation pages vÃ o document trÆ°á»›c
- âœ… Báº¡n cÃ³ context tá»« nhiá»u pages â†’ HÃ£y táº­n dá»¥ng!

VÃ Dá»¤:
Page 0: "THÃ”NG BÃO THUáº¾" â†’ Start TBT document
Page 1: "ÄIá»€U 1" â†’ TBT continuation â†’ ADD vÃ o pages cá»§a TBT
Page 2: "III. TÃNH THUáº¾" + báº£ng â†’ TBT continuation â†’ ADD vÃ o pages cá»§a TBT

Result: {{"type": "TBT", "pages": [0,1,2]}} âœ…

KHÃ”NG LÃ€M (single-file style):
Result: 
  {{"type": "TBT", "pages": [0]}},
  {{"type": "UNKNOWN", "pages": [1,2]}} âŒ SAI!

NHIá»†M Vá»¤:
1. XÃ¡c Ä‘á»‹nh cÃ³ BAO NHIÃŠU tÃ i liá»‡u khÃ¡c nhau trong %d trang nÃ y
2. NhÃ³m cÃ¡c trang theo tÃ i liá»‡u (pages array)
3. PhÃ¢n loáº¡i loáº¡i tÃ i liá»‡u cá»§a tá»«ng nhÃ³m
4. TrÃ­ch xuáº¥t metadata (ngÃ y cáº¥p cho GCN, mÃ u sáº¯c, v.v.)

Dáº¤U HIá»†U NHáº¬N BIáº¾T:

TRANG 1 Cá»¦A TÃ€I LIá»†U (New Document):
- CÃ³ TIÃŠU Äá»€ CHÃNH á»Ÿ TOP 30%% (Ä‘áº§u trang)
- Cá»¡ chá»¯ Lá»šN, IN HOA, cÄƒn giá»¯a
- CÃ³ quá»‘c huy (Ä‘á»‘i vá»›i GCN)
- KhÃ¡c biá»‡t rÃµ vá» format/mÃ u sáº¯c so vá»›i trang trÆ°á»›c

TRANG TIáº¾P Ná»I (Continuation - Trang 2, 3, 4...):
- KHÃ”NG cÃ³ tiÃªu Ä‘á» chÃ­nh á»Ÿ Ä‘áº§u
- Chá»‰ cÃ³ section headers: "II.", "III.", "ÄIá»€U 2", "PHáº¦N II"
- CÃ¹ng format/mÃ u sáº¯c vá»›i trang trÆ°á»›c
- Ná»™i dung liÃªn tá»¥c (Ä‘iá»u khoáº£n, chá»¯ kÃ½, báº£ng biá»ƒu)

ğŸš¨ QUAN TRá»ŒNG - NHáº¬N DIá»†N CONTINUATION PAGES:
CÃ¡c dáº¥u hiá»‡u sau = CONTINUATION (trang tiáº¿p theo, KHÃ”NG pháº£i document má»›i):
1. Section headers vá»›i sá»‘: "II.", "III.", "IV.", "V.", "ÄIá»€U 2", "ÄIá»€U 3", "PHáº¦N II", "Má»¤C III"
2. Báº£ng biá»ƒu vá»›i sá»‘ phÃ¢n cáº¥p: "4.1", "4.2", "4.2.1", "4.2.2", "(1.1)", "(2.1.3)"
3. Text body tiáº¿p ná»‘i: "...tiáº¿p theo...", "...nhÆ° sau:", danh sÃ¡ch bullet points
4. Chá»¯ kÃ½/con dáº¥u á»Ÿ cuá»‘i trang
5. KhÃ´ng cÃ³ header chÃ­nh thá»©c (quá»‘c huy, cÆ¡ quan ban hÃ nh)
6. **"Lá»œI CHá»¨NG Cá»¦A CÃ”NG CHá»¨NG VIÃŠN"** â†’ Trang chá»¯ kÃ½ cÃ´ng chá»©ng (cuá»‘i document)
7. **Danh sÃ¡ch ngÆ°á»i kÃ½, con dáº¥u cÃ´ng chá»©ng** â†’ Trang cuá»‘i document

ğŸš¨ Äáº¶C BIá»†T - TRANG CÃ”NG CHá»¨NG (KHÃ”NG pháº£i document má»›i):
Náº¿u trang cÃ³:
- "Lá»œI CHá»¨NG Cá»¦A CÃ”NG CHá»¨NG VIÃŠN"
- "CÃ”NG CHá»¨NG VIÃŠN"
- Con dáº¥u cÃ´ng chá»©ng (há»“ng/Ä‘á»)
- Danh sÃ¡ch chá»¯ kÃ½ cÃ¡c Ã”ng/BÃ 
- VÄƒn phÃ²ng cÃ´ng chá»©ng

â†’ ÄÃ¢y lÃ  TRANG CUá»I (signature page) cá»§a document
â†’ KHÃ”NG pháº£i document má»›i
â†’ GOM VÃ€O document trÆ°á»›c (TTHGD, PCTSVC, HDCQ, HDUQ, v.v.)

VÃ Dá»¤ ÄÃšNG:
Page 0: "THá»A THUáº¬N Há»˜ GIA ÄÃŒNH" â†’ TTHGD
Page 1-3: Ná»™i dung thá»a thuáº­n â†’ TTHGD continuation
Page 4: "Lá»œI CHá»¨NG Cá»¦A CÃ”NG CHá»¨NG VIÃŠN" + danh sÃ¡ch â†’ TTHGD continuation (signature page)
Page 5: Con dáº¥u, chá»¯ kÃ½ â†’ TTHGD continuation

Result: {{"type": "TTHGD", "pages": [0,1,2,3,4,5]}} âœ…

KHÃ”NG LÃ€M:
  {{"type": "TTHGD", "pages": [0,1,2,3]}},
  {{"type": "GTLQ", "pages": [4,5]}}  âŒ SAI!

VÃ Dá»¤ CONTINUATION - PHáº¢I GOM VÃ€O DOCUMENT TRÆ¯á»šC:
âœ… "III. TÃNH THUáº¾ Cá»¦A CÆ  QUAN THUáº¾" + báº£ng 4.1, 4.2
   â†’ Section header vá»›i sá»‘ La MÃ£
   â†’ Báº£ng biá»ƒu phÃ¢n cáº¥p
   â†’ ÄÃ¢y lÃ  continuation cá»§a document trÆ°á»›c (cÃ³ thá»ƒ lÃ  TBT, HDCQ, etc.)
   â†’ KHÃ”NG classify thÃ nh UNKNOWN
   â†’ GOM VÃ€O document cÃ³ trang trÆ°á»›c Ä‘Ã³
   
   VÃ Dá»¤ Cá»¤ THá»‚:
   Page 4 (index 4): "THÃ”NG BÃO THUáº¾" (title) â†’ TBT
   Page 5 (index 5): "ÄIá»€U 1: ..." â†’ TBT continuation
   Page 6 (index 6): "III. TÃNH THUáº¾" + báº£ng 4.1 â†’ TBT continuation âœ… (KHÃ”NG pháº£i UNKNOWN)
   
   Result: {{"type": "TBT", "pages": [4, 5, 6], ...}}

âœ… "ÄIá»€U 2: Ná»˜I DUNG THá»A THUáº¬N PHÃ‚N CHIA"
   â†’ Section header
   â†’ Continuation cá»§a TTHGD hoáº·c PCTSVC
   â†’ GOM VÃ€O document trÆ°á»›c

âœ… Trang chá»‰ cÃ³ báº£ng biá»ƒu (khÃ´ng cÃ³ title)
   â†’ Continuation
   â†’ GOM VÃ€O document trÆ°á»›c

âœ… Trang cÃ³ "1.1 TrÆ°á»ng há»£p...", "1.2 TrÆ°á»ng há»£p..." (numbered list)
   â†’ Continuation vá»›i structured content
   â†’ GOM VÃ€O document trÆ°á»›c

âŒ SAI - Classify continuation thÃ nh UNKNOWN:
Page 5: "THÃ”NG BÃO THUáº¾" (title)
Page 6: "III. TÃNH THUáº¾" + báº£ng
â†’ AI classify:
  {{"type": "TBT", "pages": [5]}},
  {{"type": "UNKNOWN", "pages": [6]}}  âŒ SAI!
â†’ ÄÃšNG:
  {{"type": "TBT", "pages": [5, 6]}}  âœ…

RANH GIá»šI GIá»®A CÃC TÃ€I LIá»†U:
- Thay Ä‘á»•i rÃµ rá»‡t: mÃ u giáº¥y (há»“ng â†’ tráº¯ng), format (cÃ³ quá»‘c huy â†’ khÃ´ng cÃ³)
- Xuáº¥t hiá»‡n tiÃªu Ä‘á» chÃ­nh má»›i á»Ÿ TOP
- Thay Ä‘á»•i hoÃ n toÃ n vá» layout

ğŸ¯ Dáº¤U HIá»†U VISUAL - CÃ™NG DOCUMENT (Cá»°C Ká»² QUAN TRá»ŒNG):

**1. Dáº¤U GIÃP LAI (Overlapping Stamp):**
Náº¿u tháº¥y CON Dáº¤U Äá»/Há»’NG bá»‹ Cáº®T NGANG qua nhiá»u pages:
- Page 1: CÃ³ PHáº¦N TRÃŠN cá»§a con dáº¥u (top half)
- Page 2: CÃ³ PHáº¦N GIá»®A cá»§a con dáº¥u (middle)
- Page 3: CÃ³ PHáº¦N DÆ¯á»šI cá»§a con dáº¥u (bottom half)

â†’ ÄÃ¢y lÃ  Dáº¤U GIÃP LAI!
â†’ 3-4 pages nÃ y Ä‘Æ°á»£c Ä‘Ã³ng dáº¥u CÃ™NG LÃšC (giáº¥y chá»“ng lÃªn nhau)
â†’ **Báº®T BUá»˜C CÃ™NG 1 DOCUMENT**
â†’ PHáº¢I GOM Táº¤T Cáº¢ pages cÃ³ partial stamp vÃ o 1 document

VÃ Dá»¤:
Page 0: "THá»A THUáº¬N Há»˜ GIA ÄÃŒNH" + pháº§n trÃªn con dáº¥u Ä‘á» (â¬†ï¸ top half)
Page 1: Text body + pháº§n giá»¯a con dáº¥u Ä‘á» (â¬Œ middle)
Page 2: Text body + pháº§n giá»¯a con dáº¥u Ä‘á» (â¬Œ middle)
Page 3: "Lá»œI CHá»¨NG..." + pháº§n dÆ°á»›i con dáº¥u Ä‘á» (â¬‡ï¸ bottom half)

â†’ **4 pages cÃ³ cÃ¹ng con dáº¥u bá»‹ cáº¯t** â†’ CÃ™NG 1 DOCUMENT!
â†’ Result: {{"type": "TTHGD", "pages": [0,1,2,3]}} âœ…

**2. Dáº¤U HOÃ€N CHá»ˆNH (Complete Stamp):**
Náº¿u page cÃ³ con dáº¥u HOÃ€N CHá»ˆNH (full circle, khÃ´ng bá»‹ cáº¯t):
â†’ ÄÃ¢y cÃ³ thá»ƒ lÃ  trang Äá»˜C Láº¬P (single document)
â†’ HOáº¶C trang cuá»‘i cá»§a document

**3. KHÃ”NG CÃ“ Dáº¤U:**
Náº¿u page khÃ´ng cÃ³ con dáº¥u:
â†’ CÃ³ thá»ƒ lÃ  trang giá»¯a document
â†’ Check title vÃ  continuation patterns

ğŸš¨ NGUYÃŠN Táº®C Dáº¤U GIÃP LAI:
- Partial stamp (bá»‹ cáº¯t) = **STRONG SIGNAL** cÃ¹ng document
- Æ¯u tiÃªn cao hÆ¡n cáº£ title/content analysis
- Náº¿u tháº¥y dáº¥u giÃ¡p lai â†’ GOM NGAY, khÃ´ng cáº§n nghi ngá»

---

""" % (batch_size, batch_size, batch_size)

    unknown_rules = """

âš ï¸ QUAN TRá»ŒNG - KHI NÃ€O TRáº¢ Vá»€ "UNKNOWN":
CHá»ˆ tráº£ vá» "UNKNOWN" khi:
1. Trang thá»±c sá»± khÃ´ng cÃ³ tiÃªu Ä‘á» VÃ€ khÃ´ng match continuation patterns
2. Title khÃ´ng thuá»™c 98 loáº¡i VÃ€ khÃ´ng pháº£i continuation
3. Trang hoÃ n toÃ n trá»‘ng hoáº·c khÃ´ng Ä‘á»c Ä‘Æ°á»£c

âŒ KHÃ”NG tráº£ vá» "UNKNOWN" cho:
- Trang cÃ³ section headers (III., ÄIá»€U 2) â†’ Continuation, gom vÃ o doc trÆ°á»›c
- Trang cÃ³ báº£ng biá»ƒu structured â†’ Continuation, gom vÃ o doc trÆ°á»›c
- Trang cÃ³ text liÃªn tá»¥c vá»›i trang trÆ°á»›c â†’ Continuation, gom vÃ o doc trÆ°á»›c

ğŸ¯ NGUYÃŠN Táº®C: Khi nghi ngá» â†’ Gom vÃ o document trÆ°á»›c (safer than creating new UNKNOWN doc)

âš ï¸ Äáº¶C BIá»†T - GOM CONTINUATION THAY VÃŒ TRáº¢ Vá»€ UNKNOWN:
Náº¾U trang khÃ´ng cÃ³ title NHÆ¯NG cÃ³ dáº¥u hiá»‡u continuation:
- Section headers: "II.", "III.", "ÄIá»€U X"
- Báº£ng biá»ƒu: tables vá»›i numbers
- Text body: tiáº¿p tá»¥c content

â†’ KHÃ”NG táº¡o document UNKNOWN riÃªng
â†’ GOM VÃ€O document trÆ°á»›c Ä‘Ã³
â†’ Extend "pages" array cá»§a document trÆ°á»›c

VÃ Dá»¤ ÄÃšNG:
Page 0: "THÃ”NG BÃO THUáº¾" (title) â†’ TBT
Page 1: "ÄIá»€U 1: ..." (section) â†’ TBT continuation
Page 2: "III. TÃNH THUáº¾" + báº£ng (section + table) â†’ TBT continuation

Result: {{"type": "TBT", "pages": [0, 1, 2], ...}} âœ…

KHÃ”NG LÃ€M:
  {{"type": "TBT", "pages": [0, 1]}},
  {{"type": "UNKNOWN", "pages": [2]}}  âŒ

CHá»ˆ TRáº¢ Vá»€ "UNKNOWN" KHI:
- Trang hoÃ n toÃ n láº¡ (khÃ´ng cÃ³ title, khÃ´ng cÃ³ continuation patterns)
- Title thá»±c sá»± khÃ´ng thuá»™c 98 loáº¡i (VD: "Báº¢N GIáº¢I TRÃŒNH", "VÄ‚N Báº¢N YÃŠU Cáº¦U")
- Trang trá»‘ng, scan lá»—i, khÃ´ng Ä‘á»c Ä‘Æ°á»£c

---

"""

    # GCN-specific metadata rules (CRITICAL!)
    gcn_metadata_rules = """

ğŸš¨ Cá»°C Ká»² QUAN TRá»ŒNG - GCN METADATA (Báº®T BUá»˜C):

Khi classify báº¥t ká»³ page nÃ o lÃ  "GCN", báº¡n PHáº¢I:

**1. TÃŒM MÃ€U Sáº®C (color):**
- Quan sÃ¡t mÃ u ná»n giáº¥y
- MÃ u Ä‘á»/cam (red/orange) â†’ "color": "red"
- MÃ u há»“ng (pink) â†’ "color": "pink"  
- MÃ u tráº¯ng hoáº·c khÃ´ng rÃµ â†’ "color": "unknown"

**2. TÃŒM NGÃ€Y Cáº¤P (issue_date) - Báº®T BUá»˜C:**
- âš ï¸ KHÃ”NG BAO GIá»œ bá» qua bÆ°á»›c nÃ y!
- TÃ¬m á»Ÿ trang 2 (náº¿u GCN A3) hoáº·c trang 1 bottom
- Text gáº§n: "NgÃ y cáº¥p", "Cáº¥p ngÃ y", "TM. UBND", chá»¯ kÃ½
- CÃ³ thá»ƒ viáº¿t TAY (handwritten)

**Formats phá»• biáº¿n:**
- "DD/MM/YYYY" â†’ Return: "27/10/2021"
- "NgÃ y 25 thÃ¡ng 8 nÄƒm 2010" â†’ Convert & return: "25/8/2010"
- "MM/YYYY" (náº¿u má») â†’ Return: "02/2012"
- "YYYY" (náº¿u ráº¥t má») â†’ Return: "2012"
- KhÃ´ng tÃ¬m tháº¥y â†’ Return: null

**Confidence levels:**
- "full": Äá»c Ä‘Æ°á»£c Ä‘áº§y Ä‘á»§ DD/MM/YYYY
- "partial": Chá»‰ MM/YYYY
- "year_only": Chá»‰ YYYY
- "not_found": KhÃ´ng tÃ¬m tháº¥y

**3. METADATA RESPONSE - Báº®T BUá»˜C:**

âœ… VÃ Dá»¤ ÄÃšNG (CÃ³ ngÃ y cáº¥p):
{{
  "type": "GCN",
  "pages": [5, 6],
  "confidence": 0.98,
  "reasoning": "GCN mÃ u há»“ng, quá»‘c huy, ngÃ y cáº¥p 27/10/2021",
  "metadata": {{
    "color": "pink",
    "issue_date": "27/10/2021",
    "issue_date_confidence": "full"
  }}
}}

âœ… VÃ Dá»¤ ÄÃšNG (KhÃ´ng tÃ¬m tháº¥y date):
{{
  "type": "GCN",
  "pages": [0],
  "confidence": 0.95,
  "reasoning": "GCN trang 1, mÃ u há»“ng, chÆ°a cÃ³ ngÃ y cáº¥p",
  "metadata": {{
    "color": "pink",
    "issue_date": null,
    "issue_date_confidence": "not_found"
  }}
}}

âŒ SAI - THIáº¾U METADATA:
{{
  "type": "GCN",
  "pages": [5, 6],
  "metadata": {{}}  // âŒ EMPTY! Must have color & issue_date!
}}

âŒ SAI - KHÃ”NG TÃŒM DATE:
{{
  "type": "GCN",
  "pages": [5, 6],
  "metadata": {{
    "color": "pink"
    // âŒ MISSING issue_date fields!
  }}
}}

âš ï¸ NHá»š: Má»i GCN document PHáº¢I cÃ³ metadata vá»›i:
- "color": "red" | "pink" | "unknown"
- "issue_date": "DD/MM/YYYY" | null
- "issue_date_confidence": "full" | "partial" | "year_only" | "not_found"

---

"""

    output_format = f"""

---

ğŸ¯ OUTPUT FORMAT - Báº®T BUá»˜C:

ğŸš¨ğŸš¨ğŸš¨ CRITICAL - MUST USE "documents" ARRAY ğŸš¨ğŸš¨ğŸš¨
Your response MUST be a JSON object with a "documents" key containing an array.
DO NOT return a single document object at the root level!

CORRECT FORMAT (REQUIRED):
{{
  "documents": [
    {{
      "type": "HDCQ",
      "pages": [0, 1, 2, 3, 4],
      "confidence": 0.95,
      "reasoning": "5 trang Ä‘áº§u cÃ¹ng format, trang 0 cÃ³ tiÃªu Ä‘á» 'Há»¢P Äá»’NG CHUYá»‚N NHÆ¯á»¢NG', trang 1-4 lÃ  continuation pages vá»›i ÄIá»€U 2, ÄIá»€U 3",
      "metadata": {{}}
    }},
    {{
      "type": "GCN",
      "pages": [5, 6],
      "confidence": 0.98,
      "reasoning": "Trang 5-6 lÃ  GCN mÃ u há»“ng, cÃ³ quá»‘c huy, tÃ¬m tháº¥y ngÃ y cáº¥p á»Ÿ trang 6",
      "metadata": {{
        "color": "pink",
        "issue_date": "27/10/2021",
        "issue_date_confidence": "full"
      }}
    }},
    {{
      "type": "UNKNOWN",
      "pages": [7, 8, 9],
      "confidence": 0.3,
      "reasoning": "3 trang cuá»‘i khÃ´ng rÃµ rÃ ng, khÃ´ng cÃ³ tiÃªu Ä‘á», khÃ´ng match 98 loáº¡i",
      "metadata": {{}}
    }}
  ]
}}

âŒ WRONG FORMAT (DO NOT USE):
{{
  "type": "HDCQ",
  "pages": [0, 1, 2],
  "confidence": 0.95,
  "reasoning": "...",
  "metadata": {{}}
}}
This is WRONG! You must wrap it in "documents" array!

ğŸš¨ Cá»°C Ká»² QUAN TRá»ŒNG - Báº®T BUá»˜C RETURN Táº¤T Cáº¢ {batch_size} PAGES:
- Báº¡n PHáº¢I assign Má»ŒI page (0 Ä‘áº¿n {batch_size-1}) vÃ o 1 document
- Náº¿u page khÃ´ng rÃµ â†’ assign vÃ o document type "UNKNOWN"
- KHÃ”NG BAO GIá»œ bá» qua page nÃ o
- Tá»•ng sá»‘ pages trong "pages" arrays = {batch_size}

VÃ Dá»¤ ÄÃšNG ({batch_size} pages):
- Document 1: pages [0,1,2,3,4] (5 pages)
- Document 2: pages [5,6,7,8] (4 pages)
- Document 3: pages [9,10,...,{batch_size-1}] ({batch_size-9} pages)
â†’ Total: {batch_size} pages âœ…

VÃ Dá»¤ SAI:
- Document 1: pages [0,1,2] (3 pages only)
- Document 2: pages [5,6] (2 pages, SKIP pages 3-4!)
â†’ Total: 5 pages âŒ (Missing pages 3,4,7,8,...,{batch_size-1})

INDEXING:
- pages dÃ¹ng 0-indexed (trang Ä‘áº§u tiÃªn = 0, trang cuá»‘i = {batch_size-1})
- Náº¿u chá»‰ cÃ³ 1 document â†’ váº«n tráº£ vá» array vá»›i 1 pháº§n tá»­
"""

    # Combine: intro + original rules + unknown rules + GCN metadata + output format
    full_multi_prompt = multi_image_intro + single_image_prompt + unknown_rules + gcn_metadata_rules + output_format
    
    return full_multi_prompt


def get_multi_image_prompt_full(batch_size):
    """Get FULL prompt (Flash Full rules) for multi-image batch"""
    single_prompt = get_classification_prompt()
    return adapt_prompt_for_multi_image(single_prompt, batch_size)


def get_multi_image_prompt_lite(batch_size):
    """Get LITE prompt (Flash Lite rules) for multi-image batch"""
    single_prompt = get_classification_prompt_lite()
    return adapt_prompt_for_multi_image(single_prompt, batch_size)


def encode_image_base64(image_path, max_width=1500, max_height=2100):
    """Encode image to base64 with smart resize"""
    try:
        img = Image.open(image_path)
        
        # Convert to RGB if needed
        if img.mode in ('RGBA', 'LA', 'P'):
            img = img.convert('RGB')
        
        # Resize
        resized_img, resize_info = resize_image_smart(img, max_width, max_height)
        
        # Encode to base64 with quality 85 (balance between size and OCR accuracy)
        buffer = io.BytesIO()
        resized_img.save(buffer, format='JPEG', quality=85, optimize=True)
        img_bytes = buffer.getvalue()
        encoded = base64.b64encode(img_bytes).decode('utf-8')
        
        return encoded, resize_info
    except Exception as e:
        print(f"Error encoding image {image_path}: {e}", file=sys.stderr)
        return None, None


def batch_classify_tesseract_text(image_paths, api_key, last_known_type=None):
    """
    NEW MODE: Tesseract OCR + Gemini Text Classification
    
    Approach:
    1. Extract text from each image using Tesseract (local, fast)
    2. Send only text to Gemini Text API for classification
    
    Benefits:
    - Faster: No need to upload large base64 images
    - Cheaper: Text API is 10-20x cheaper than Vision API
    - Less 503 errors: Smaller requests
    - Can handle larger batches: 20-30 files instead of 5
    
    Args:
        image_paths: List of image file paths
        api_key: Gemini API key
        last_known_type: Not used in this mode (each file processed independently)
    
    Returns:
        List of classification results
    """
    if not TESSERACT_TEXT_AVAILABLE:
        print("[ERROR] Tesseract+Text mode not available. Install pytesseract.", file=sys.stderr)
        return [{
            'type': 'UNKNOWN',
            'short_code': 'UNKNOWN',
            'confidence': 0.0,
            'error': 'Tesseract not available',
            'method': 'error'
        } for _ in image_paths]
    
    print(f"\n[Tesseract+Text Mode] Processing {len(image_paths)} images...", file=sys.stderr)
    
    results = []
    
    for i, img_path in enumerate(image_paths):
        try:
            print(f"[{i+1}/{len(image_paths)}] Processing: {os.path.basename(img_path)}", file=sys.stderr)
            
            # Process with Tesseract + Gemini Text
            result = tesseract_text_process(img_path, api_key)
            
            # Add pages info (each image is a separate "document" in this mode)
            result['pages'] = [i]
            
            results.append(result)
            
            print(f"  âœ… Result: {result['short_code']} (confidence: {result['confidence']*100:.1f}%)", file=sys.stderr)
            
        except Exception as e:
            print(f"  âŒ Error: {e}", file=sys.stderr)
            results.append({
                'type': 'UNKNOWN',
                'short_code': 'UNKNOWN',
                'confidence': 0.0,
                'pages': [i],
                'error': str(e),
                'method': 'tesseract_text'
            })
    
    print(f"\n[Tesseract+Text Mode] Completed {len(results)}/{len(image_paths)} classifications", file=sys.stderr)
    
    return results




def batch_classify_fixed(image_paths, api_key, engine_type='gemini-flash', batch_size=8, last_known_type=None):
    """
    PhÆ°Æ¡ng Ã¡n 1: Fixed Batch Size vá»›i SEQUENTIAL METADATA
    
    Args:
        image_paths: List of file paths
        api_key: Google API key
        engine_type: 'gemini-flash', 'gemini-flash-lite', or 'gemini-flash-hybrid'
        batch_size: Files per batch
        last_known_type: Metadata tá»« file cuá»‘i batch trÆ°á»›c {short_code, confidence, has_title}
    
    Strategy:
        - Batch 1: Process files 0-4, return lastKnown tá»« file 4
        - Batch 2: Process files 5-9 WITH lastKnown tá»« file 4
          * File 5 cÃ³ title â†’ Bá» qua lastKnown, dÃ¹ng title má»›i
          * File 5 khÃ´ng cÃ³ title â†’ Ãp dá»¥ng sequential tá»« lastKnown
        - No overlap needed â†’ 0% overhead!
    """
    
    # Determine model and prompt based on engine type
    if engine_type == 'gemini-flash-lite':
        model_name = 'gemini-2.5-flash-lite'
        prompt_getter = get_multi_image_prompt_lite
        print(f"\n{'='*80}", file=sys.stderr)
        print(f"âš¡ BATCH MODE: Fixed ({batch_size} files, NO overlap) + Flash LITE", file=sys.stderr)
        print(f"   Model: {model_name}", file=sys.stderr)
        print("   Prompt: Lite (simplified, 60% crop rules)", file=sys.stderr)
        print("   Metadata: Sequential naming from previous batch", file=sys.stderr)
        print(f"{'='*80}", file=sys.stderr)
    elif engine_type == 'gemini-flash-hybrid':
        model_name = 'gemini-2.5-flash-lite'  # Start with Lite for hybrid
        prompt_getter = get_multi_image_prompt_lite
        print(f"\n{'='*80}", file=sys.stderr)
        print(f"ğŸ”„ BATCH MODE: Fixed ({batch_size} files, NO overlap) + HYBRID", file=sys.stderr)
        print("   Strategy: Two-tier (Lite â†’ Full if low confidence)", file=sys.stderr)
        print(f"   Model (Tier 1): {model_name}", file=sys.stderr)
        print("   Metadata: Sequential naming from previous batch", file=sys.stderr)
        print(f"{'='*80}", file=sys.stderr)
    else:  # gemini-flash (default)
        model_name = 'gemini-2.5-flash'
        prompt_getter = get_multi_image_prompt_full
        print(f"\n{'='*80}", file=sys.stderr)
        print(f"ğŸ¤– BATCH MODE: Fixed ({batch_size} files, NO overlap) + Flash FULL", file=sys.stderr)
        print(f"   Model: {model_name}", file=sys.stderr)
        print("   Prompt: Full (complete 98-rule classification)", file=sys.stderr)
        print("   Metadata: Sequential naming from previous batch", file=sys.stderr)
        print(f"{'='*80}", file=sys.stderr)
    
    if last_known_type:
        print(f"\nğŸ“Œ Received lastKnown from previous batch:", file=sys.stderr)
        print(f"   Type: {last_known_type.get('short_code')}", file=sys.stderr)
        print(f"   Confidence: {last_known_type.get('confidence', 0):.0%}", file=sys.stderr)
        print(f"   Has title: {last_known_type.get('has_title', False)}", file=sys.stderr)
    
    all_results = []
    processed_files = set()
    batch_num = 0
    current_idx = 0
    current_last_known = last_known_type  # Start with provided lastKnown
    
    while current_idx < len(image_paths):
        batch_num += 1
        
        # Calculate batch range WITHOUT overlap
        batch_start = current_idx
        batch_end = min(len(image_paths), current_idx + batch_size)
        batch_paths = image_paths[batch_start:batch_end]
        
        print(f"\nğŸ“¦ Batch {batch_num}: Files {batch_start}-{batch_end-1} ({len(batch_paths)} images)", file=sys.stderr)
        
        for i, path in enumerate(batch_paths):
            print(f"   [{i}] {os.path.basename(path)}", file=sys.stderr)
        
        # Encode all images in batch
        print(f"ğŸ–¼ï¸ Encoding {len(batch_paths)} images...", file=sys.stderr)
        encoded_images = []
        for path in batch_paths:
            encoded, resize_info = encode_image_base64(path)
            if encoded:
                encoded_images.append(encoded)
                print(f"   âœ… {os.path.basename(path)}: {resize_info.get('original_size', 'N/A')} â†’ {resize_info.get('new_size', 'N/A')}", file=sys.stderr)
            else:
                print(f"   âŒ Failed to encode {os.path.basename(path)}", file=sys.stderr)
        
        if not encoded_images:
            print(f"âŒ No valid images in batch {batch_num}", file=sys.stderr)
            continue
        
        # Build multi-image payload
        parts = [{"text": prompt_getter(len(batch_paths))}]
        for img_data in encoded_images:
            parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": img_data
                }
            })
        
        payload = {
            "contents": [{"parts": parts}],
            "generationConfig": {
                "temperature": 0.1,
                "topP": 0.8,
                "topK": 10,
                "maxOutputTokens": 8000,  # Large enough for 20 documents Ã— 400 tokens each
                "responseMimeType": "application/json"  # Force JSON output
            },
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"}
            ]
        }
        
        # Call Gemini API with retry logic
        print(f"ğŸ“¡ Sending batch request to {model_name}...", file=sys.stderr)
        print(f"   Batch size: {len(batch_paths)} files", file=sys.stderr)
        # Calculate approximate request size
        import json
        payload_size_mb = len(json.dumps(payload)) / (1024 * 1024)
        print(f"   Request size: ~{payload_size_mb:.2f} MB", file=sys.stderr)
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}"
        
        max_retries = 3
        retry_delay = 10  # seconds
        
        for attempt in range(max_retries):
            try:
                response = requests.post(api_url, json=payload, timeout=120)
                response.raise_for_status()
                result_data = response.json()
                
                # Reset all error counters on success
                if ERROR_HANDLER_AVAILABLE:
                    handle_success()
                
                break  # Success, exit retry loop
                
            except requests.exceptions.HTTPError as e:
                status_code = e.response.status_code
                error_type = get_error_type_from_status(status_code) if ERROR_HANDLER_AVAILABLE else str(status_code)
                
                if ERROR_HANDLER_AVAILABLE:
                    # Use centralized error handler
                    context = {
                        "batch_num": batch_num,
                        "batch_size": batch_size,
                        "attempt": attempt + 1,
                        "max_retries": max_retries
                    }
                    error_info = handle_error(error_type, e, context)
                    
                    # Check if should stop
                    if error_info["should_stop"]:
                        print(f"âŒ Stopping due to critical error: {status_code}", file=sys.stderr)
                        if error_info["error_response"]:
                            print_error_response(error_info["error_response"])
                        sys.exit(1)
                    
                    # Check if should retry
                    if error_info["should_retry"] and attempt < max_retries - 1:
                        wait_time = error_info["wait_time"]
                        print(f"   Retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                        if batch_size > 5 and status_code in [500, 503]:
                            print(f"   ğŸ’¡ Tip: Try reducing Smart batch size to 5-8 in Settings", file=sys.stderr)
                        import time
                        time.sleep(wait_time)
                        continue
                    else:
                        # Max retries reached or should not retry
                        if error_info["is_critical"] and error_info["error_response"]:
                            print_error_response(error_info["error_response"])
                            sys.exit(1)
                        raise
                else:
                    # Legacy error handling (fallback)
                    if status_code in [500, 503]:
                        if attempt < max_retries - 1:
                            wait_time = retry_delay * (2 ** attempt)
                            print(f"âš ï¸ {status_code} Server Error, retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                            import time
                            time.sleep(wait_time)
                            continue
                        else:
                            raise
                    elif status_code == 429:
                        if attempt < max_retries - 1:
                            wait_time = 60 * (2 ** attempt)
                            print(f"âš ï¸ 429 Rate Limit, retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                            import time
                            time.sleep(wait_time)
                            continue
                        else:
                            raise
                    else:
                        print(f"âŒ HTTP {status_code} Error: {e}", file=sys.stderr)
                        raise
            except requests.exceptions.Timeout as e:
                # Timeout error
                if ERROR_HANDLER_AVAILABLE:
                    context = {"batch_num": batch_num, "batch_size": batch_size, "attempt": attempt + 1}
                    error_info = handle_error("timeout", e, context)
                    
                    if error_info["should_retry"] and attempt < max_retries - 1:
                        wait_time = error_info["wait_time"]
                        print(f"   Retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                        import time
                        time.sleep(wait_time)
                        continue
                    else:
                        if error_info["error_response"]:
                            print_error_response(error_info["error_response"])
                        raise
                else:
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)
                        print(f"âš ï¸ Timeout error, retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                        import time
                        time.sleep(wait_time)
                        continue
                    else:
                        raise
            except requests.exceptions.RequestException as e:
                # Network errors
                if ERROR_HANDLER_AVAILABLE:
                    context = {"batch_num": batch_num, "batch_size": batch_size, "attempt": attempt + 1}
                    error_info = handle_error("network", e, context)
                    
                    if error_info["should_retry"] and attempt < max_retries - 1:
                        wait_time = error_info["wait_time"]
                        print(f"   Retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                        import time
                        time.sleep(wait_time)
                        continue
                    else:
                        if error_info["error_response"]:
                            print_error_response(error_info["error_response"])
                        raise
                else:
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)
                        print(f"âš ï¸ Network error, retry {attempt + 1}/{max_retries} in {wait_time}s...", file=sys.stderr)
                        import time
                        time.sleep(wait_time)
                        continue
                    else:
                        raise
        
        # Add delay between batches to avoid rate limiting
        if batch_num < ((len(image_paths) + batch_size - 1) // batch_size):
            import time
            inter_batch_delay = 5  # 5 seconds between batches
            print(f"â¸ï¸ Waiting {inter_batch_delay}s before next batch...", file=sys.stderr)
            time.sleep(inter_batch_delay)
        
        try:
            
            print(f"ğŸ“Š Response status: {response.status_code}", file=sys.stderr)
            
            # Debug: Check finish reason and safety
            if 'candidates' in result_data and len(result_data['candidates']) > 0:
                candidate = result_data['candidates'][0]
                finish_reason = candidate.get('finishReason', 'UNKNOWN')
                print(f"ğŸ” Finish reason: {finish_reason}", file=sys.stderr)
                
                if finish_reason == 'MAX_TOKENS':
                    print("âš ï¸ WARNING: Response truncated due to MAX_TOKENS!", file=sys.stderr)
                    print("   Some pages may be missing from response", file=sys.stderr)
            
            # Parse response
            if 'candidates' in result_data and len(result_data['candidates']) > 0:
                candidate = result_data['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    parts = candidate['content']['parts']
                    if len(parts) > 0 and 'text' in parts[0]:
                        response_text = parts[0]['text']
                        
                        print(f"ğŸ“„ Raw response preview: {response_text[:200]}...", file=sys.stderr)
                        
                        # DEBUG: Log full response to understand parsing issues
                        print(f"\nğŸ” DEBUG - Full response length: {len(response_text)} chars", file=sys.stderr)
                        if len(response_text) < 500:
                            print(f"ğŸ“„ Full response (short):", file=sys.stderr)
                            print(response_text, file=sys.stderr)
                        
                        # Check for common issues
                        has_documents = '"documents"' in response_text
                        has_json_markers = '```json' in response_text
                        starts_with_brace = response_text.strip().startswith('{')
                        
                        print(f"   Has 'documents' key: {has_documents}", file=sys.stderr)
                        print(f"   Has ```json markers: {has_json_markers}", file=sys.stderr)
                        print(f"   Starts with brace: {starts_with_brace}", file=sys.stderr)
                        
                        # Extract JSON from response - try multiple patterns
                        json_match = re.search(r'\{[\s\S]*"documents"[\s\S]*\}', response_text)
                        if not json_match:
                            # Try finding JSON with triple backticks
                            json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', response_text)
                            if json_match:
                                response_text = json_match.group(1)
                            else:
                                # Try finding any JSON object
                                json_match = re.search(r'(\{[\s\S]*\})', response_text)
                                if json_match:
                                    response_text = json_match.group(1)
                        else:
                            response_text = json_match.group(0)
                        
                        if response_text:
                            try:
                                batch_result = json.loads(response_text)
                                
                                # Check if documents key exists
                                if 'documents' not in batch_result or not isinstance(batch_result.get('documents'), list):
                                    print(f"âš ï¸ Batch {batch_num} WARNING: No 'documents' array in response!", file=sys.stderr)
                                    print(f"   Response keys: {list(batch_result.keys())}", file=sys.stderr)
                                    
                                    # FALLBACK: Check if response is a single document (LLM returned wrong format)
                                    if 'type' in batch_result and 'pages' in batch_result:
                                        print(f"   ğŸ”„ Auto-fixing: LLM returned single document format instead of array", file=sys.stderr)
                                        print(f"   Wrapping into documents array...", file=sys.stderr)
                                        batch_result = {
                                            'documents': [batch_result]
                                        }
                                    else:
                                        print(f"   âŒ Cannot auto-fix: Response format unrecognized", file=sys.stderr)
                                        print(f"   This batch will fallback to individual processing", file=sys.stderr)
                                        raise ValueError("Missing 'documents' array in response")
                            
                                print(f"âœ… Batch {batch_num} complete:", file=sys.stderr)
                                
                                # Validate: Check if all pages are covered
                                total_pages_in_batch = len(batch_paths)
                                pages_returned = set()
                                
                                for doc in batch_result.get('documents', []):
                                    doc_type = doc.get('type', 'UNKNOWN')
                                    pages = doc.get('pages', [])
                                    confidence = doc.get('confidence', 0)
                                    print(f"   ğŸ“„ {doc_type}: {len(pages)} pages, confidence {confidence:.0%}", file=sys.stderr)
                                    
                                    # Collect all page indices
                                    for p in pages:
                                        pages_returned.add(p)
                                
                                # Check for missing pages
                                expected_pages = set(range(total_pages_in_batch))
                                missing_pages = expected_pages - pages_returned
                                
                                if missing_pages:
                                    print(f"   âš ï¸ WARNING: AI didn't return {len(missing_pages)} pages: {sorted(missing_pages)}", file=sys.stderr)
                                    print("      These files will be processed by fallback", file=sys.stderr)
                                else:
                                    print(f"   âœ… All {total_pages_in_batch} pages accounted for", file=sys.stderr)
                                
                                # Map results back to original file paths WITH sequential naming
                                batch_results_with_sequential = []
                                
                                for doc in batch_result.get('documents', []):
                                    doc_type = doc.get('type', 'UNKNOWN')
                                    doc_confidence = doc.get('confidence', 0.5)
                                    doc_reasoning = doc.get('reasoning', '')
                                    doc_metadata = doc.get('metadata', {})
                                    
                                    # DEBUG: Log metadata for GCN
                                    if doc_type == 'GCN':
                                        print(f"\nğŸ” DEBUG - GCN Metadata:", file=sys.stderr)
                                        print(f"   Type: {doc_type}", file=sys.stderr)
                                        print(f"   Metadata: {doc_metadata}", file=sys.stderr)
                                        print(f"   Has color: {'color' in doc_metadata}", file=sys.stderr)
                                        print(f"   Has issue_date: {'issue_date' in doc_metadata}", file=sys.stderr)
                                        if doc_metadata:
                                            print(f"   color value: {doc_metadata.get('color', 'MISSING')}", file=sys.stderr)
                                            print(f"   issue_date value: {doc_metadata.get('issue_date', 'MISSING')}", file=sys.stderr)
                                        print(f"\n", file=sys.stderr)
                                    
                                    for page_idx in doc.get('pages', []):
                                        if page_idx < len(batch_paths):
                                            file_path = batch_paths[page_idx]
                                            file_name = os.path.basename(file_path)
                                            
                                            # Determine if this file has title (high confidence, not UNKNOWN)
                                            has_title = (doc_confidence >= 0.8 and doc_type != 'UNKNOWN')
                                            
                                            # Apply sequential naming logic
                                            final_type = doc_type
                                            final_confidence = doc_confidence
                                            applied_sequential = False
                                            
                                            # If file is UNKNOWN or low confidence AND we have lastKnown
                                            if (doc_type == 'UNKNOWN' or doc_confidence < 0.5) and current_last_known:
                                                final_type = current_last_known['short_code']
                                                final_confidence = current_last_known['confidence']
                                                applied_sequential = True
                                                print(f"   ğŸ”„ Sequential: {file_name} ({doc_type} {doc_confidence:.0%}) â†’ {final_type}", file=sys.stderr)
                                            
                                            # Update lastKnown if this file has good classification
                                            if doc_type != 'UNKNOWN' and doc_confidence >= 0.7 and has_title:
                                                current_last_known = {
                                                    'short_code': doc_type,
                                                    'confidence': doc_confidence,
                                                    'has_title': True
                                                }
                                                print(f"   ğŸ“Œ Updated lastKnown: {doc_type} ({doc_confidence:.0%})", file=sys.stderr)
                                            
                                            processed_files.add(file_path)
                                            batch_results_with_sequential.append({
                                                'file_path': file_path,
                                                'file_name': file_name,
                                                'short_code': final_type,
                                                'confidence': final_confidence,
                                                'reasoning': doc_reasoning,
                                                'metadata': doc_metadata,
                                                'method': 'batch_fixed',
                                                'batch_num': batch_num,
                                                'applied_sequential': applied_sequential,
                                                'original_classification': doc_type if applied_sequential else None
                                            })
                                
                                all_results.extend(batch_results_with_sequential)
                            except json.JSONDecodeError as je:
                                print(f"âš ï¸ JSON decode error in batch {batch_num}: {je}", file=sys.stderr)
                                print(f"   Response text: {response_text[:500]}...", file=sys.stderr)
                        else:
                            print(f"âš ï¸ No valid JSON in response for batch {batch_num}", file=sys.stderr)
            
        except Exception as e:
            print(f"âŒ Batch {batch_num} error: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
        
        # Move to next batch (increment by batch_size, not batch_end)
        current_idx += batch_size
    
    print(f"\n{'='*80}", file=sys.stderr)
    print(f"âœ… BATCH MODE 1 COMPLETE: {len(all_results)} files processed", file=sys.stderr)
    
    # Detect missing files
    all_input_files = set(image_paths)
    missing_files = all_input_files - processed_files
    
    if missing_files:
        print(f"âš ï¸ WARNING: {len(missing_files)} files were NOT processed by AI:", file=sys.stderr)
        for missing_file in sorted(missing_files):
            print(f"   âŒ {os.path.basename(missing_file)}", file=sys.stderr)
        print("   Possible causes: AI didn't return page indices, JSON parsing error", file=sys.stderr)
        print(f"\nğŸ”„ FALLBACK: Processing {len(missing_files)} missing files individually...", file=sys.stderr)
        
        # Fallback: Process missing files with single-file tier1 scan
        for missing_file in sorted(missing_files):
            try:
                print(f"   ğŸ”„ Processing {os.path.basename(missing_file)}...", file=sys.stderr)
                result = quick_scan_tier1(missing_file, api_key)
                all_results.append({
                    'file_path': missing_file,
                    'file_name': os.path.basename(missing_file),
                    'short_code': result.get('short_code', 'UNKNOWN'),
                    'confidence': result.get('confidence', 0.5),
                    'reasoning': result.get('reasoning', 'Fallback single-file scan'),
                    'metadata': result.get('metadata', {}),
                    'method': 'batch_fallback',
                    'batch_num': 'fallback'
                })
                print(f"      âœ… {result.get('short_code', 'UNKNOWN')} ({result.get('confidence', 0):.0%})", file=sys.stderr)
            except Exception as e:
                print(f"      âŒ Error: {e}", file=sys.stderr)
                # Add as UNKNOWN if fallback also fails
                all_results.append({
                    'file_path': missing_file,
                    'file_name': os.path.basename(missing_file),
                    'short_code': 'UNKNOWN',
                    'confidence': 0.0,
                    'reasoning': f'Fallback failed: {str(e)}',
                    'metadata': {},
                    'method': 'batch_fallback_failed',
                    'batch_num': 'fallback'
                })
        
        print(f"âœ… Fallback complete: {len(all_results)} total results (original + fallback)", file=sys.stderr)
    else:
        print(f"âœ… All {len(all_input_files)} input files were successfully processed", file=sys.stderr)
    
    print(f"{'='*80}", file=sys.stderr)
    
    # Return results AND lastKnown for next batch
    return {
        'results': all_results,
        'last_known_type': current_last_known
    }


def quick_scan_tier1(image_path, api_key):
    """Quick scan vá»›i Tier 1 Ä‘á»ƒ detect document boundaries"""
    from ocr_engine_gemini_flash import classify_document_gemini_flash
    
    try:
        result = classify_document_gemini_flash(
            image_path=image_path,
            api_key=api_key,
            crop_top_percent=0.60,
            model_type='gemini-flash-lite',
            enable_resize=True,
            max_width=1500,
            max_height=2100
        )
        
        # âœ… POST-PROCESSING FIXES (same as process_document.py)
        short_code = result.get('short_code', 'UNKNOWN')
        reasoning_text = result.get('reasoning', '').upper()
        
        # Fix #1: PKTHS â†’ GTLQ (when "KIá»‚M SOÃT" detected)
        if short_code == "PKTHS" and ("KIá»‚M SOÃT" in reasoning_text or "KIEM SOAT" in reasoning_text):
            print(f"ğŸ”§ Post-processing fix: PKTHS â†’ GTLQ (detected 'KIá»‚M SOÃT')", file=sys.stderr)
            result['short_code'] = "GTLQ"
            result['reasoning'] = f"[AUTO-FIXED: PKTHSâ†’GTLQ] {result.get('reasoning', '')}"
        
        # Fix #2: UNKNOWN â†’ GTLQ (when GTLQ keywords detected)
        elif short_code == "UNKNOWN":
            gtlq_keywords = ["PHIáº¾U XIN Lá»–I", "Háº¸N Láº I NGÃ€Y TRáº¢ Káº¾T QUáº¢", "PHIáº¾U KIá»‚M SOÃT", 
                            "Bá»˜ PHáº¬N TIáº¾P NHáº¬N", "GIáº¤Y TIáº¾P NHáº¬N Há»’ SÆ "]
            for keyword in gtlq_keywords:
                if keyword in reasoning_text:
                    print(f"ğŸ”§ Post-processing fix: UNKNOWN â†’ GTLQ (detected '{keyword}')", file=sys.stderr)
                    result['short_code'] = "GTLQ"
                    result['confidence'] = 0.95
                    result['reasoning'] = f"[AUTO-FIXED: UNKNOWNâ†’GTLQ] {result.get('reasoning', '')}"
                    break
        
        # Fix #3: BMT â†’ HSKT (when "TRÃCH Lá»¤C" or "Báº¢N Äá»’" detected)
        elif short_code == "BMT":
            hskt_keywords = ["TRÃCH Lá»¤C", "Báº¢N Äá»’ Äá»ŠA CHÃNH", "ÄO TÃCH", "CHá»ˆNH LÃ"]
            for keyword in hskt_keywords:
                if keyword in reasoning_text:
                    print(f"ğŸ”§ Post-processing fix: BMT â†’ HSKT (detected '{keyword}')", file=sys.stderr)
                    result['short_code'] = "HSKT"
                    result['reasoning'] = f"[AUTO-FIXED: BMTâ†’HSKT] {result.get('reasoning', '')}"
                    break
        
        return result
    except Exception as e:
        print(f"Quick scan error for {image_path}: {e}", file=sys.stderr)
        return {'short_code': 'ERROR', 'confidence': 0}


def group_by_document(quick_results, file_paths):
    """
    NhÃ³m files thÃ nh documents dá»±a trÃªn quick scan results
    Returns: List of document groups [[0,1,2], [3,4], [5,6,7,8], ...]
    """
    print("\nğŸ§  Analyzing document boundaries...", file=sys.stderr)
    
    groups = []
    current_group = [0]
    last_type = quick_results[0].get('short_code', 'UNKNOWN')
    
    for i in range(1, len(quick_results)):
        result = quick_results[i]
        short_code = result.get('short_code', 'UNKNOWN')
        confidence = result.get('confidence', 0)
        reasoning = result.get('reasoning', '').lower()
        
        # Check if this is a new document
        is_new_document = False
        
        # High confidence with clear title â†’ New document
        if confidence >= 0.8 and short_code != 'UNKNOWN':
            is_new_document = True
            print(f"   ğŸ“„ [{i}] New document detected: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Low confidence + continuation indicators â†’ Same document
        elif confidence < 0.5 and any(kw in reasoning for kw in ['section header', 'ii.', 'iii.', 'thá»­a Ä‘áº¥t']):
            is_new_document = False
            print(f"   â¡ï¸ [{i}] Continuation page: {short_code} ({confidence:.0%})", file=sys.stderr)
        
        # Borderline case - use confidence
        else:
            is_new_document = (confidence >= 0.7)
            print(f"   â“ [{i}] Borderline: {short_code} ({confidence:.0%}) â†’ {'New' if is_new_document else 'Continue'}", file=sys.stderr)
        
        if is_new_document:
            # Start new group
            groups.append(current_group)
            current_group = [i]
            last_type = short_code
        else:
            # Continue current group
            current_group.append(i)
    
    # Add last group
    if current_group:
        groups.append(current_group)
    
    print(f"\nâœ… Grouped into {len(groups)} documents:", file=sys.stderr)
    for g_idx, group in enumerate(groups):
        print(f"   Document {g_idx + 1}: {len(group)} pages {group}", file=sys.stderr)
    
    return groups


def batch_classify_smart(image_paths, api_key, engine_type='gemini-flash', last_known_type=None, max_batch_size=8):
    """
    PhÆ°Æ¡ng Ã¡n 2: Smart Batching vá»›i SEQUENTIAL METADATA
    
    Args:
        max_batch_size: Maximum files per batch (default 15, can be reduced if needed)
    """
    print(f"\n{'='*80}", file=sys.stderr)
    print("ğŸ§  BATCH MODE 2: Smart Batching (AI Document Detection)", file=sys.stderr)
    print(f"{'='*80}", file=sys.stderr)
    
    total_files = len(image_paths)
    
    # Smart batch size strategy WITH user-configurable max
    if total_files <= max_batch_size:
        batch_size = total_files
        print(f"ğŸ“Š Strategy: Send ALL {total_files} files in 1 batch (max={max_batch_size})", file=sys.stderr)
    else:
        batch_size = max_batch_size
        print(f"ğŸ“Š Strategy: Send {batch_size} files per batch (user configured max={max_batch_size})", file=sys.stderr)
    
    print("   Sequential metadata: Pass lastKnown between batches (0% overhead)", file=sys.stderr)
    
    # Use fixed batch with smart size + sequential metadata
    return batch_classify_fixed(image_paths, api_key, engine_type=engine_type, batch_size=batch_size, last_known_type=last_known_type)


# CLI interface for testing
if __name__ == "__main__":
    if len(sys.argv) < 4:
        print("Usage: python batch_processor.py <mode> <engine_type> <api_key> <image1> <image2> ...", file=sys.stderr)
        print("Modes: fixed, smart", file=sys.stderr)
        print("Engine types: gemini-flash, gemini-flash-lite, gemini-flash-hybrid", file=sys.stderr)
        print("Example: python batch_processor.py smart gemini-flash-hybrid AIza... img1.jpg img2.jpg img3.jpg", file=sys.stderr)
        sys.exit(1)
    
    mode = sys.argv[1]
    engine_type = sys.argv[2]
    api_key = sys.argv[3]
    image_paths = sys.argv[4:]
    
    print(f"ğŸ” Batch processing {len(image_paths)} images in '{mode}' mode with '{engine_type}'", file=sys.stderr)
    
    if mode == 'fixed':
        batch_data = batch_classify_fixed(image_paths, api_key, engine_type=engine_type, batch_size=5, last_known_type=None)
    elif mode == 'smart':
        # Check for optional max_batch_size env variable
        env_value = os.environ.get('SMART_MAX_BATCH_SIZE', '10')
        print(f"ğŸ” DEBUG: SMART_MAX_BATCH_SIZE env = '{env_value}'", file=sys.stderr)
        max_batch_size = int(env_value)
        print(f"ğŸ“Š Smart mode max_batch_size: {max_batch_size}", file=sys.stderr)
        batch_data = batch_classify_smart(image_paths, api_key, engine_type=engine_type, last_known_type=None, max_batch_size=max_batch_size)
    elif mode == 'tesseract_text':
        # NEW MODE: Tesseract OCR + Gemini Text Classification
        print(f"âš¡ [NEW MODE] Using Tesseract + Gemini Text approach", file=sys.stderr)
        batch_data = batch_classify_tesseract_text(image_paths, api_key, last_known_type=None)
    else:
        print(f"âŒ Unknown mode: {mode}", file=sys.stderr)
        sys.exit(1)
    
    # Extract results
    results = batch_data['results'] if isinstance(batch_data, dict) else batch_data
    
    # Output JSON to stdout for IPC
    print(json.dumps(results, ensure_ascii=False))
    
    print(f"\nğŸ“Š BATCH COMPLETE: {len(results)} files processed", file=sys.stderr)
