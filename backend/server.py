from fastapi import FastAPI, APIRouter, UploadFile, File, HTTPException
from fastapi.responses import FileResponse
from dotenv import load_dotenv
from starlette.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
import os
import logging
from pathlib import Path
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Optional
import uuid
from datetime import datetime, timezone
import base64
from io import BytesIO
from PIL import Image
from reportlab.lib.pagesizes import A4, A3, landscape, portrait
from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
try:
    from pypdf import PdfMerger
except ImportError:
    from pypdf import PdfWriter as PdfMerger
import tempfile
import asyncio
from emergentintegrations.llm.chat import LlmChat, UserMessage, ImageContent


ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# MongoDB connection
mongo_url = os.environ.get('MONGO_URL', 'mongodb://localhost:27017')
client = AsyncIOMotorClient(mongo_url)
db = client[os.environ.get('DB_NAME', 'document_scanner_db')]

# Create the main app without a prefix
app = FastAPI()

# Create a router with the /api prefix
api_router = APIRouter(prefix="/api")

# Emergent LLM Key
EMERGENT_LLM_KEY = os.environ.get('EMERGENT_LLM_KEY')

# Document type mapping
DOCUMENT_TYPES = {
    "B·∫£n m√¥ t·∫£ ranh gi·ªõi, m·ªëc gi·ªõi th·ª≠a ƒë·∫•t": "BMT",
    "B·∫£n v·∫Ω (Tr√≠ch l·ª•c, ƒëo t√°ch, ch·ªânh l√Ω)": "HSKT",
    "Tr√≠ch l·ª•c": "HSKT",
    "Tr√≠ch ƒëo": "HSKT",
    "Phi·∫øu ƒëo ƒë·∫°c ch·ªânh l√Ω": "HSKT",
    "B·∫£n v·∫Ω t√°ch th·ª≠a": "HSKT",
    "B·∫£n v·∫Ω h·ª£p th·ª≠a": "HSKT",
    "B·∫£n v·∫Ω ho√†n c√¥ng": "BVHC",
    "B·∫£n v·∫Ω nh√†": "BVN",
    "B·∫£ng k√™ khai di·ªán t√≠ch ƒëang s·ª≠ d·ª•ng": "BKKDT",
    "B·∫£ng li·ªát k√™ danh s√°ch c√°c th·ª≠a ƒë·∫•t c·∫•p gi·∫•y": "DSCG",
    "Bi√™n b·∫£n b√°n ƒë·∫•u gi√° t√†i s·∫£n": "BBBDG",
    "Bi√™n b·∫£n b√†n giao ƒë·∫•t tr√™n th·ª±c ƒë·ªãa": "BBGD",
    "Bi√™n b·∫£n c·ªßa H·ªôi ƒë·ªìng ƒëƒÉng k√Ω ƒë·∫•t ƒëai l·∫ßn ƒë·∫ßu": "BBHDDK",
    "Bi√™n b·∫£n ki·ªÉm tra nghi·ªám thu c√¥ng tr√¨nh x√¢y d·ª±ng": "BBNT",
    "Bi√™n b·∫£n ki·ªÉm tra sai s√≥t tr√™n Gi·∫•y ch·ª©ng nh·∫≠n": "BBKTSS",
    "Bi√™n b·∫£n ki·ªÉm tra, x√°c minh hi·ªán tr·∫°ng s·ª≠ d·ª•ng ƒë·∫•t": "BBKTHT",
    "Bi√™n b·∫£n v·ªÅ vi·ªác k·∫øt th√∫c c√¥ng khai c√¥ng b·ªë di ch√∫c": "BBKTDC",
    "Bi√™n b·∫£n v·ªÅ vi·ªác k·∫øt th√∫c th√¥ng b√°o ni√™m y·∫øt c√¥ng khai k·∫øt qu·∫£ ki·ªÉm tra h·ªì s∆° ƒëƒÉng k√Ω c·∫•p GCNQSD ƒë·∫•t": "KTCKCG",
    "Bi√™n b·∫£n v·ªÅ vi·ªác k·∫øt th√∫c th√¥ng b√°o ni√™m y·∫øt c√¥ng khai v·ªÅ vi·ªác m·∫•t GCNQSD ƒë·∫•t": "KTCKMG",
    "Bi√™n lai thu thu·∫ø s·ª≠ d·ª•ng ƒë·∫•t phi n√¥ng nghi·ªáp": "BLTT",
    "CƒÉn c∆∞·ªõc c√¥ng d√¢n": "CCCD",
    "Danh s√°ch ch·ªß s·ª≠ d·ª•ng v√† c√°c th·ª≠a ƒë·∫•t (m·∫´u 15)": "DS15",
    "Danh s√°ch c√¥ng khai h·ªì s∆° c·∫•p gi·∫•y CNQSDƒê": "DSCK",
    "Di ch√∫c": "DICHUC",
    "ƒê∆°n cam k·∫øt, Gi·∫•y cam K·∫øt": "DCK",
    "ƒê∆°n ƒëƒÉng k√Ω bi·∫øn ƒë·ªông ƒë·∫•t ƒëai, t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t": "DDKBD",
    "ƒê∆°n ƒëƒÉng k√Ω ƒë·∫•t ƒëai, t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t": "DDK",
    "ƒê∆°n ƒë·ªÅ ngh·ªã chuy·ªÉn h√¨nh th·ª©c giao ƒë·∫•t (cho thu√™ ƒë·∫•t)": "CHTGD",
    "ƒê∆°n ƒë·ªÅ ngh·ªã ƒëi·ªÅu ch·ªânh quy·∫øt ƒë·ªãnh giao ƒë·∫•t (cho thu√™ ƒë·∫•t, cho ph√©p chuy·ªÉn m·ª•c ƒë√≠ch)": "DCQDGD",
    "ƒê∆°n ƒë·ªÅ ngh·ªã mi·ªÖn gi·∫£m L·ªá ph√≠ tr∆∞·ªõc b·∫°, thu·∫ø thu nh·∫≠p c√° nh√¢n": "DMG",
    "ƒê∆°n ƒë·ªÅ ngh·ªã s·ª≠ d·ª•ng ƒë·∫•t k·∫øt h·ª£p ƒëa m·ª•c ƒë√≠ch": "DMD",
    "ƒê∆°n x√°c nh·∫≠n, Gi·∫•y X√°c nh·∫≠n": "DXN",
    "ƒê∆°n xin (ƒë·ªÅ ngh·ªã) chuy·ªÉn m·ª•c ƒë√≠ch s·ª≠ d·ª•ng ƒë·∫•t": "DXCMD",
    "ƒê∆°n xin (ƒë·ªÅ ngh·ªã) gia h·∫°n s·ª≠ d·ª•ng ƒë·∫•t": "DGH",
    "ƒê∆°n xin (ƒë·ªÅ ngh·ªã) giao ƒë·∫•t, cho thu√™ ƒë·∫•t": "DXGD",
    "ƒê∆°n xin (ƒë·ªÅ ngh·ªã) t√°ch th·ª≠a ƒë·∫•t, h·ª£p th·ª≠a ƒë·∫•t": "DXTHT",
    "ƒê∆°n xin c·∫•p ƒë·ªïi Gi·∫•y ch·ª©ng nh·∫≠n": "DXCD",
    "ƒê∆°n xin ƒëi·ªÅu ch·ªânh th·ªùi h·∫°n s·ª≠ d·ª•ng ƒë·∫•t c·ªßa d·ª± √°n ƒë·∫ßu t∆∞": "DDCTH",
    "ƒê∆°n xin x√°c nh·∫≠n l·∫°i th·ªùi h·∫°n s·ª≠ d·ª•ng ƒë·∫•t n√¥ng nghi·ªáp": "DXNTH",
    "Gi·∫•y ch·ª©ng nh·∫≠n k·∫øt h√¥n": "GKH",
    "Gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t, quy·ªÅn s·ªü h·ªØu t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t": "GCNM",
    "Gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t": "GCNC",
    "Gi·∫•y ƒë·ªÅ ngh·ªã x√°c nh·∫≠n c√°c kho·∫£n n·ªôp v√†o ng√¢n s√°ch": "GXNNVTC",
    "Gi·∫•y Khai Sinh": "GKS",
    "Gi·∫•y n·ªôp ti·ªÅn v√†o Ng√¢n s√°ch nh√† n∆∞·ªõc": "GNT",
    "Gi·∫•y sang nh∆∞·ª£ng ƒë·∫•t": "GSND",
    "Gi·∫•y t·ªù li√™n quan (c√°c lo·∫°i gi·∫•y t·ªù k√®m theo)": "GTLQ",
    "Gi·∫•y ·ªßy quy·ªÅn": "GUQ",
    "Gi·∫•y x√°c nh·∫≠n ƒëƒÉng k√Ω l·∫ßn ƒë·∫ßu": "GXNDKLD",
    "Gi·∫•y xin ph√©p x√¢y d·ª±ng": "GPXD",
    "Ho√° ƒë∆°n gi√° tr·ªã gia tƒÉng": "hoadon",
    "Ho√†n th√†nh c√¥ng t√°c b·ªìi th∆∞·ªùng h·ªó tr·ª£": "HTBTH",
    "H·ª£p ƒë·ªìng chuy·ªÉn nh∆∞·ª£ng, t·∫∑ng cho quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t": "HDCQ",
    "H·ª£p ƒë·ªìng mua b√°n t√†i s·∫£n b√°n ƒë·∫•u gi√°": "HDBDG",
    "H·ª£p ƒë·ªìng th·∫ø ch·∫•p quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t": "HDTHC",
    "H·ª£p ƒë·ªìng thi c√¥ng": "HDTCO",
    "H·ª£p ƒë·ªìng thu√™ ƒë·∫•t, ƒëi·ªÅu h·ªânh h·ª£p ƒë·ªìng thu√™ ƒë·∫•t": "HDTD",
    "H·ª£p ƒë·ªìng ·ªßy quy·ªÅn": "HDUQ",
    "Phi·∫øu chuy·ªÉn th√¥ng tin nghƒ©a v·ª• t√†i ch√≠nh": "PCT",
    "Phi·∫øu chuy·ªÉn th√¥ng tin ƒë·ªÉ x√°c ƒë·ªãnh nghƒ©a v·ª• t√†i ch√≠nh": "PCT",
    "Gi·∫•y ti·∫øp nh·∫≠n h·ªì s∆° v√† h·∫πn tr·∫£ k·∫øt qu·∫£": "BN",
    "Bi√™n nh·∫≠n h·ªì s∆°": "BN",
    "Phi·∫øu ki·ªÉm tra h·ªì s∆°": "PKTHS",
    "Phi·∫øu l·∫•y √Ω ki·∫øn khu d√¢n c∆∞": "PLYKDC",
    "Phi·∫øu x√°c nh·∫≠n k·∫øt qu·∫£ ƒëo ƒë·∫°c": "PXNKQDD",
    "Phi·∫øu y√™u c·∫ßu ƒëƒÉng k√Ω bi·ªán ph√°p b·∫£o ƒë·∫£m b·∫±ng quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t, t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t": "DKTC",
    "Phi·∫øu y√™u c·∫ßu ƒëƒÉng k√Ω thay ƒë·ªïi n·ªôi dung bi·ªán ph√°p b·∫£o ƒë·∫£m b·∫±ng quy·ªÅn sdƒë, t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t": "DKTD",
    "Phi·∫øu y√™u c·∫ßu x√≥a ƒëƒÉng k√Ω bi·ªán ph√°p b·∫£o ƒë·∫£m b·∫±ng quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t, t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t": "DKXTC",
    "Qu√©t m√£ QR": "QR",
    "Quy·∫øt ƒë·ªãnh cho ph√©p chuy·ªÉn m·ª•c ƒë√≠ch": "QDCMD",
    "Quy·∫øt ƒë·ªãnh cho ph√©p t√°ch, h·ª£p th·ª≠a ƒë·∫•t": "QDTT",
    "Quy·∫øt ƒë·ªãnh chuy·ªÉn h√¨nh th·ª©c giao ƒë·∫•t (cho thu√™ ƒë·∫•t)": "QDCHTGD",
    "Quy·∫øt ƒë·ªãnh ƒëi·ªÅu ch·ªânh quy·∫øt ƒë·ªãnh giao ƒë·∫•t (cho thu√™ ƒë·∫•t, cho ph√©p chuy·ªÉn m·ª•c ƒë√≠ch)": "QDDCGD",
    "Quy·∫øt ƒë·ªãnh ƒëi·ªÅu ch·ªânh th·ªùi h·∫°n SDƒê c·ªßa d·ª± √°n ƒë·∫ßu t∆∞": "QDDCTH",
    "Quy·∫øt ƒë·ªãnh gia h·∫°n s·ª≠ d·ª•ng ƒë·∫•t khi h·∫øt th·ªùi h·∫°n SDƒê": "QDGH",
    "Quy·∫øt ƒë·ªãnh giao ƒë·∫•t, cho thu√™ ƒë·∫•t": "QDGTD",
    "Quy·∫øt ƒë·ªãnh h·ªßy Gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t": "QDHG",
    "Quy·∫øt ƒë·ªãnh ph√™ duy·ªát ph∆∞∆°ng √°n b·ªìi th∆∞·ªùng, h·ªó tr·ª£, t√°i ƒë·ªãnh c∆∞": "QDPDBT",
    "Quy·∫øt ƒë·ªãnh ph√™ quy·ªát ƒëi·ªÅu ch·ªânh quy ho·∫°ch": "QDDCQH",
    "Quy·∫øt ƒë·ªãnh ph√™ quy·ªát ƒë∆°n gi√°": "QDPDDG",
    "Quy·∫øt ƒë·ªãnh thi h√†nh √°n theo ƒë∆°n y√™u c·∫ßu": "QDTHA",
    "Quy·∫øt ƒë·ªãnh thu h·ªìi ƒë·∫•t": "QDTH",
    "Quy·∫øt ƒë·ªãnh v·ªÅ h√¨nh th·ª©c s·ª≠ d·ª•ng ƒë·∫•t": "QDHTSD",
    "Quy·∫øt ƒë·ªãnh x·ª≠ ph·∫°t": "QDXP",
    "S∆° ƒë·ªì d·ª± ki·∫øn t√°ch th·ª≠a": "SDTT",
    "Th√¥ng b√°o c·∫≠p nh·∫≠t, ch·ªânh l√Ω bi·∫øn ƒë·ªông": "TBCNBD",
    "Th√¥ng b√°o c√¥ng b·ªë c√¥ng khai di ch√∫c": "CKDC",
    "Th√¥ng b√°o thu·∫ø (tr∆∞·ªõc b·∫°, thu·∫ø TNCN, ti·ªÅn s·ª≠ d·ª•ng ƒë·∫•t)": "TBT",
    "Th√¥ng b√°o v·ªÅ vi·ªác chuy·ªÉn th√¥ng tin Gi·∫•y ch·ª©ng nh·∫≠n b·ªã m·∫•t ƒë·ªÉ ni√™m y·∫øt c√¥ng khai": "TBMG",
    "Th√¥ng b√°o v·ªÅ vi·ªác c√¥ng khai k·∫øt qu·∫£ th·∫©m tra x√©t duy·ªát h·ªì s∆° c·∫•p gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t": "TBCKCG",
    "Th√¥ng b√°o v·ªÅ vi·ªác ni√™m y·∫øt c√¥ng khai m·∫•t gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t": "TBCKMG",
    "Th√¥ng b√°o x√°c nh·∫≠n Ho√†n th√†nh nghƒ©a v·ª• t√†i ch√≠nh": "HTNVTC",
    "T·ªù khai thu·∫ø (tr∆∞·ªõc b·∫°, thu·∫ø TNCN, ti·ªÅn s·ª≠ d·ª•ng ƒë·∫•t)": "TKT",
    "T·ªù tr√¨nh v·ªÅ giao ƒë·∫•t (cho thu√™ ƒë·∫•t, cho ph√©p chuy·ªÉn m·ª•c ƒë√≠ch)": "TTr",
    "T·ªù tr√¨nh v·ªÅ vi·ªác ƒëƒÉng k√Ω ƒë·∫•t ƒëai, t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t (UBND x√£)": "TTCG",
    "VƒÉn b·∫£n cam k·∫øt t√†i s·∫£n ri√™ng": "CKTSR",
    "VƒÉn b·∫£n ch·∫•p thu·∫≠n cho ph√©p chuy·ªÉn m·ª•c ƒë√≠ch": "VBCTCMD",
    "VƒÉn b·∫£n ƒë·ªÅ ngh·ªã ch·∫•p thu·∫≠n nh·∫≠n chuy·ªÉn nh∆∞·ª£ng, thu√™, g√≥p v·ªën quy·ªÅn sdƒë": "VBDNCT",
    "VƒÉn b·∫£n ƒë·ªÅ ngh·ªã th·∫©m ƒë·ªãnh, ph√™ duy·ªát ph∆∞∆°ng √°n sdƒë": "PDPASDD",
    "VƒÉn b·∫£n th·ªèa thu·∫≠n ph√¢n chia di s·∫£n th·ª´a k·∫ø": "VBTK",
    "VƒÉn b·∫£n th·ªèa thu·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t c·ªßa h·ªô gia ƒë√¨nh": "TTHGD",
    "VƒÉn b·∫£n tho·∫£ thu·∫≠n v·ªÅ vi·ªác ch·∫•m d·ª©t quy·ªÅn h·∫°n ch·∫ø ƒë·ªëi v·ªõi th·ª≠a ƒë·∫•t li·ªÅn k·ªÅ": "CDLK",
    "VƒÉn b·∫£n th·ªèa thu·∫≠n v·ªÅ vi·ªác x√°c l·∫≠p quy·ªÅn h·∫°n ch·∫ø ƒë·ªëi v·ªõi th·ª≠a ƒë·∫•t li·ªÅn k·ªÅ": "HCLK",
    "VƒÉn b·∫£n t·ª´ ch·ªëi nh·∫≠n di s·∫£n th·ª´a k·∫ø": "VBTC",
    "VƒÉn b·∫£n ph√¢n chia t√†i s·∫£n chung v·ª£ ch·ªìng": "PCTSVC"
}


# Define Models
class ScanResult(BaseModel):
    model_config = ConfigDict(extra="ignore")
    
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    original_filename: str
    detected_type: str
    detected_full_name: str
    short_code: str
    confidence_score: float
    image_base64: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))


class UpdateFilenameRequest(BaseModel):
    id: str
    new_short_code: str


class ExportPDFRequest(BaseModel):
    scan_ids: List[str]


class DocumentRule(BaseModel):
    model_config = ConfigDict(extra="ignore")
    
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    full_name: str
    short_code: str
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))


class CreateRuleRequest(BaseModel):
    full_name: str
    short_code: str


class UpdateRuleRequest(BaseModel):
    full_name: Optional[str] = None
    short_code: Optional[str] = None


async def get_document_rules() -> dict:
    """Get all document rules from database or initialize from DOCUMENT_TYPES"""
    try:
        # Create unique index on short_code to prevent duplicates
        try:
            await db.document_rules.create_index("short_code", unique=True)
        except:
            pass  # Index already exists
        
        # Check if rules exist in database
        rules_count = await db.document_rules.count_documents({})
        
        if rules_count == 0:
            # First time: migrate from DOCUMENT_TYPES to database
            logger.info("Initializing document rules from DOCUMENT_TYPES")
            for full_name, short_code in DOCUMENT_TYPES.items():
                try:
                    rule = DocumentRule(
                        full_name=full_name,
                        short_code=short_code
                    )
                    # Use insert_one with error handling to prevent duplicates
                    await db.document_rules.insert_one(rule.model_dump())
                except Exception as e:
                    # Skip if already exists (duplicate key error)
                    if "duplicate" in str(e).lower():
                        logger.debug(f"Rule {short_code} already exists, skipping")
                    else:
                        logger.error(f"Error inserting rule {short_code}: {e}")
            
            final_count = await db.document_rules.count_documents({})
            logger.info(f"Initialized {final_count} document rules")
        
        # Fetch all rules from database
        rules_cursor = db.document_rules.find({})
        rules = await rules_cursor.to_list(length=None)
        
        # Convert to dict format {full_name: short_code}
        rules_dict = {rule['full_name']: rule['short_code'] for rule in rules}
        
        return rules_dict
    except Exception as e:
        logger.error(f"Error getting document rules: {e}")
        # Fallback to hardcoded DOCUMENT_TYPES
        return DOCUMENT_TYPES


async def smart_crop_and_analyze(image_bytes: bytes) -> tuple[str, dict]:
    """
    Two-Pass Smart Cropping:
    Pass 1: Crop 30% and detect emblem
    - If emblem found in 30% ‚Üí GCN m·ªõi (qu·ªëc huy ·ªü ƒë·∫ßu 10-15%) ‚Üí crop 40%
    - If no emblem in 30% ‚Üí GCN c≈© (qu·ªëc huy ·ªü gi·ªØa 30-40%) ‚Üí crop 60%
    Pass 2: Analyze with optimal crop
    
    Note: N·∫øu emblem XU·∫§T HI·ªÜN trong 30% crop th√¨ c√≥ nghƒ©a n√≥ ·ªü v·ªã tr√≠ 0-30%,
    V·∫™N C√ì TH·ªÇ l√† GCN c≈© v·ªõi qu·ªëc huy ·ªü 25-30%. ƒê·ªÉ an to√†n, tƒÉng crop l√™n 40-60%.
    
    Returns: (cropped_image_base64, analysis_result)
    """
    try:
        # PASS 1: Quick emblem detection with 30% crop
        logger.info("üîç PASS 1: Detecting emblem with 30% crop...")
        quick_crop_base64 = resize_image_for_api(image_bytes, max_size=800, crop_top_only=True, crop_percentage=0.30)
        
        has_emblem = await detect_emblem_in_image(quick_crop_base64)
        
        # PASS 2: Smart cropping based on emblem detection
        if has_emblem:
            # Emblem found in 30% crop ‚Üí Could be GCN m·ªõi OR GCN c≈©
            # Use 65% to be safe (covers single page + 2-page spread cases)
            logger.info("‚úÖ Emblem detected in top 30% ‚Üí Using 65% crop (safe for all GCN types)")
            optimal_crop_percentage = 0.65
        else:
            # No emblem in 30% ‚Üí Very rare, maybe not GCN or image issue
            # Use 70% as maximum safe crop
            logger.info("‚ö†Ô∏è  Emblem NOT detected in top 30% ‚Üí Using 70% crop (maximum coverage)")
            optimal_crop_percentage = 0.70
        
        # Create optimal crop for final analysis
        cropped_image_base64 = resize_image_for_api(
            image_bytes, 
            max_size=1024, 
            crop_top_only=True, 
            crop_percentage=optimal_crop_percentage
        )
        
        # Analyze with optimal crop
        logger.info(f"üìä PASS 2: Analyzing document with {int(optimal_crop_percentage*100)}% crop...")
        analysis_result = await analyze_document_with_vision(cropped_image_base64)
        
        return cropped_image_base64, analysis_result
        
    except Exception as e:
        logger.error(f"Error in smart crop and analyze: {e}")
        # Fallback: use 45% crop (middle ground)
        logger.info("‚ö†Ô∏è  Fallback to 45% crop due to error")
        fallback_crop = resize_image_for_api(image_bytes, max_size=1024, crop_top_only=True, crop_percentage=0.45)
        analysis = await analyze_document_with_vision(fallback_crop)
        return fallback_crop, analysis


async def detect_emblem_in_image(image_base64: str) -> bool:
    """Quick check to detect Vietnamese national emblem in image - for smart cropping"""
    try:
        chat = LlmChat(
            api_key=EMERGENT_LLM_KEY,
            session_id=f"emblem_detect_{uuid.uuid4()}",
            system_message="You are a visual recognition expert. Answer with simple YES or NO only."
        ).with_model("openai", "gpt-4o")
        
        image_content = ImageContent(image_base64=image_base64)
        
        prompt = """Look at this image carefully.
        
Do you see the Vietnamese national emblem (QU·ªêC HUY VI·ªÜT NAM)?
The emblem has these features:
- Golden/yellow star with 5 points
- Red circular background
- Hammer and sickle symbols
- Usually at the top of official documents

Answer with ONLY ONE WORD:
- "YES" if you clearly see the Vietnamese national emblem
- "NO" if you don't see it or unsure

Your answer:"""
        
        user_message = UserMessage(text=prompt, file_contents=[image_content])
        response = await chat.send_message(user_message)
        
        # Parse response
        answer = response.strip().upper()
        has_emblem = "YES" in answer
        
        logger.info(f"Emblem detection result: {answer} ‚Üí {has_emblem}")
        return has_emblem
        
    except Exception as e:
        logger.error(f"Error detecting emblem: {e}")
        # Fallback: assume emblem exists (use safer crop)
        return False


async def analyze_document_with_vision(image_base64: str) -> dict:
    """Analyze document using OpenAI Vision API with dynamic rules from database"""
    try:
        # Get document rules from database
        document_rules = await get_document_rules()
        
        # Create a mapping list for the prompt
        doc_types_list = "\n".join([f"- {full_name}: {code}" for full_name, code in document_rules.items()])
        
        # Initialize LlmChat with precision focus
        chat = LlmChat(
            api_key=EMERGENT_LLM_KEY,
            session_id=f"doc_scan_{uuid.uuid4()}",
            system_message="""B·∫°n l√† AI chuy√™n gia ph√¢n lo·∫°i t√†i li·ªáu ƒë·∫•t ƒëai Vi·ªát Nam.
            NHI·ªÜM V·ª§: ƒê·ªçc CH√çNH X√ÅC t·ª´ng t·ª´ trong ti√™u ƒë·ªÅ, ph√¢n bi·ªát c√°c lo·∫°i gi·∫•y t·ªù t∆∞∆°ng t·ª±.
            L∆ØU √ù: Nhi·ªÅu lo·∫°i t√†i li·ªáu ch·ªâ kh√°c nhau 1-2 t·ª´ (v√≠ d·ª•: "bi·∫øn ƒë·ªông" vs kh√¥ng c√≥).
            Tr·∫£ v·ªÅ JSON v·ªõi t√™n v√† m√£ CH√çNH X√ÅC t·ª´ danh s√°ch."""
        ).with_model("openai", "gpt-4o")  # gpt-4o for precision
        
        # Create image content
        image_content = ImageContent(image_base64=image_base64)
        
        # Create user message with OPTIMIZED prompt - focus on qu·ªëc huy + title
        prompt = f"""IMPORTANT: This is a DOCUMENT ANALYSIS task for land registry documents. 
Any faces/photos in the document are part of official government records (ID photos on land certificates).
Please analyze ONLY the document text and official stamps, not the personal photos.

Nh·∫≠n di·ªán t√†i li·ªáu d·ª±a v√†o QU·ªêC HUY v√† TI√äU ƒê·ªÄ.

üéØ ∆ØU TI√äN 1: NH·∫¨N DI·ªÜN QU·ªêC HUY VI·ªÜT NAM
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ N·∫øu th·∫•y QU·ªêC HUY Vi·ªát Nam (ng√¥i sao v√†ng, b√∫a li·ªÅm) ‚Üí ƒê√¢y l√† t√†i li·ªáu ch√≠nh th·ª©c

üîç Sau ƒë√≥ ki·ªÉm tra ti√™u ƒë·ªÅ:
  ‚Ä¢ "Gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t, quy·ªÅn s·ªü h·ªØu t√†i s·∫£n g·∫Øn li·ªÅn v·ªõi ƒë·∫•t" ‚Üí GCNM (GCN m·ªõi - ti√™u ƒë·ªÅ D√ÄI)
  ‚Ä¢ "Gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t" (KH√îNG c√≥ "quy·ªÅn s·ªü h·ªØu...") ‚Üí GCNC (GCN c≈© - ti√™u ƒë·ªÅ NG·∫ÆN)
  ‚Ä¢ N·∫øu ch·ªâ th·∫•y "GI·∫§Y CH·ª®NG NH·∫¨N" m√† kh√¥ng r√µ ti·∫øp theo ‚Üí GCNC
  ‚Ä¢ C√°c lo·∫°i kh√°c theo danh s√°ch b√™n d∆∞·ªõi

‚ö†Ô∏è IMPORTANT for 2-page horizontal documents:
- If you see orange/colored background with national emblem on RIGHT side ‚Üí This is GCNC
- Focus on the RIGHT page for title

‚ö†Ô∏è IGNORE any photos/faces in the document - focus ONLY on text and official stamps.

‚ö†Ô∏è QUY T·∫ÆC NGHI√äM NG·∫∂T: CH·ªà CH·∫§P NH·∫¨N KHI KH·ªöP 100% CH√çNH X√ÅC!
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ùå KH√îNG ƒë∆∞·ª£c ƒëo√°n ho·∫∑c ch·ªçn "g·∫ßn gi·ªëng"
‚ùå KH√îNG ƒë∆∞·ª£c b·ªè qua t·ª´ kh√≥a ph√¢n bi·ªát
‚úÖ CH·ªà ch·ªçn khi kh·ªõp CH√çNH X√ÅC 100% v·ªõi danh s√°ch

N·∫æU KH√îNG CH·∫ÆC CH·∫ÆN 100% ‚Üí Tr·∫£ v·ªÅ "CONTINUATION" (trang ti·∫øp theo c·ªßa t√†i li·ªáu tr∆∞·ªõc)

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

C√ÅC C·∫∂P D·ªÑ NH·∫¶M - PH·∫¢I KH·ªöP CH√çNH X√ÅC:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
1. "ƒê∆°n ƒëƒÉng k√Ω BI·∫æN ƒê·ªòNG ƒë·∫•t ƒëai" ‚Üí DDKBD (PH·∫¢I c√≥ "BI·∫æN ƒê·ªòNG")
   "ƒê∆°n ƒëƒÉng k√Ω ƒë·∫•t ƒëai" ‚Üí DDK (KH√îNG c√≥ "BI·∫æN ƒê·ªòNG")
   N·∫øu kh√¥ng r√µ c√≥ "BI·∫æN ƒê·ªòNG" kh√¥ng ‚Üí "CONTINUATION"

2. "H·ª£p ƒë·ªìng CHUY·ªÇN NH∆Ø·ª¢NG" ‚Üí HDCQ (PH·∫¢I c√≥ "CHUY·ªÇN NH∆Ø·ª¢NG")
   "H·ª£p ƒë·ªìng THU√ä" ‚Üí HDTD (PH·∫¢I c√≥ "THU√ä")
   "H·ª£p ƒë·ªìng TH·∫æ CH·∫§P" ‚Üí HDTHC (PH·∫¢I c√≥ "TH·∫æ CH·∫§P")
   N·∫øu kh√¥ng r√µ lo·∫°i n√†o ‚Üí "CONTINUATION"

3. "Quy·∫øt ƒë·ªãnh CHO PH√âP chuy·ªÉn m·ª•c ƒë√≠ch" ‚Üí QDCMD (PH·∫¢I c√≥ "CHO PH√âP")
   N·∫øu kh√¥ng th·∫•y "CHO PH√âP" r√µ r√†ng ‚Üí "CONTINUATION"

DANH S√ÅCH ƒê·∫¶Y ƒê·ª¶ (kh·ªõp ch√≠nh x√°c):
{doc_types_list}

QUY TR√åNH KI·ªÇM TRA:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
1. T√¨m qu·ªëc huy Vi·ªát Nam (n·∫øu c√≥ ‚Üí t√†i li·ªáu ch√≠nh th·ª©c)
2. ƒê·ªçc ti√™u ƒë·ªÅ ƒë·∫ßy ƒë·ªß
3. T√¨m trong danh s√°ch c√≥ t√™n CH√çNH X√ÅC 100%?
4. N·∫æU C√ì ‚Üí Tr·∫£ v·ªÅ t√™n + m√£ ch√≠nh x√°c, confidence: 0.9
5. N·∫æU KH√îNG ‚Üí Tr·∫£ v·ªÅ "CONTINUATION", confidence: 0.1

TR·∫¢ V·ªÄ JSON:
{{
  "detected_full_name": "T√™n CH√çNH X√ÅC t·ª´ danh s√°ch HO·∫∂C 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ'",
  "short_code": "M√É CH√çNH X√ÅC HO·∫∂C 'CONTINUATION'",
  "confidence": 0.9 ho·∫∑c 0.1
}}

‚ùó NH·ªö: Th√† tr·∫£ v·ªÅ "CONTINUATION" c√≤n h∆°n ƒëo√°n sai!"""
        
        user_message = UserMessage(
            text=prompt,
            file_contents=[image_content]
        )
        
        # Send message and get response
        response = await chat.send_message(user_message)
        
        # Parse JSON response
        import json
        # Extract JSON from response (handle markdown code blocks)
        response_text = response.strip()
        
        # Log raw response for debugging
        logger.info(f"OpenAI Vision raw response: {response_text[:200]}")
        
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()
        
        # Try to parse JSON
        try:
            result = json.loads(response_text)
        except json.JSONDecodeError as je:
            logger.error(f"JSON parse error. Raw response: {response_text}")
            
            # Check if OpenAI refused due to content policy
            if "can't help" in response_text.lower() or "sorry" in response_text.lower():
                logger.warning("OpenAI refused to analyze - likely due to face detection. Returning CONTINUATION.")
                return {
                    "detected_full_name": "Kh√¥ng c√≥ ti√™u ƒë·ªÅ",
                    "short_code": "CONTINUATION",
                    "confidence": 0.1
                }
            
            # Try to extract JSON manually if it's embedded in text
            import re
            json_match = re.search(r'\{[^}]+\}', response_text)
            if json_match:
                result = json.loads(json_match.group())
            else:
                # If all fails, return CONTINUATION instead of ERROR
                logger.error("Cannot parse response, returning CONTINUATION")
                return {
                    "detected_full_name": "Kh√¥ng c√≥ ti√™u ƒë·ªÅ",
                    "short_code": "CONTINUATION",
                    "confidence": 0.1
                }
        
        return {
            "detected_full_name": result.get("detected_full_name", "Kh√¥ng x√°c ƒë·ªãnh"),
            "short_code": result.get("short_code", "UNKNOWN"),
            "confidence": result.get("confidence", 0.0)
        }
        
    except Exception as e:
        logger.error(f"Error analyzing document: {e}")
        return {
            "detected_full_name": "L·ªói ph√¢n t√≠ch",
            "short_code": "ERROR",
            "confidence": 0.0
        }


def resize_image_for_api(image_bytes: bytes, max_size: int = 1024, crop_top_only: bool = True, crop_percentage: float = 0.35) -> str:
    """Resize image and convert to base64 - SMART CROP: adaptive based on emblem detection"""
    try:
        img = Image.open(BytesIO(image_bytes))
        
        # Convert to RGB if necessary
        if img.mode in ('RGBA', 'LA', 'P'):
            img = img.convert('RGB')
        
        # SMART CROP: Dynamic crop based on crop_percentage parameter
        if crop_top_only:
            width, height = img.size
            crop_height = int(height * crop_percentage)
            img = img.crop((0, 0, width, crop_height))
            logger.info(f"Smart cropped: {height}px ‚Üí {crop_height}px ({int(crop_percentage*100)}% crop)")
        
        # Optimized resize for speed + quality balance
        if max(img.size) > max_size:
            ratio = max_size / max(img.size)
            new_size = tuple(int(dim * ratio) for dim in img.size)
            img = img.resize(new_size, Image.Resampling.LANCZOS)
        
        # High quality for Vietnamese OCR
        buffered = BytesIO()
        img.save(buffered, format="JPEG", quality=80, optimize=True)
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        
        return img_base64
    except Exception as e:
        logger.error(f"Error resizing image: {e}")
        raise


def create_pdf_from_image(image_base64: str, output_path: str, filename: str):
    """Create a PDF file from base64 image - AUTO-DETECT A3/A4 and orientation"""
    try:
        # Decode base64 image
        image_data = base64.b64decode(image_base64)
        img = Image.open(BytesIO(image_data))
        
        # Convert to RGB if necessary
        if img.mode in ('RGBA', 'LA', 'P'):
            img = img.convert('RGB')
        
        img_width, img_height = img.size
        
        # SMART PAGE SIZE DETECTION
        # A3 = 297mm x 420mm = 842pt x 1191pt
        # A4 = 210mm x 297mm = 595pt x 842pt
        
        # Determine if landscape or portrait
        is_landscape = img_width > img_height
        
        # Calculate aspect ratio to detect A3 vs A4
        aspect_ratio = max(img_width, img_height) / min(img_width, img_height)
        
        # A3 aspect ratio ‚âà 1.414 (‚àö2)
        # A4 aspect ratio ‚âà 1.414 (‚àö2) - same!
        # But A3 is larger in pixels
        
        # Detect paper size based on image dimensions
        # If image is very large (> 3000px on long side), likely A3
        long_side = max(img_width, img_height)
        
        if long_side > 3000 or (img_width > 3000 and img_height > 2000):
            # A3 size
            if is_landscape:
                page_size = landscape(A3)
                logger.info(f"Detected A3 Landscape: {img_width}x{img_height}")
            else:
                page_size = portrait(A3)
                logger.info(f"Detected A3 Portrait: {img_width}x{img_height}")
        else:
            # A4 size (default)
            if is_landscape:
                page_size = landscape(A4)
                logger.info(f"Detected A4 Landscape: {img_width}x{img_height}")
            else:
                page_size = portrait(A4)
                logger.info(f"Detected A4 Portrait: {img_width}x{img_height}")
        
        # Create PDF with detected page size
        c = canvas.Canvas(output_path, pagesize=page_size)
        
        page_width, page_height = page_size
        
        # Calculate scaling to fit page while maintaining aspect ratio
        scale = min(page_width / img_width, page_height / img_height) * 0.95  # 95% to leave margin
        
        new_width = img_width * scale
        new_height = img_height * scale
        
        # Center the image
        x = (page_width - new_width) / 2
        y = (page_height - new_height) / 2
        
        # Draw image
        img_reader = ImageReader(BytesIO(image_data))
        c.drawImage(img_reader, x, y, width=new_width, height=new_height)
        
        c.save()
        logger.info(f"Created PDF: {output_path} with page size {page_width:.0f}x{page_height:.0f}pt")
        
    except Exception as e:
        logger.error(f"Error creating PDF: {e}")
        raise


@api_router.post("/retry-scan")
async def retry_scan(scan_id: str):
    """Retry scanning a failed document"""
    try:
        # DEBUG: Log query
        logger.info(f"Retry scan for id: {scan_id}")
        
        # Get the failed scan from database
        failed_scan = await db.scan_results.find_one({"id": scan_id})
        
        if not failed_scan:
            total_docs = await db.scan_results.count_documents({})
            logger.error(f"Scan not found. Total docs: {total_docs}")
            raise HTTPException(status_code=404, detail=f"Scan not found (searched in {total_docs} documents)")
        
        if failed_scan.get('short_code') != 'ERROR':
            raise HTTPException(status_code=400, detail="Document is not in error state")
        
        # Check if we have the image
        image_base64 = failed_scan.get('image_base64')
        if not image_base64:
            raise HTTPException(status_code=400, detail="No image data to retry")
        
        # Decode image and retry scan
        import base64
        image_bytes = base64.b64decode(image_base64)
        
        # Create cropped image for OCR (35% crop, 1024px)
        cropped_image_base64 = resize_image_for_api(image_bytes, crop_top_only=True, max_size=1024)
        
        # Retry analysis with Vision API
        analysis_result = await analyze_document_with_vision(cropped_image_base64)
        
        # Update database with new result
        update_result = await db.scan_results.update_one(
            {"id": scan_id},
            {"$set": {
                "detected_type": analysis_result["detected_full_name"],
                "detected_full_name": analysis_result["detected_full_name"],
                "short_code": analysis_result["short_code"],
                "confidence_score": analysis_result["confidence"]
            }}
        )
        
        if update_result.modified_count == 0:
            raise HTTPException(status_code=500, detail="Failed to update scan result")
        
        return {
            "message": "Retry successful",
            "detected_type": analysis_result["detected_full_name"],
            "short_code": analysis_result["short_code"],
            "confidence": analysis_result["confidence"]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrying scan {scan_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.post("/scan-document", response_model=ScanResult)
async def scan_document(file: UploadFile = File(...)):
    """Scan a single document and detect type"""
    try:
        # Read file content
        content = await file.read()
        
        # DETECT ASPECT RATIO FROM ORIGINAL IMAGE FIRST (before any resize)
        img_original = Image.open(BytesIO(content))
        img_width, img_height = img_original.size
        aspect_ratio = img_width / img_height
        
        logger.info(f"Original image: {img_width}x{img_height}, aspect ratio: {aspect_ratio:.2f}")
        
        # If aspect ratio > 1.35, likely 2-page horizontal spread or wide format (like GCN c≈©)
        # Lowered from 1.5 to 1.35 to catch more 2-page documents
        if aspect_ratio > 1.35:
            # 2-page spread or wide format: Title may be at 40-60%, use 65% crop
            crop_percent = 0.65
            logger.info(f"‚Üí Detected 2-page/wide format (aspect {aspect_ratio:.2f}) ‚Üí Using 65% crop")
        else:
            # Single page portrait: 50% crop sufficient
            crop_percent = 0.50
            logger.info(f"‚Üí Detected single page (aspect {aspect_ratio:.2f}) ‚Üí Using 50% crop")
        
        # Create FULL image for preview/storage (AFTER detection)
        full_image_base64 = resize_image_for_api(content, crop_top_only=False, max_size=1280)
        
        # Crop and analyze with detected percentage
        cropped_image_base64 = resize_image_for_api(content, crop_top_only=True, max_size=800, crop_percentage=crop_percent)
        analysis_result = await analyze_document_with_vision(cropped_image_base64)
        
        # Create scan result with FULL image for preview
        scan_result = ScanResult(
            original_filename=file.filename,
            detected_type=analysis_result["detected_full_name"],
            detected_full_name=analysis_result["detected_full_name"],
            short_code=analysis_result["short_code"],
            confidence_score=analysis_result["confidence"],
            image_base64=full_image_base64  # Store full image
        )
        
        # Save to database
        doc = scan_result.model_dump()
        doc['timestamp'] = doc['timestamp'].isoformat()
        await db.scan_results.insert_one(doc)
        
        return scan_result
        
    except Exception as e:
        logger.error(f"Error scanning document: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def apply_smart_grouping(results: List[ScanResult]) -> List[ScanResult]:
    """
    Smart grouping: STRICT mode - ch·ªâ nh√≥m khi kh√¥ng c√≥ ti√™u ƒë·ªÅ r√µ r√†ng
    
    Quy t·∫Øc NGHI√äM NG·∫∂T:
    - File c√≥ ti√™u ƒë·ªÅ CH√çNH X√ÅC (confidence > 0.8) ‚Üí T√†i li·ªáu m·ªõi
    - File KH√îNG c√≥ ti√™u ƒë·ªÅ (CONTINUATION ho·∫∑c confidence < 0.2) ‚Üí Trang ti·∫øp theo
    - File l·ªói (ERROR) ‚Üí Gi·ªØ nguy√™n
    """
    if not results:
        return results
    
    grouped = []
    last_valid_code = None
    last_valid_name = None
    continuation_count = 0
    
    for i, result in enumerate(results):
        # Skip error results
        if result.short_code == "ERROR":
            grouped.append(result)
            continuation_count = 0
            continue
        
        # STRICT: Only treat as continuation if very low confidence
        is_continuation = (
            result.short_code == "CONTINUATION" or 
            result.confidence_score < 0.2 or  # Increased threshold from 0.3 to 0.2
            "kh√¥ng c√≥ ti√™u ƒë·ªÅ" in result.detected_full_name.lower()
        )
        
        if is_continuation and last_valid_code:
            # This is a continuation page - use previous document's name
            continuation_count += 1
            logger.info(f"Page {i+1} ({result.original_filename}) identified as continuation of {last_valid_code} (page {continuation_count + 1})")
            
            grouped.append(ScanResult(
                id=result.id,
                original_filename=result.original_filename,
                detected_type=f"{last_valid_name} (trang {continuation_count + 1})",
                detected_full_name=f"{last_valid_name} (trang {continuation_count + 1})",
                short_code=last_valid_code,
                confidence_score=0.95,  # High confidence for grouped pages
                image_base64=result.image_base64,
                timestamp=result.timestamp
            ))
        else:
            # This is a new document with clear title
            last_valid_code = result.short_code
            last_valid_name = result.detected_full_name
            continuation_count = 0
            
            # Mark as page 1 if it has good confidence
            if result.confidence_score > 0.7:
                grouped.append(ScanResult(
                    id=result.id,
                    original_filename=result.original_filename,
                    detected_type=f"{result.detected_full_name} (trang 1)",
                    detected_full_name=f"{result.detected_full_name} (trang 1)",
                    short_code=result.short_code,
                    confidence_score=result.confidence_score,
                    image_base64=result.image_base64,
                    timestamp=result.timestamp
                ))
            else:
                grouped.append(result)
    
    return grouped


@api_router.post("/batch-scan", response_model=List[ScanResult])
async def batch_scan(files: List[UploadFile] = File(...)):
    """Scan multiple documents - OPTIMIZED for 50+ files with controlled concurrency"""
    try:
        # Semaphore to limit concurrent API calls (avoid rate limits and timeout)
        MAX_CONCURRENT = 5  # Reduced from 10 to 5 to avoid timeout
        semaphore = asyncio.Semaphore(MAX_CONCURRENT)
        
        async def process_file(file, retry_count=0):
            async with semaphore:  # Control concurrency
                max_retries = 2
                try:
                    # Read file content
                    content = await file.read()
                    
                    # DETECT ASPECT RATIO FROM ORIGINAL IMAGE FIRST
                    img_original = Image.open(BytesIO(content))
                    img_width, img_height = img_original.size
                    aspect_ratio = img_width / img_height
                    
                    # 2-page spread or wide format needs more crop (lowered threshold to 1.35)
                    crop_percent = 0.65 if aspect_ratio > 1.35 else 0.50
                    
                    # Create FULL image for preview (AFTER detection)
                    full_image_base64 = resize_image_for_api(content, max_size=1280, crop_top_only=False)
                    
                    cropped_image_base64 = resize_image_for_api(content, max_size=800, crop_top_only=True, crop_percentage=crop_percent)
                    analysis_result = await analyze_document_with_vision(cropped_image_base64)
                    
                    # Create scan result with FULL image for display
                    scan_result = ScanResult(
                        original_filename=file.filename,
                        detected_type=analysis_result["detected_full_name"],
                        detected_full_name=analysis_result["detected_full_name"],
                        short_code=analysis_result["short_code"],
                        confidence_score=analysis_result["confidence"],
                        image_base64=full_image_base64  # Store full image
                    )
                    
                    return scan_result
                except Exception as e:
                    error_msg = str(e)
                    
                    # Auto retry for timeout/connection errors
                    is_retryable = ("timeout" in error_msg.lower() or 
                                   "connection" in error_msg.lower() or
                                   "rate limit" in error_msg.lower())
                    
                    if is_retryable and retry_count < max_retries:
                        logger.warning(f"Retrying {file.filename} (attempt {retry_count + 1}/{max_retries})")
                        await asyncio.sleep(2 ** retry_count)  # Exponential backoff
                        return await process_file(file, retry_count + 1)
                    
                    logger.error(f"Error processing {file.filename}: {error_msg}", exc_info=True)
                    
                    # Categorize error
                    if "rate limit" in error_msg.lower():
                        error_type = "Rate Limit"
                        error_detail = "Qu√° nhi·ªÅu request. ƒê√£ th·ª≠ retry."
                    elif "timeout" in error_msg.lower():
                        error_type = "Timeout"
                        error_detail = f"API ph·∫£n h·ªìi qu√° l√¢u. ƒê√£ th·ª≠ {retry_count + 1} l·∫ßn."
                    elif "json" in error_msg.lower():
                        error_type = "JSON Parse Error"
                        error_detail = "AI tr·∫£ v·ªÅ d·ªØ li·ªáu kh√¥ng ƒë√∫ng format."
                    elif "connection" in error_msg.lower():
                        error_type = "Connection Error"
                        error_detail = f"M·∫•t k·∫øt n·ªëi m·∫°ng. ƒê√£ th·ª≠ {retry_count + 1} l·∫ßn."
                    elif "api_key" in error_msg.lower() or "unauthorized" in error_msg.lower():
                        error_type = "API Key Error"
                        error_detail = "L·ªói x√°c th·ª±c API key."
                    else:
                        error_type = "Unknown Error"
                        error_detail = error_msg[:100]
                    
                    # Return error result with details
                    return ScanResult(
                        original_filename=file.filename,
                        detected_type=f"‚ùå {error_type}",
                        detected_full_name=error_detail,
                        short_code="ERROR",
                        confidence_score=0.0,
                        image_base64=""
                    )
        
        # Process files with controlled concurrency
        tasks = [process_file(file) for file in files]
        results = await asyncio.gather(*tasks, return_exceptions=False)
        
        # SMART GROUPING: Apply continuation logic
        logger.info("Applying smart grouping for multi-page documents...")
        grouped_results = apply_smart_grouping(results)
        
        # Filter out error results and save valid ones
        valid_results = [r for r in grouped_results if r.short_code != "ERROR"]
        
        # Save to database in batch
        if valid_results:
            docs = []
            for result in valid_results:
                doc = result.model_dump()
                doc['timestamp'] = doc['timestamp'].isoformat()
                docs.append(doc)
            
            await db.scan_results.insert_many(docs)
        
        return grouped_results
        
    except Exception as e:
        logger.error(f"Error in batch scan: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.get("/scan-history", response_model=List[ScanResult])
async def get_scan_history():
    """Get all scan history"""
    try:
        results = await db.scan_results.find({}, {"_id": 0}).sort("timestamp", -1).to_list(1000)
        
        # Convert ISO string timestamps back to datetime objects
        for result in results:
            if isinstance(result['timestamp'], str):
                result['timestamp'] = datetime.fromisoformat(result['timestamp'])
        
        return results
        
    except Exception as e:
        logger.error(f"Error getting scan history: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.put("/update-filename")
async def update_filename(request: UpdateFilenameRequest):
    """Update the short code for a scan result - allows duplicate names"""
    try:
        # Validate input
        if not request.new_short_code or not request.new_short_code.strip():
            raise HTTPException(status_code=400, detail="Short code cannot be empty")
        
        # DEBUG: Log query
        logger.info(f"Searching for document with id: {request.id}")
        
        # Check if document exists first - also log count
        total_docs = await db.scan_results.count_documents({})
        logger.info(f"Total documents in collection: {total_docs}")
        
        existing = await db.scan_results.find_one({"id": request.id})
        
        if not existing:
            # DEBUG: Try to find by any field containing this id
            logger.error(f"Document not found. Searching all docs...")
            sample = await db.scan_results.find({}).limit(2).to_list(2)
            if sample:
                logger.error(f"Sample doc keys: {list(sample[0].keys())}")
            raise HTTPException(status_code=404, detail=f"Document with id {request.id} not found in {total_docs} documents")
        
        # Update the short code (duplicates are allowed)
        result = await db.scan_results.update_one(
            {"id": request.id},
            {"$set": {"short_code": request.new_short_code.strip()}}
        )
        
        logger.info(f"Updated filename for {request.id}: {existing.get('short_code')} -> {request.new_short_code}")
        
        return {
            "message": "Filename updated successfully",
            "old_code": existing.get('short_code'),
            "new_code": request.new_short_code.strip()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating filename: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.post("/export-single-document")
async def export_single_document(request: ExportPDFRequest):
    """Export a single document as PDF"""
    try:
        # Get scan result
        result = await db.scan_results.find_one(
            {"id": {"$in": request.scan_ids}},
            {"_id": 0}
        )
        
        if not result:
            raise HTTPException(status_code=404, detail="Scan result not found")
        
        # Create temp directory
        temp_dir = tempfile.mkdtemp()
        short_code = result.get('short_code', 'UNKNOWN')
        pdf_path = os.path.join(temp_dir, f"{short_code}.pdf")
        
        # Create PDF
        create_pdf_from_image(
            result['image_base64'],
            pdf_path,
            short_code
        )
        
        return FileResponse(
            pdf_path,
            media_type="application/pdf",
            filename=f"{short_code}.pdf"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error exporting single document: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.post("/export-pdf-single")
async def export_pdf_single(request: ExportPDFRequest):
    """Export scans as PDFs, automatically grouping by short_code"""
    try:
        # Get scan results
        results = await db.scan_results.find(
            {"id": {"$in": request.scan_ids}},
            {"_id": 0}
        ).to_list(1000)
        
        if not results:
            raise HTTPException(status_code=404, detail="No scan results found")
        
        # Group results by short_code
        grouped_results = {}
        for result in results:
            short_code = result.get('short_code', 'UNKNOWN')
            if short_code not in grouped_results:
                grouped_results[short_code] = []
            grouped_results[short_code].append(result)
        
        # Create temp directory for PDFs
        temp_dir = tempfile.mkdtemp()
        pdf_files = []
        
        # Process each group
        for short_code, group in grouped_results.items():
            if len(group) == 1:
                # Single document - create one PDF
                output_path = os.path.join(temp_dir, f"{short_code}.pdf")
                create_pdf_from_image(
                    group[0]['image_base64'],
                    output_path,
                    short_code
                )
                pdf_files.append(output_path)
            else:
                # Multiple documents with same code - merge into one PDF
                temp_pdfs = []
                for idx, result in enumerate(group):
                    temp_path = os.path.join(temp_dir, f"temp_{short_code}_{idx}.pdf")
                    create_pdf_from_image(
                        result['image_base64'],
                        temp_path,
                        f"{short_code}_{idx}"
                    )
                    temp_pdfs.append(temp_path)
                
                # Merge PDFs with same short_code
                merger = PdfMerger()
                for temp_pdf in temp_pdfs:
                    merger.append(temp_pdf)
                
                merged_path = os.path.join(temp_dir, f"{short_code}.pdf")
                merger.write(merged_path)
                merger.close()
                
                # Clean up temp files
                for temp_pdf in temp_pdfs:
                    os.remove(temp_pdf)
                
                pdf_files.append(merged_path)
        
        # Create a zip file with all PDFs
        zip_path = os.path.join(temp_dir, "documents_single.zip")
        import zipfile
        with zipfile.ZipFile(zip_path, 'w') as zipf:
            for pdf_file in pdf_files:
                zipf.write(pdf_file, os.path.basename(pdf_file))
        
        return FileResponse(
            zip_path,
            media_type="application/zip",
            filename="documents_single.zip"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error exporting single PDFs: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.post("/export-pdf-merged")
async def export_pdf_merged(request: ExportPDFRequest):
    """Export all scans merged into one PDF"""
    try:
        # Get scan results
        results = await db.scan_results.find(
            {"id": {"$in": request.scan_ids}},
            {"_id": 0}
        ).to_list(1000)
        
        if not results:
            raise HTTPException(status_code=404, detail="No scan results found")
        
        # Create temp directory
        temp_dir = tempfile.mkdtemp()
        pdf_files = []
        
        # Create individual PDFs first
        for idx, result in enumerate(results):
            output_path = os.path.join(temp_dir, f"temp_{idx}.pdf")
            create_pdf_from_image(
                result['image_base64'],
                output_path,
                f"page_{idx}"
            )
            pdf_files.append(output_path)
        
        # Merge PDFs
        merger = PdfMerger()
        for pdf_file in pdf_files:
            merger.append(pdf_file)
        
        # Save merged PDF
        merged_path = os.path.join(temp_dir, "documents_merged.pdf")
        merger.write(merged_path)
        merger.close()
        
        return FileResponse(
            merged_path,
            media_type="application/pdf",
            filename="documents_merged.pdf"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error exporting merged PDF: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.delete("/clear-history")
async def clear_history():
    """Clear all scan history"""
    try:
        result = await db.scan_results.delete_many({})
        return {"message": f"Deleted {result.deleted_count} scan results"}
    except Exception as e:
        logger.error(f"Error clearing history: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.get("/rules", response_model=List[DocumentRule])
async def get_rules():
    """Get all document rules"""
    try:
        rules_cursor = db.document_rules.find({})
        rules = await rules_cursor.to_list(length=None)
        
        # Initialize if empty
        if not rules:
            await get_document_rules()  # This will initialize from DOCUMENT_TYPES
            rules_cursor = db.document_rules.find({})
            rules = await rules_cursor.to_list(length=None)
        
        return [DocumentRule(**rule) for rule in rules]
    except Exception as e:
        logger.error(f"Error getting rules: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.post("/rules", response_model=DocumentRule)
async def create_rule(request: CreateRuleRequest):
    """Create a new document rule - ALLOWS DUPLICATE short_codes"""
    try:
        # REMOVED: No longer check for duplicate short_code
        # Multiple document types can have the same short code
        
        # Create new rule
        new_rule = DocumentRule(
            full_name=request.full_name,
            short_code=request.short_code
        )
        
        # Insert to database
        await db.document_rules.insert_one(new_rule.model_dump())
        
        logger.info(f"Created new rule: {request.full_name} -> {request.short_code}")
        return new_rule
    except Exception as e:
        logger.error(f"Error creating rule: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.put("/rules/{rule_id}", response_model=DocumentRule)
async def update_rule(rule_id: str, request: UpdateRuleRequest):
    """Update an existing document rule - ALLOWS DUPLICATE short_codes"""
    try:
        # Find existing rule
        existing_rule = await db.document_rules.find_one({"id": rule_id})
        if not existing_rule:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y quy t·∫Øc")
        
        # REMOVED: No longer check for duplicate short_code
        # Multiple document types can have the same short code
        
        # Prepare update data
        update_data = {"updated_at": datetime.now(timezone.utc)}
        if request.full_name:
            update_data["full_name"] = request.full_name
        if request.short_code:
            update_data["short_code"] = request.short_code
        
        # Update in database
        await db.document_rules.update_one(
            {"id": rule_id},
            {"$set": update_data}
        )
        
        # Get updated rule
        updated_rule = await db.document_rules.find_one({"id": rule_id})
        
        logger.info(f"Updated rule {rule_id}: {update_data}")
        return DocumentRule(**updated_rule)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating rule: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.delete("/rules/{rule_id}")
async def delete_rule(rule_id: str):
    """Delete a document rule"""
    try:
        # Find existing rule
        existing_rule = await db.document_rules.find_one({"id": rule_id})
        if not existing_rule:
            raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y quy t·∫Øc")
        
        # Delete from database
        result = await db.document_rules.delete_one({"id": rule_id})
        
        if result.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Kh√¥ng th·ªÉ x√≥a quy t·∫Øc")
        
        logger.info(f"Deleted rule {rule_id}: {existing_rule['full_name']}")
        return {"message": "ƒê√£ x√≥a quy t·∫Øc th√†nh c√¥ng", "deleted_rule": existing_rule['full_name']}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting rule: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.post("/rules/cleanup-duplicates")
async def cleanup_duplicate_rules():
    """Remove duplicate rules, keep only unique short_codes - OPTIMIZED"""
    try:
        # Use aggregation pipeline for better performance with large datasets
        pipeline = [
            {
                "$group": {
                    "_id": "$short_code",
                    "ids": {"$push": "$id"},
                    "count": {"$sum": 1}
                }
            },
            {
                "$match": {
                    "count": {"$gt": 1}  # Only groups with duplicates
                }
            }
        ]
        
        duplicates_groups = await db.document_rules.aggregate(pipeline).to_list(length=None)
        
        if not duplicates_groups:
            total_count = await db.document_rules.count_documents({})
            return {
                "message": "Kh√¥ng c√≥ quy t·∫Øc tr√πng l·∫∑p", 
                "remaining": total_count,
                "deleted": 0
            }
        
        # Collect IDs to delete (keep first, delete rest)
        ids_to_delete = []
        for group in duplicates_groups:
            # Keep first ID, delete the rest
            ids_to_delete.extend(group['ids'][1:])
        
        # Delete in batches to avoid timeout
        BATCH_SIZE = 100
        total_deleted = 0
        
        for i in range(0, len(ids_to_delete), BATCH_SIZE):
            batch = ids_to_delete[i:i+BATCH_SIZE]
            result = await db.document_rules.delete_many({"id": {"$in": batch}})
            total_deleted += result.deleted_count
            logger.info(f"Deleted batch {i//BATCH_SIZE + 1}: {result.deleted_count} rules")
        
        remaining_count = await db.document_rules.count_documents({})
        logger.info(f"Cleanup complete: deleted {total_deleted} duplicates, {remaining_count} remaining")
        
        return {
            "message": f"ƒê√£ x√≥a {total_deleted} quy t·∫Øc tr√πng l·∫∑p",
            "remaining": remaining_count,
            "deleted": total_deleted
        }
            
    except Exception as e:
        logger.error(f"Error cleaning duplicates: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@api_router.get("/")
async def root():
    return {"message": "Document Scanner API"}


# Include the router in the main app
app.include_router(api_router)

app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=os.environ.get('CORS_ORIGINS', '*').split(','),
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@app.on_event("shutdown")
async def shutdown_db_client():
    client.close()